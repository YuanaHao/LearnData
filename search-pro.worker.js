const V=Object.entries,et=Object.fromEntries,st="ENTRIES",L="KEYS",T="VALUES",_="";class D{set;_type;_path;constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=E(this._path);if(E(s)===_)return{done:!1,value:this.result()};const n=t.get(E(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=E(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>E(t)).filter(t=>t!==_).join("")}value(){return E(this._path).node.get(_)}result(){switch(this._type){case T:return this.value();case L:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const E=e=>e[e.length-1],nt=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return R(e,t,s,n,i,1,o,""),n},R=(e,t,s,n,o,u,i,r)=>{const d=u*i;t:for(const c of e.keys())if(c===_){const a=o[d-1];a<=s&&n.set(r,[e.get(c),a])}else{let a=u;for(let h=0;h<c.length;++h,++a){const g=c[h],m=i*a,p=m-i;let l=o[m];const f=Math.max(0,a-s-1),y=Math.min(i-1,a+s);for(let F=f;F<y;++F){const v=g!==t[F],z=o[p+F]+ +v,A=o[p+F+1]+1,w=o[m+F]+1,j=o[m+F+1]=Math.min(z,A,w);j<l&&(l=j)}if(l>s)continue t}R(e.get(c),t,s,n,o,a,i,r+c)}};class C{_tree;_prefix;_size=void 0;constructor(t=new Map,s=""){this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=O(n);for(const i of o.keys())if(i!==_&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ot(this._tree,t)}entries(){return new D(this,st)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return nt(this._tree,t,s)}get(t){const s=k(this._tree,t);return s!==void 0?s.get(_):void 0}has(t){const s=k(this._tree,t);return s!==void 0&&s.has(_)}keys(){return new D(this,L)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,I(this._tree,t).set(_,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);return n.set(_,s(n.get(_))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=I(this._tree,t);let o=n.get(_);return o===void 0&&n.set(_,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==_&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},k=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==_&&t.startsWith(s))return k(e.get(s),t.slice(s.length))},I=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==_&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const d=e.get(u);if(r===u.length)e=d;else{const c=new Map;c.set(u.slice(r),d),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ot=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(_),s.size===0)W(n);else if(s.size===1){const[o,u]=s.entries().next().value;q(n,o,u)}}},W=e=>{if(e.length===0)return;const[t,s]=O(e);if(t.delete(s),t.size===0)W(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==_&&q(e.slice(0,-1),n,o)}},q=(e,t,s)=>{if(e.length===0)return;const[n,o]=O(e);n.set(o+t,s),n.delete(o)},O=e=>e[e.length-1],ut=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,M="or",$="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},N=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},P=({score:e},{score:t})=>t-e,lt=()=>new Map,b=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[M]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),N(n.terms,u)}}return e},[$]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);N(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d}=u;return Math.log(1+(s-t+.5)/(t+.5))*(d+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},ft=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},gt={k:1.2,b:.7,d:.5},mt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof console?.[e]=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:M,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:gt},pt={combineWith:$,prefix:(e,t,s)=>t===s.length-1},Ft={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},_t={...Ft,...U},K=Symbol("*"),yt=(e,t)=>{const s=new Map,n={...e._options.searchOptions,...t};for(const[o,u]of e._documentIds){const i=n.boostDocument?n.boostDocument(u,"",e._storedFields.get(o)):1;s.set(o,{score:i,terms:[],match:{}})}return s},X=(e,t=M)=>{if(e.length===0)return new Map;const s=t.toLowerCase(),n=ht[s];if(!n)throw new Error(`Invalid combination operator: ${t}`);return e.reduce(n)||new Map},S=(e,t,s,n,o,u,i,r,d=new Map)=>{if(o==null)return d;for(const c of Object.keys(u)){const a=u[c],h=e._fieldIds[c],g=o.get(h);if(g==null)continue;let m=g.size;const p=e._avgFieldLength[h];for(const l of g.keys()){if(!e._documentIds.has(l)){ft(e,h,l,s),m-=1;continue}const f=i?i(e._documentIds.get(l),s,e._storedFields.get(l)):1;if(!f)continue;const y=g.get(l),F=e._fieldLength.get(l)[h],v=dt(y,m,e._documentCount,F,p,r),z=n*a*f*v,A=d.get(l);if(A){A.score+=z,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else d.set(l,{score:z,terms:[t],match:{[s]:[c]}})}}return d},At=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((l,f)=>({...l,[f]:G(n.boost,f)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:d}=n,{fuzzy:c,prefix:a}={...J.weights,...i},h=e._index.get(t.term),g=S(e,t.term,t.term,1,h,o,u,d);let m,p;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const l=t.fuzzy===!0?.2:t.fuzzy,f=l<1?Math.min(r,Math.round(t.term.length*l)):l;f&&(p=e._index.fuzzyGet(t.term,f))}if(m)for(const[l,f]of m){const y=l.length-t.term.length;if(!y)continue;p?.delete(l);const F=a*l.length/(l.length+.3*y);S(e,t.term,l,F,f,o,u,d,g)}if(p)for(const l of p.keys()){const[f,y]=p.get(l);if(!y)continue;const F=c*l.length/(l.length+y);S(e,t.term,l,F,f,o,u,d,g)}return g},Y=(e,t,s={})=>{if(t===K)return yt(e,s);if(typeof t!="string"){const a={...s,...t,queries:void 0},h=t.queries.map(g=>Y(e,g,a));return X(h,a.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:d}=i,c=r(t).flatMap(a=>d(a)).filter(a=>!!a).map(at(i)).map(a=>At(e,a,i));return X(c,i.combineWith)},Q=(e,t,s={})=>{const n=Y(e,t,s),o=[];for(const[u,{score:i,terms:r,match:d}]of n){const c=r.length||1,a={id:e._documentIds.get(u),score:i*c,terms:Object.keys(d),queryTerms:r,match:d};Object.assign(a,e._storedFields.get(u)),(s.filter==null||s.filter(a))&&o.push(a)}return t===K&&s.boostDocument==null&&e._options.searchOptions.boostDocument==null||o.sort(P),o},Ct=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Q(e,t,s)){const r=i.join(" "),d=n.get(r);d!=null?(d.score+=u,d.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:d}]of n)o.push({suggestion:u,terms:r,score:i/d});return o.sort(P),o};class Et{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(t?.fields==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?_t:t.autoVacuum;this._options={...mt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...pt,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const zt=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:d,serializationVersion:c},a)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const h=new Et(a);h._documentCount=t,h._nextId=s,h._documentIds=b(n),h._idToShortId=new Map,h._fieldIds=o,h._fieldLength=b(u),h._avgFieldLength=i,h._storedFields=b(r),h._dirtCount=d||0,h._index=new C;for(const[g,m]of h._documentIds)h._idToShortId.set(m,g);for(const[g,m]of e){const p=new Map;for(const l of Object.keys(m)){let f=m[l];c===1&&(f=f.ds),p.set(parseInt(l,10),b(f))}h._index.set(g,p)}return h},B=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,a=!1)=>{let h="";i===0?h=c.length>20?`… ${c.slice(-20)}`:c:a?h=c.length+i>100?`${c.slice(0,100-i)}… `:c:h=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,h&&o.push(h),i+=h.length,a||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let d=s.indexOf(n,u);if(d===-1)return null;for(;d>=0;){const c=d+n.length;if(r(e.slice(u,d)),u=c,i>100)break;d=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},wt=(e,t)=>t.contents.reduce((s,[,n])=>s+n,0)-e.contents.reduce((s,[,n])=>s+n,0),xt=(e,t)=>Math.max(...t.contents.map(([,s])=>s))-Math.max(...e.contents.map(([,s])=>s)),Z=(e,t,s={})=>{const n={};return Q(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...s}).forEach(o=>{const{id:u,terms:i,score:r}=o,d=u.includes("@"),c=u.includes("#"),[a,h]=u.split(/[#@]/),g=Number(a),m=i.sort((l,f)=>l.length-f.length).filter((l,f)=>i.slice(f+1).every(y=>!y.includes(l))),{contents:p}=n[g]??={title:"",contents:[]};if(d)p.push([{type:"customField",id:g,index:h,display:m.map(l=>o.c.map(f=>B(f,l))).flat().filter(l=>l!==null)},r]);else{const l=m.map(f=>B(o.h,f)).filter(f=>f!==null);if(l.length&&p.push([{type:c?"heading":"title",id:g,...c&&{anchor:h},display:l},r]),"t"in o)for(const f of o.t){const y=m.map(F=>B(f,F)).filter(F=>F!==null);y.length&&p.push([{type:"text",id:g,...c&&{anchor:h},display:y},r])}}}),V(n).sort(([,o],[,u])=>"max"==="total"?wt(o,u):xt(o,u)).map(([o,{title:u,contents:i}])=>{if(!u){const r=ut(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},tt=(e,t,s={})=>{const n=Ct(t,e,{fuzzy:.2,maxFuzzy:3,...s}).map(({suggestion:o})=>o);return e.includes(" ")?n:n.filter(o=>!o.includes(" "))},bt=et(V(JSON.parse("{\"/\":{\"documentCount\":174,\"nextId\":174,\"documentIds\":{\"0\":\"1\",\"1\":\"2\",\"2\":\"2#为什么ai需要分布式系统\",\"3\":\"2#什么是分布式系统\",\"4\":\"2#为什么ai训练需要分布式系统\",\"5\":\"2#分布式系统如何发挥作用\",\"6\":\"2#数据分布式的一些基本想法\",\"7\":\"2#数据并行-dp\",\"8\":\"2#典型数据并行的流程\",\"9\":\"2#horovod\",\"10\":\"2#ring-allreduce\",\"11\":\"2#reduce-scatter\",\"12\":\"2#all-gather\",\"13\":\"2#horovod工作\",\"14\":\"2#使用方法\",\"15\":\"2#pytorch-ddp\",\"16\":\"2#python前端api\",\"17\":\"2#梯度同步算法\",\"18\":\"2#传统做法\",\"19\":\"2#改进做法\",\"20\":\"2#集合通讯库\",\"21\":\"2#fsdp并行\",\"22\":\"2#zero\",\"23\":\"2#abstract\",\"24\":\"2#extended-introduction\",\"25\":\"2#where-did-all-the-memory-go\",\"26\":\"2#model-states-optimizer-states-gradients-and-parameters\",\"27\":\"2#residual-memory-consumption\",\"28\":\"2#temporary-buffers\",\"29\":\"2#memory-fragmentation\",\"30\":\"2#zero-insights-and-overview\",\"31\":\"2#insights-and-overview-zero-dp\",\"32\":\"2#insights-and-overview-zero-r\",\"33\":\"2#reducing-activation-memory\",\"34\":\"2#managing-temporary-buffers\",\"35\":\"2#managing-fragmented-memory\",\"36\":\"2#deep-dive-into-zero-dp\",\"37\":\"2#zero1\",\"38\":\"2#zero2\",\"39\":\"2#zero3\",\"40\":\"2#deep-dive-into-zero-r\",\"41\":\"2#partitioned-activation-checkpointing\",\"42\":\"2#constant-size-buffers\",\"43\":\"2#memory-defragmentation\",\"44\":\"2#zero-offload\",\"45\":\"2#背景\",\"46\":\"2#efficiency\",\"47\":\"2#unique-optimal-offload-strategy\",\"48\":\"2#zero-offload-schedule\",\"49\":\"2#单卡策略\",\"50\":\"2#多卡策略\",\"51\":\"2#流水线并行\",\"52\":\"2#gpipe\",\"53\":\"2#gpipe-abstract\",\"54\":\"2#design\",\"55\":\"2#naive-model-parallelism\",\"56\":\"2#pipeline-parallelism-1-split-into-micro-batches\",\"57\":\"2#pipeline-parallelism-2-re-materialization\",\"58\":\"2#张量并行-tp\",\"59\":\"2#megatron-lm\",\"60\":\"2#model-parallel-transformers\",\"61\":\"2#_3d并行\",\"62\":\"2#贡献\",\"63\":\"2#数据并行\",\"64\":\"2#考虑流水线并行\",\"65\":\"2#考虑张量并行\",\"66\":\"2#性能分析\",\"67\":\"2#自动并行\",\"68\":\"2#flexflow\",\"69\":\"2#简介\",\"70\":\"2#概述\",\"71\":\"2#执行模拟器\",\"72\":\"2#执行优化器\",\"73\":\"2@0\",\"74\":\"2@1\",\"75\":\"3\",\"76\":\"3#序列并行\",\"77\":\"3#megatron\",\"78\":\"3#abstract\",\"79\":\"3#activation-memory\",\"80\":\"3#tensor-parallel\",\"81\":\"3#sequence-parallel\",\"82\":\"3#deepspeed-ulysses\",\"83\":\"3#简介\",\"84\":\"3#deepspeed-ulysses的核心设计\",\"85\":\"3#sequence-parallelism\",\"86\":\"3#论文背景\",\"87\":\"3#主要工作\",\"88\":\"3#ring-attention\",\"89\":\"3#背景\",\"90\":\"3#ring-attention-1\",\"91\":\"3#distflashattn\",\"92\":\"3#背景-1\",\"93\":\"3#论文方法\",\"94\":\"3#token-level-workload-imbalance\",\"95\":\"3#prohibitive-communication-overhead\",\"96\":\"3#loadbalanced-scheduling-with-communication-and-computation-overlap\",\"97\":\"3#典型并行方式时序图\",\"98\":\"3#tp\",\"99\":\"3#dp\",\"100\":\"3#pp\",\"101\":\"3@0\",\"102\":\"3@1\",\"103\":\"4\",\"104\":\"4@0\",\"105\":\"4@1\",\"106\":\"5\",\"107\":\"5#prog1-mandelbort-threads\",\"108\":\"5#环境配置\",\"109\":\"5#任务分析\",\"110\":\"5#任务实现\",\"111\":\"5@0\",\"112\":\"5@1\",\"113\":\"6\",\"114\":\"6#序列并行\",\"115\":\"6#megatron\",\"116\":\"6#abstract\",\"117\":\"6#activation-memory\",\"118\":\"6#tensor-parallel\",\"119\":\"6#sequence-parallel\",\"120\":\"6#deepspeed-ulysses\",\"121\":\"6#简介\",\"122\":\"6#deepspeed-ulysses的核心设计\",\"123\":\"6#sequence-parallelism\",\"124\":\"6#论文背景\",\"125\":\"6#主要工作\",\"126\":\"6#ring-attention\",\"127\":\"6#背景\",\"128\":\"6#ring-attention-1\",\"129\":\"6#distflashattn\",\"130\":\"6#背景-1\",\"131\":\"6#论文方法\",\"132\":\"6#token-level-workload-imbalance\",\"133\":\"6#prohibitive-communication-overhead\",\"134\":\"6#loadbalanced-scheduling-with-communication-and-computation-overlap\",\"135\":\"6@0\",\"136\":\"6@1\",\"137\":\"7\",\"138\":\"7#题目引入\",\"139\":\"7#题目分析\",\"140\":\"7#斯特林公式\",\"141\":\"7#代码实现\",\"142\":\"7#总结\",\"143\":\"7@0\",\"144\":\"7@1\",\"145\":\"8\",\"146\":\"8#文生图\",\"147\":\"8#distrifusion\",\"148\":\"8#特点\",\"149\":\"8#以往方法\",\"150\":\"8#相关工作\",\"151\":\"8#背景知识\",\"152\":\"8#本文方法\",\"153\":\"8#位移补丁并行\",\"154\":\"8#总结\",\"155\":\"8#pipefusion\",\"156\":\"8#摘要\",\"157\":\"8#问题\",\"158\":\"8#本文方法-1\",\"159\":\"8#xdit\",\"160\":\"8#overview\",\"161\":\"8#dit-主干网络混合并行\",\"162\":\"8#parallel-vae\",\"163\":\"8#简单灵活的开发接口\",\"164\":\"8#pipefusion-1\",\"165\":\"8#usp-混合序列并行\",\"166\":\"8#cfg-parallel\",\"167\":\"8#hybrid-parallel\",\"168\":\"8#parallel-vae-1\",\"169\":\"8@0\",\"170\":\"8@1\",\"171\":\"9\",\"172\":\"10\",\"173\":\"11\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,8],\"1\":[2],\"2\":[1],\"3\":[1,13],\"4\":[1,24],\"5\":[1],\"6\":[1,83],\"7\":[3],\"8\":[1,16],\"9\":[1,31],\"10\":[2,8],\"11\":[2,14],\"12\":[2,3],\"13\":[1,22],\"14\":[1,62],\"15\":[2,18],\"16\":[1,64],\"17\":[1,3],\"18\":[1,5],\"19\":[1,32],\"20\":[1,28],\"21\":[1],\"22\":[1,18],\"23\":[1,15],\"24\":[2,24],\"25\":[6,8],\"26\":[6,41],\"27\":[3],\"28\":[2,3],\"29\":[2,6],\"30\":[4,2],\"31\":[5,17],\"32\":[5],\"33\":[3,11],\"34\":[3,2],\"35\":[4,2],\"36\":[5],\"37\":[1,4],\"38\":[1,4],\"39\":[1,13],\"40\":[5],\"41\":[3,12],\"42\":[3,3],\"43\":[2,4],\"44\":[2,14],\"45\":[1,16],\"46\":[1,34],\"47\":[4,52],\"48\":[3],\"49\":[1,103],\"50\":[1,40],\"51\":[1],\"52\":[1,16],\"53\":[2,21],\"54\":[1],\"55\":[3,44],\"56\":[7,13],\"57\":[5,8],\"58\":[3],\"59\":[2,20],\"60\":[3,137],\"61\":[1,21],\"62\":[1,20],\"63\":[1,7],\"64\":[1,28],\"65\":[1,2],\"66\":[1,48],\"67\":[1],\"68\":[1,28],\"69\":[1,39],\"70\":[1,46],\"71\":[1,11],\"72\":[1,22],\"73\":[null,null,1],\"74\":[null,null,3],\"75\":[4],\"76\":[1],\"77\":[1,13],\"78\":[1,21],\"79\":[2,92],\"80\":[2,20],\"81\":[2,81],\"82\":[2,28],\"83\":[1,28],\"84\":[2,29],\"85\":[2,12],\"86\":[1,7],\"87\":[1,14],\"88\":[2,14],\"89\":[1,3],\"90\":[2,55],\"91\":[1,6],\"92\":[1,5],\"93\":[1,4],\"94\":[4,16],\"95\":[3,10],\"96\":[7,8],\"97\":[1],\"98\":[1],\"99\":[1],\"100\":[1],\"101\":[null,null,1],\"102\":[null,null,3],\"103\":[2],\"104\":[null,null,1],\"105\":[null,null,2],\"106\":[3],\"107\":[3],\"108\":[1,38],\"109\":[1,46],\"110\":[1,51],\"111\":[null,null,2],\"112\":[null,null,3],\"113\":[4],\"114\":[1],\"115\":[1,13],\"116\":[1,21],\"117\":[2,92],\"118\":[2,20],\"119\":[2,81],\"120\":[2,28],\"121\":[1,28],\"122\":[2,29],\"123\":[2,12],\"124\":[1,7],\"125\":[1,14],\"126\":[2,14],\"127\":[1,3],\"128\":[2,55],\"129\":[1,6],\"130\":[1,5],\"131\":[1,4],\"132\":[4,16],\"133\":[3,10],\"134\":[7,8],\"135\":[null,null,1],\"136\":[null,null,3],\"137\":[2],\"138\":[1,23],\"139\":[1,52],\"140\":[1,45],\"141\":[1,33],\"142\":[1,1],\"143\":[null,null,1],\"144\":[null,null,2],\"145\":[4],\"146\":[1,24],\"147\":[1,29],\"148\":[1,5],\"149\":[1,18],\"150\":[1,27],\"151\":[1,48],\"152\":[1,28],\"153\":[1,45],\"154\":[1,12],\"155\":[1,10],\"156\":[1,5],\"157\":[1,12],\"158\":[1,16],\"159\":[1,69],\"160\":[1],\"161\":[2,13],\"162\":[2,16],\"163\":[1],\"164\":[1,1],\"165\":[2,5],\"166\":[2,23],\"167\":[2,76],\"168\":[2,24],\"169\":[null,null,1],\"170\":[null,null,3],\"171\":[1,3],\"172\":[1],\"173\":[1]},\"averageFieldLength\":[1.860651418843633,23.702859670752236,0.22585222531762375],\"storedFields\":{\"0\":{\"h\":\"个人介绍\",\"t\":[\"计算机大二本科小白，\",\"主攻CV/NLP等AI方向，\",\"同时对分布式开发、并行运算感兴趣，\",\"算法还是0基础新人，\",\"路漫漫其修远兮。\"]},\"1\":{\"h\":\"MLSys_分布式开发\"},\"2\":{\"h\":\"为什么AI需要分布式系统\"},\"3\":{\"h\":\"什么是分布式系统\",\"t\":[\"分布式系统顾名思义，就是将单一计算机节点要做的任务分布在多个计算机节点上完成的，各个节点之间通过网络进行通讯的计算机系统。\",\"这种系统的好处是显而易见的，我们可以把一台机器的Task进行切分，可能大幅度提升计算效率；我们可以把一台机器存不下的任务放到多个结点里面，拓展数据规模。\",\"但同时也可能引入更多的问题，多台机器之间通讯耗费的时间会不会比原先计算的时间更长？切分后的任务如何再将结果重新合在一起？这些都问题都有进一步研究的价值。\"]},\"4\":{\"h\":\"为什么AI训练需要分布式系统\",\"t\":[\"在训练AI时，无论在CV方向还是NLP等其他方向，无论使用哪一种模型，总是绕不开训练集和测试集这两部分，我们总要拿出一部分数据用来预训练模型，另一部分用来测试模型效果。\",\"数据集越大，训练效果越好，这基本上是在不考虑机器性能上限的情况下，我们针对AI训练达成的一种共识，所以在训练AI时我们希望能使用尽可能大的数据集，使用尽可能多的参数，来对尽可能多的标签进行刻画。但可惜一台机器的性能总是有限的。\",\"这个时候就不得不引入分布式系统来改善这种局面了，如果我们一台机器、一张GPU/TPU没办法高效完成我们的运算，那我们可以分到多张卡上面；如果我们一台机器存储不开我们的数据集，那我们可以分到多台设备上，或者考虑让CPU也存储一部分模型数据。这就是AI使用分布式系统想要解决的问题。\"]},\"5\":{\"h\":\"分布式系统如何发挥作用\"},\"6\":{\"h\":\"数据分布式的一些基本想法\",\"t\":[\"如果我们希望分布式系统在AI训练中发挥他的力量，我们要怎么做呢？大家都对ML有一些基本的认识了，无论哪一种模型，在训练过程中总会有数据（输入、输出），模型（参数、激活函数）等等需要保存的东西。\",\"现在假定我们要做一个训练任务，我们有我们自己的很大的数据集，我希望能把这个数据很快的训练完，那我们就可以考虑把数据切分成好几份，然后给不同的GPU去算每一个单独的部分，这种每个一份数据切分成多份，给不同的GPU做计算的方式，但每一个GPU均做保留完整的模型做计算，被我们称作数据并行（Data Patallel，简称DP）。\",\"既然可能有很大的数据集需要切分，那自然也可能有很大的模型进行切分，让每一个GPU仅保留几个隐藏层的模型（参数、激活函数），这样可以训练更大的模型，提升训练精度，这种按层切分模型，不切分数据的方式，被称作模型并行（Model Patallel，简称MP）。\",\"这种按层切分的方式固然可以增加可容纳的模型的大小，但是仅让一个卡存模型的几层在计算某些必须要用到之前的数据的模型时可能不尽如人意，通讯成本会比较高昂。\",\"为了解决层切分的弊端，我们可以考虑将计算过程中的算子/计算矩阵进行切分，让每一张卡只保留必须的参数和算子，产生部分输出，这样就可以将每一部分计算矩阵进行并发处理，我们将这种方式称作张量并行/算子并行（Tensor Patallel，简称TP），谷歌专门为TP开发了TPU进行并发适配，TP也是应用较广的基本并发方式。\",\"既然我们有了对数据切分的方法，有了对模型算子切分的方法，那我们也可以考虑进行结合，既切分优化器状态，又切分模型的参数、梯度，这种并行方式被称作完全分片数据并行（Fully Sharded Data Parallel，简称FSDP）。\",\"前面所提及的方法，是在利用切分层内数据/优化器状态/模型绕过MP方法按层切分时可能带来的通讯瓶颈，但是也可以利用类似CPU指令流水线执行的方式，进行数据计算，切分模型让不同GPU延后计算开始时间，从而保证通讯无boundary，这种方式被称作流水线并行（Pipe Parallel，简称PP）。\",\"在探讨了DP、PP、TP基本并行方式后，我们可以考虑将三种并行方式进行综合，考虑一些综合利用并行方式的策略，这种并行考量被称为3D并行。\",\"事实上对于采用哪种并行模式，要用多少张卡进行并行，并行中使用的参数如何调整，是一件非常复杂的事情，我们期望有可以自动为我们选定并行方法的策略，这种策略被称作自动并行。\"]},\"7\":{\"h\":\"数据并行（DP）\"},\"8\":{\"h\":\"典型数据并行的流程\",\"t\":[\"分配n块计算GPU（图中0-2）;1块梯度收集GPU\",\"每块GPU均拷贝一份完整的模型\",\"把一份Data（也可以是一个batch）切分成若干份给不同的GPU\",\"每一块GPU完成Forward和Backward后，计算出本地的梯度\",\"把本地梯度push到梯度收集GPU，梯度收集GPU聚合梯度\",\"计算GPU从聚合GPU中pull完整梯度，更新模型参数，保证各个计算GPU模型同步\",\"聚合再下发梯度操作被称为AllReduce\"]},\"9\":{\"h\":\"Horovod\",\"t\":[\"Horovod: fast and easy distributed deep learning in TensorFlow\",\"https://arxiv.org/abs/1802.05799https://github.com/uber/horovod\",\"传统的DP在带来使用大数据集可能的同时，又同时增加了额外的通讯开销，本文还指出DP代码重构成本较大。\",\"本文通过在不同数量卡上训练结果进行说明：\",\"可以明显看到GPU数量增多时，TensorFlow框架下的通讯开销越大，在128卡时甚至已经超过训练开销。\",\"为了解决传统DP计算节点和聚合节点比例不好确定导致的通讯成本/计算成本过大的问题，本文指出了ring-allreduce的方式。\"]},\"10\":{\"h\":\"Ring-AllReduce\",\"t\":[\"假设有4块GPU，每块GPU上的数据也对应被切成4份。AllReduce就是要让每块GPU上的数据都变成箭头右边汇总的样子。\",\"Ring-AllReduce将这个过程分为Reduce-Scatter和All-Gather。\"]},\"11\":{\"h\":\"Reduce-Scatter\",\"t\":[\"定义网络拓扑关系，使得每个GPU只和其相邻的两块GPU通讯。每次发送对应位置的数据进行累加。每一次累加更新都形成一个拓扑环。\",\"3次更新之后，每块GPU上都有一块数据拥有了对应位置完整的聚合（图中红色）。此时，Reduce-Scatter阶段结束。进入All-Gather阶段。目标是把红色块的数据广播到其余GPU对应的位置上。\"]},\"12\":{\"h\":\"All-Gather\",\"t\":[\"相邻GPU对应位置进行通讯，对应位置数据不再做相加，而是直接替换\"]},\"13\":{\"h\":\"Horovod工作\",\"t\":[\"将百度的 TensorFlow ring-allreduce 算法的实现转化为一个独立的 Python 包，命名为 Horovod\",\"使用 NCCL 库实现了 TensorFlow ring-allreduce，并优化了性能\",\"添加了对单机多卡的支持\",\"改进了 API，添加 broadcast 操作，仅需 4 步即可使用 Horovod\"]},\"14\":{\"h\":\"使用方法\",\"t\":[\"import tensorflow as tf import horovod.tensorflow as hvd # 初始化 Horovod hvd.init() # 固定 GPU 以处理本地 rank（每个进程一个 GPU） config = tf.ConfigProto() config.gpu_options.visible_device_list = str(hvd.local_rank()) # 构建模型... loss = ... opt = tf.train.AdagradOptimizer(0.01) # 添加 Horovod 分布式优化器 opt = hvd.DistributedOptimizer(opt) # 添加hook，在初始化期间将变量从 rank 0 广播到所有其他进程 hooks = [hvd.BroadcastGlobalVariablesHook(0)] # 创建训练操作 train_op = opt.minimize(loss) # MonitoredTrainingSession 负责会话初始化、从检查点恢复、保存到检查点以及在完成或发生错误时关闭 with tf.train.MonitoredTrainingSession(checkpoint_dir=\\\"/tmp/train_logs\\\", config=config, hooks=hooks) as mon_sess: while not mon_sess.should_stop(): # 执行同步训练 mon_sess.run(train_op)\"]},\"15\":{\"h\":\"PyTorch DDP\",\"t\":[\"PyTorch Distributed: Experiences on Accelerating Data Parallel Training\",\"https://arxiv.org/abs/2006.15704https://github.com/pytorch/pytorch/\"]},\"16\":{\"h\":\"Python前端API\",\"t\":[\" 1 import torch 2 import torch.nn as nn 3 import torch.nn.parallel as par 4 import torch.optim as optim 5 6 # initialize torch.distributed properly 7 # with init_process_group 8 9 # setup model and optimizer 10 net = nn.Linear(10, 10) 11 net = par.DistributedDataParallel(net) 12 opt = optim.SGD(net.parameters(), lr=0.01) 13 14 # run forward pass 15 inp = torch.randn(20, 10) 16 exp = torch.randn(20, 10) 17 out = net(inp) 18 19 # run backward pass 20 nn.MSELoss()(out, exp).backward() 21 22 # update parameters 23 opt.step()\",\"PyTorch在设计API时做到了仅调用第11行代码中的DistributedDataParallel部分，就可以实现从本机训练到分布式训练的部署。\"]},\"17\":{\"h\":\"梯度同步算法\",\"t\":[\"论文中所提到的DP背景本篇blog前文均有提及，不再赘述。\"]},\"18\":{\"h\":\"传统做法\",\"t\":[\"batch较小时，通讯效率低\",\"计算与聚合之间存在间隔，很难做到即时通讯。\"]},\"19\":{\"h\":\"改进做法\",\"t\":[\"PyTorch使用Gradient Bucketing，在小batch时，选择收集到一定量的梯度，再做聚合和同步。\",\"PyTorch使用hook机制，在反向传播计算完成后，调用自定义函数，当在同一个bucket中的梯度的hook都被调用后，就调用AllReduce对该bucket进行通信。\",\"这种方式有两个问题需要注意：\",\"（1）由于每个机器（进程）是独立计算的，因此不同机器之间处理的bucket的顺序将会不一致，这会导致梯度同步结果出现错误。因此，我们需要保证不同机器处理bucket的顺序一致。\",\"使用参数的反序作为梯度放入bucket的顺序。依据是，后向传播的顺序与梯度更新的顺序大致可认为是相同的。\",\"（2）不同迭代中，使用的参数可能不相同，使得某些参数的梯度不需要用到。\",\"前向传播结束后从输出开始遍历计算图，记录哪些参数参与计算，哪些参数没有参与计算，对于没有参与计算的参数，则直接标记为ready。\"]},\"20\":{\"h\":\"集合通讯库\",\"t\":[\"PyTorch DDP支持三种通讯库：NCCL，Gloo和MPI。DDP支持用户使用统一的API ProcessGroup来调用不同的集合通讯库。\",\"NCCL doc:https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api.html\",\"Gloo doc:https://docs.solo.io/gateway/latest\",\"MPI doc:https://www.open-mpi.org/doc/\"]},\"21\":{\"h\":\"FSDP并行\"},\"22\":{\"h\":\"ZeRO\",\"t\":[\"ZeRO: memory optimizations toward Training trillion parameter Models\",\"https://github.com/microsoft/DeepSpeedhttps://arxiv.org/abs/1910.02054\"]},\"23\":{\"h\":\"Abstract\",\"t\":[\"深度学习模型在训练万亿级别参数时存在根本性限制，现有DP（数据并行）、MP（模型并行）方法无法满足日益增长的参数量需求，ZeRO（零冗余优化器）消除了数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度（计算与通信比率的定量或定性度量），使我们能够根据设备数量按比例扩展模型大小，同时保持高效率，实现超线性优化。\"]},\"24\":{\"h\":\"Extended Introduction\",\"t\":[\"巨量参数可以大幅提升NLP处理能力，但参数量的上升在传统的单机GPU、TPU运算中无以为继，简单增加机器数量也用处不大。\",\"现有的PP（流水线并行）、MP都在通信和计算效率之间权衡，但重点是计算规模和速度。\",\"现有系统在模型训练中内存消耗的全部范围，并将其分为两部分:\",\"1）对于大型模型，【大部分内存】被模型状态占用，包括优化器状态、梯度和参数（不得不保存的模型内容）。\",\"2）剩余内存被激活、临时缓冲区和不可用的碎片内存所消耗，本文将其统称为残差状态（residual states）。\"]},\"25\":{\"h\":\"Where Did All the Memory Go\",\"t\":[\"在模型计算的过程中，只有很少一部分被过程中产生的冗余量使用了（residual states），大部分内存都被用于模型本身数据的存储和运算（model states），ZeRO使用不同的方式优化这两种内存使用。\"]},\"26\":{\"h\":\"Model States: Optimizer States, Gradients and Parameters\",\"t\":[\"论文以Adam优化器为例说明了模型造成的内存浪费是一件难以接受的事情，Adam本身对每一个参数都需要保留momentum和variance两个参数进行数据更新。\",\"看上去这是仅仅由1变2的内存保留变化，但是由于NVIDIA对于fp16精度计算的高优化度，要服务于计算速度内存变化就会发生膨胀。\",\"一般来说，在计算时，参数、输入、输出，均采用fp16半精度运算，但是考虑在更新权重的时候，可能在模型训练过程中梯度不大等原因，如果还是使用半精度进行运算，可能并不会产生权重累计，导致模型训练失效，所以此时应采用fp32的全精度计算，这又是一个仅在计算时才能用到的额外copy。\",\"FP32:1位符号位，8位指数位，23位尾数位 FP16:1位符号位，5位指数位，10位尾数位 BF16:1位符号位，8位指数位，7位尾数位\",\"假设有Ψ个参数，那就有4Ψ个byte（fp16）存储模型的参数和输入，同时又有12Ψ个byte（fp32）存储momentum和variance（Adam），也就是单计算仅需4Ψ，但是在更新参数进行新的计算时，需要额外的12Ψ，本文将其记作2Ψ+2Ψ+KΨ，其中K取决于模型。\"]},\"27\":{\"h\":\"Residual Memory Consumption\"},\"28\":{\"h\":\"Temporary buﬀers\",\"t\":[\"是指通讯过程中、计算过程中产生的一些临时数据.\"]},\"29\":{\"h\":\"Memory Fragmentation\",\"t\":[\"是指碎片化内存，用pytorch等使用虚拟内存分配方式的库时，可能内存池的维护并不能做到完全利用，论文假设有30%内存其实根本无法使用。\"]},\"30\":{\"h\":\"ZeRO: Insights and Overview\",\"t\":[\"这一部分主要介绍作者使用ZeRO的一些想法。\"]},\"31\":{\"h\":\"Insights and Overview: ZeRO-DP\",\"t\":[\"a）数据并行比模型并行更好，效率更高，因为通讯更少，计算粒度更精细\",\"b）数据并行内存使用并不高效，因为每一个分发计算都需要完全copy所有数据\",\"c）两种并行都需要存储模型状态变量，单就这一块部分内存而言，两者使用均不高效。\",\"我们可以考虑在某一个GPU中存储和更新参数，而在其他GPU需要使用其进行计算时，再进行通讯获取参数，从而降低内存占用。\"]},\"32\":{\"h\":\"Insights and Overview: ZeRO-R\"},\"33\":{\"h\":\"Reducing Activation Memory\",\"t\":[\"a）MP占据模型内存，但是需要时常更新\",\"b）大模型即使的带宽小的情况下，也可以考虑每个GPU都各自保存和重算部分数据\",\"ZeRO考虑可以使用将模型内存切分成多分，分开重算，多次通讯的方式收集数据，减少内存使用。\"]},\"34\":{\"h\":\"Managing Temporary buﬀers\",\"t\":[\"对于临时缓存采用开一段固定大小内存的方式进行反复存储。\"]},\"35\":{\"h\":\"Managing fragmented Memory.\",\"t\":[\"内存碎片通过对不同寿命的内存进行整理，减少内存释放和分配的时间\"]},\"36\":{\"h\":\"Deep Dive into ZeRO-DP\"},\"37\":{\"h\":\"ZeRO1\",\"t\":[\"对优化器本身存储的fp32数据进行切分，使每个GPU仅留1/N份数据。\"]},\"38\":{\"h\":\"ZeRO2\",\"t\":[\"对数据并行中存储的fp16梯度进行切分，使每个GPU仅留1/N份数据。\"]},\"39\":{\"h\":\"ZeRO3\",\"t\":[\"对数据并行中存储的fp16参数进行切分，使每个GPU仅留1/N份数据，使用时通讯获取完整参数。\",\"ZeRO1/2依托allreduce算法实现，NVIDIA本身支持这种优化并不会有通讯增加，但是ZeRO3需要对参数进行切分和更新，每次都会有Ψ 的额外通讯开销，但是事实上可以考虑在不同隐藏层中实现异步更新参数 ，使得参数额外开销尽可能减少。\"]},\"40\":{\"h\":\"Deep Dive into ZeRO-R\"},\"41\":{\"h\":\"Partitioned Activation Checkpointing\",\"t\":[\"采用Megatron的模型并行方式，每个GPU保存1/N参数，对切分后部分输入分开运算，使用reduce-scatter更新各个参数状态，与ZeRO-DP的区别是，计算参数时，每个GPU都没有保存或通讯获取完整的参数，而是对输出进行通讯和更新。\"]},\"42\":{\"h\":\"Constant Size Buﬀers\",\"t\":[\"使用固定大小buffer指定数据的单批发送量，保证带宽不浪费。\"]},\"43\":{\"h\":\"Memory Defragmentation\",\"t\":[\"将数据池分为两部分，一部分存储大批量的计算数据，另一部分动态存储临时数据。\"]},\"44\":{\"h\":\"ZeRO-Offload\",\"t\":[\"ZeRO-Offload：Democratizing Billion-Scale Model Training\",\"https://arxiv.org/pdf/2101.06840https://arxiv.org/pdf/2101.06840\"]},\"45\":{\"h\":\"背景\",\"t\":[\"GPU内存占用是一件非常昂贵的事情，在过去训练中人们往往忽略了CPU的计算潜力，高估了GPU的存储性能。\",\"ZeRO-Offload 优化：尽量减少数据在 GPU 与 CPU 之间的移动，并减少 CPU 计算时间，同时最大限度地节省 GPU 上的内存。\"]},\"46\":{\"h\":\"Efficiency\",\"t\":[\"论文提出了一种名为Efficiency的offload策略，通过分析确定了CPU和GPU设备之间的最佳计算和数据划分策略，以在三个关键方面达到最优化：\",\"在CPU上的计算量比GPU少多个数量级，防止CPU性能瓶颈；\",\"最小化CPU和GPU之间的通信量，防止通信瓶颈；\",\"在实现最小通信量的同时，可证明地最大化节约GPU内存。\",\"offload 优化器计算要求CPU进行O(M)次计算，而GPU需进行O(MB)次计算，其中M和B分别为模型规模和 batch size 。在大多数情况下， batch size 较大，CPU计算量并不是瓶颈，但对于小 batch size，CPU计算量可能成为瓶颈。为了解决这个问题，采取了两种优化措施：\",\"高效的CPU优化器，其速度比现有技术快6倍；\",\"延迟一步的参数更新，允许将CPU优化器步骤与GPU计算重叠，同时确保准确性。这两种措施共同保证了ZeRO-Offload在小 batch size 下也能保持效率。\"]},\"47\":{\"h\":\"Unique Optimal Offload Strategy\",\"t\":[\"为了确定最佳的下载策略，ZeRO-Offload将深度学习训练建模为数据流图，将该图分割为CPU和GPU设备之间的部分。\",\"训练的计算复杂度通常为O(MB)，其中M为模型大小，B为有效batch size。为避免CPU计算成为瓶颈，只有那些计算复杂度低于O(MB)的计算才能转移到CPU上\",\"FWD 和 BWD 的计算复杂度都是O(MB)，必须在GPU上进行，而其余的计算，如范数计算、权重更新等，其复杂度为O(M)，可以转移到CPU上\",\"还需要最小化 CPU 与 GPU 的通信带宽，如图中所示，最小通信量为 BWD后 GPU 发送到 CPU 的 2M 梯度与 CPU 发送到 GPU 的 2M 参数，只有将 fp32 模型状态（momentum 32、variance 32和p32），Param Update 和 float2half 计算放置在一起，为一个 CPU 上的 Update Super Node，才能达成最小通信量策略\"]},\"48\":{\"h\":\"ZeRO-Offload Schedule\"},\"49\":{\"h\":\"单卡策略\",\"t\":[\"ZeRO-Offload将数据进行分区，将fp16参数存储在GPU上，fp16梯度和所有优化器状态存储在CPU上。\",\"在训练过程中，首先通过 FWD 计算损失。由于fp16参数已经位于GPU上，因此这部分计算不需要与CPU进行通信。\",\"在 BWD 过程中，不同参数的梯度在后向调度的不同位置计算。ZeRO-Offload可以立即将这些梯度逐个或切分传输到 CPU 内存中。\",\"因此，在将梯度传输到CPU内存之前，只需要在GPU内存中临时保存少量的梯度。此外，每个梯度传输可以与反向计算重叠，消除大部分通信成本。\",\"在 BWD 之后，ZeRO-Offload在CPU上直接更新fp32参数和剩余的优化器状态，并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存中的fp16参数中。\",\"下图GPU与CPU二次通信应该是从CPU到GPU\",\"for_parallel rank in range(world_size): # 初始化每个进程的层 initialize_layers() for batch in dataset: # 前向传播 x = forward(batch) # 计算损失并反向传播 compute_loss(x, batch).backward() # 反向传播梯度 backward(x.grad) # 更新参数 step() def _is_owner(i): # 判断当前进程是否拥有第 i 层 return True if rank owns i else False def initialize_layers(): for i in range(num_layers): l = layers[i] # 在 GPU 上分配半精度参数 allocate_on_gpu l.param_fp16 if _is_owner(i): # 在 CPU 上分配全精度参数、优化器状态和梯度 allocate_on_cpu l.param_fp32 allocate_on_cpu l.optim_states_fp32 allocate_on_cpu l.param_grad def forward(x): # 前向传播逻辑 for i in range(num_layers): x = layers[i].forward(x) return x def backward(dx): # 反向传播逻辑 for i in range(num_layers, 0, -1): dx = layers[i].backward(dx) # 将梯度减少到拥有该层的进程 reduce(layers[i].grad, dest_rank = _owner_rank(i)) if _is_owner(i): # 将梯度复制到 CPU l.cpu_grad.copy(l.grad) else: pass # 删除 GPU 上的梯度 del layers[i].grad def step(): # 参数更新逻辑 for i in range(num_layers): l = layers[i] if _is_owner(i): # 在 CPU 上更新参数 update_in_cpu(l.optim_states_fp32, l.cpu_grad, l.param_fp32) # 将更新后的参数复制回 GPU l.param_fp16.copy(l.param_fp32) # 广播更新后的参数 BROADCAST(l.param_fp16, src = _owner_rank(i))\"]},\"50\":{\"h\":\"多卡策略\",\"t\":[\"ZeRO-Offload 将梯度和优化器状态在不同的 GPU 之间进行 partition，并且每个 GPU 将自己的 part offload 到 CPU 内存中，存储持续整个训练过程\",\"BWD 过程中，在 GPU 上 reduce-scatter 计算梯度并平均，每个 GPU 仅将属于其 part 的平均梯度 offload 到 CPU 内存中\",\"一旦梯度在 CPU 上可用，优化器状态 part 对应的每个 DP 进程直接在 CPU 上并行更新对应的参数 part\",\"更新完成后，参数 part 发送到 GPU，在 GPU 上对参数进行类似 ZeRO-2 的 all-gather 操作\"]},\"51\":{\"h\":\"流水线并行\"},\"52\":{\"h\":\"GPipe\",\"t\":[\"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism\",\"https://arxiv.org/abs/1811.06965\"]},\"53\":{\"h\":\"Gpipe Abstract\",\"t\":[\"增大模型规模可以提升模型效果，但是单卡/单机GPU内存有限，必须引入分布式系统\",\"GPipe使用模型并行（MP）方案，将模型切分成一连串stage，每个stage放在独立的设备（GPU/TPU）上，实现对超大规模模型的支持\",\"利用Pipeline（PP）的方案，提高了模型并行模式下的设备利用率\",\"最终GPipe通过更大规模的模型和更大的batch_size，在图像和NLP的模型上都得到更好的模型效果。\"]},\"54\":{\"h\":\"Design\"},\"55\":{\"h\":\"Naive Model Parallelism\",\"t\":[\"论文中提到的MP就是传统的MP方案，不是Magatron-LM提出的新MP方案（事实上是TP），对数据按层切分，而非对所有输入、输出、参数都按块切分。\",\"模型有 12 层（layer），可以切为 4 份（4个cell），每份 3 层。然后每份放到一块 GPU 上\",\"第 k 个 cell 的模型参数，就放在第 k 块 GPU 上。\",\"Fk和 Bk 分别表示第 k 个 cell 的 forward 和 backward 计算\",\"单批量以这种顺序进行计算，Fk和 Bk 分别表示第 k 个批次的forward和backward运算（注意与上张图不同），每一种颜色代表一块 GPU，每一列代表一个时间段\",\"问题是：每块 GPU 都会有大量的空闲时间\"]},\"56\":{\"h\":\"Pipeline Parallelism 1 - Split into micro-batches\",\"t\":[\"单batch（mini-batch）切成更小的micro-batch，然后流水线并行（类似CPU指令执行）\",\"为什么不流水线并行batch，而是切分后再流水线并行？ 多batch可以提速，但占用空间会多很多 多batch训练时，可能会导致梯度更新不稳定，结果收敛不明显\"]},\"57\":{\"h\":\"Pipeline Parallelism 2 - re-materialization\",\"t\":[\"GPU 只保留最开始的输入，中间结果全部丢掉；计算梯度时，再重新计算这些中间结果。\",\"减少计算时内存占用，但要增加计算时长。\"]},\"58\":{\"h\":\"张量并行（TP）\"},\"59\":{\"h\":\"Megatron-LM\",\"t\":[\"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\",\"https://arxiv.org/abs/1909.08053https://github.com/NVIDIA/Megatron-LM\"]},\"60\":{\"h\":\"Model Parallel Transformers\",\"t\":[\"在MLP中最常用的操作是MM + GELU，即输入与参数矩阵相乘 和 激活函数\",\"一种方法是沿着其行(row)将weight矩阵A分割，并沿着其列(columns)输入X分割，来实现tensor-model-parallel，从下图中我们可以看出，在该方法下，需要通过同步来保障语义对等。\",\"另一种方法是沿着它的列(columns)分割A，这种方法的好处是保障了各自在独立计算时的语义对等，不需要进行额外通讯\",\"不过由于我们使用的A是同一个，在反向传播时要保证在保存不同切分W的GPU中均及时更新A，在PyTorch中可以通过下面的代码简单实现：\",\"class f(torch.autograd.Function): def forward(ctx, x): return x def backward(ctx, gradient): all_reduce(gradient) return gradient\",\"通过结合上述两种方法，可以在transformer中实现简单并发，针对MLP、SA两个环节做了如下优化，实现MLP的整体的tensor-model-parallel且语义和原始MLP对等：\",\"在优化 embedding 层时，PyTorch 将 embedding 操作视为对输入进行索引操作，即在权重矩阵上执行 index_select 操作。实际上，这个操作等价于先对输入进行 one-hot 编码，然后与权重矩阵进行矩阵乘法（mm）操作。可以将 embedding 层视为线性层来处理，并使用模型并行操作来优化。\",\"class VocabParallelEmbedding(): def __init__(self, num_embeddings, embedding_dim, init_method=init.xavier_normal_): super(VocabParallelEmbedding, self).__init__() ... # 通过获取当前 tensor_model_parallel 的 rank 来确定当前卡要 embedding 的 category id，初始化权重 self.vocab_start_index, self.vocab_end_index = VocabUtility.vocab_range_from_global_vocab_size( self.num_embeddings, get_tensor_model_parallel_rank(), self.tensor_model_parallel_size) self.num_embeddings_per_partition = self.vocab_end_index - self.vocab_start_index self.weight = Parameter(torch.empty( self.num_embeddings_per_partition, self.embedding_dim, dtype=args.params_dtype)) ... def forward(self, input_): if self.tensor_model_parallel_size > 1: # 把当前卡上不要的 category idx mask 掉 input_mask = (input_ < self.vocab_start_index) | \\\\ (input_ >= self.vocab_end_index) # Mask 掉的输入 masked_input = input_.clone() - self.vocab_start_index masked_input[input_mask] = 0 else: masked_input = input_ # 获取嵌入向量 output_parallel = F.embedding(masked_input, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse) # 把 mask 掉的 category idx 对应的嵌入向量处理成 0 if self.tensor_model_parallel_size > 1: output_parallel[input_mask, :] = 0.0 # 在所有模型并行 GPU 上进行 reduce 操作 output = reduce_from_tensor_model_parallel_region(output_parallel) return output\"]},\"61\":{\"h\":\"3D并行\",\"t\":[\"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM\",\"https://arxiv.org/pdf/2104.04473https://github.com/NVIDIA/Megatron-LM\"]},\"62\":{\"h\":\"贡献\",\"t\":[\"如何合理的使用上面的多种并行技术一直是一个困难的命题，本文中研究人员提出了一种名为PTD-P的策略，在一个大规模的GPU集群上达到超过50%的效能。\",\"本文探讨了以下几种因素对效率的影响：\",\"不同的并行策略：通常张量并行只适用于一个multi-GPU服务器内部，而流水线并行则几乎只用于更大的模型（多机集群）\",\"流水线并行中的schedule：对通信、pipeline bubble的大小、内存都有重要的影响\",\"参数的选取（如microbatch的大小）：对于内存使用和kernel计算效率、pipeline bubble大小也有影响\"]},\"63\":{\"h\":\"数据并行\",\"t\":[\"问题：\",\"batch过小，会导致单块GPU利用率降低，而通信成本大大增长\",\"理论上GPU数量等于batch size，这也限制了可以使用的卡的规模\"]},\"64\":{\"h\":\"考虑流水线并行\",\"t\":[\"流水线并行技术将一个模型（Transformer）中的不同层发放到多块GPU上，将原本一个batch的数据分解为多个更小的micro batch，通过编排forward pass和backward pass可以来获取不同的性能。 为了确保optimizer semantics（也就是端到端的计算能够正确的完成），不同GPU之间需要进行定期同步---在每一个batch计算完毕时执行pipeline flush操作（完成一次周期内所有micro batch的操作，此时不再提供任何其他新的输入），我们称这个过程中的等待时间为pipeline bubble。 在流水线并行技术中，micro batch的数量/pipeline尺寸（并行使用的GPU的数量）越大，通常pipeline flush消耗的时间则越小。\"]},\"65\":{\"h\":\"考虑张量并行\",\"t\":[\"见前文TP内容。\"]},\"66\":{\"h\":\"性能分析\",\"t\":[\"令：\",\"(p,t,d)分别表示PP/TP/DP并行维度\",\"n:GPU总数，有$ n=ptd $\",\"B:Global Batch Size\",\"Micro Batch Size\",\"m=B/(b*d)：每Pipeline中的microbatch的数量\",\"可以得到以下结论：\",\"对于多个GPU集群，通常建议在集群上采用流水线并行，每一个集群上的节点假如包含g个GPU卡，通常在一个节点内部采用不超过g张卡做张量并行\",\"考虑使用模型并行（包括流水线并行和张量并行）和数据并行时，通常建议在GPU内存满足的情况下最大化模型并行（t和p），然后通过数据并行技术扩展集群上进行加速\",\"microbatch的选择也非常重要，取决于最终的throughput（吞吐量）和memory，需要综合流水线并行p、数据并行d、全局BatchSize B考虑\",\"Activation Recompute也是一种可以考虑的方法，在大模型训练中内存往往比算力更加宝贵，所以一种可取的方法是将一部中间过程结果丢弃，在需要时（如进行backward更新）重新计算，以节省内存。\"]},\"67\":{\"h\":\"自动并行\"},\"68\":{\"h\":\"FlexFlow\",\"t\":[\"Beyond Data and Model Parallelism for Deep Neural Networks\",\"https://arxiv.org/pdf/1807.05358\",\"本文引入了 SOAP（一个更加全面的并行策略搜索空间） 和 FlexFlow（一个高效的并行策略搜索引擎）, 为了加速并行策略的搜索， FlexFlow 引入了一个 novel execution simulator, 可以准确预测并行策略的性能。\"]},\"69\":{\"h\":\"简介\",\"t\":[\"论文提出从 SOAP 4个维度描述搜索空间:\",\"Operator: 描述不同的 Operator 的并行方法 -- OP并行(如何将一个 大OP 拆分到 多个 devices 上)\",\"Sample: 对于单个 Operator, Sample 描述训练数据如何并行 -- 数据并行\",\"Parameter: 对于单个 Operator, Parameter 描述参数如何并行 -- 模型并行\",\"Attribute: 对于单个 Sample, Attribute 描述单个 Sample 不同属性如何并行 -- Sample并行(如何将一个 Sample 拆分到多个 devices 上)\",\"关键的问题是: 如何快速搜索 SOAP 空间\",\"快速，增量的执行模拟器 -- execution simulator\",\"Markov Chain Monte Carlo (马尔可夫链蒙特卡洛) 搜索算法 -- execution optimizer\"]},\"70\":{\"h\":\"概述\",\"t\":[\"同时输入 Compute Graph 和 Device Topology，FlexFlow负责将 Operator 分发到不同的设备上，确定 Operator 的执行顺序。\",\"SOAP 搜索空间对于不同的 Operator 【卷积、池化、全连接层、激活函数、归一化、嵌入、矩阵乘法、损失函数】，可拆分的维度并不一样，但大多数 Operator(除去BatchNormalization)，都支持 Sample 维度的拆分, 大多数 Operator 都能支持2种或者更多的拆分方式。 假设模型包含 N 个 Operator，那并行配置 至少有 2^N 种。当然，实际情况种，通常还会加一些约束条件, 比如 co-location（操作符的共置）, 但即使这样，要在合理时间内找到最优解依然比较困难。\"]},\"71\":{\"h\":\"执行模拟器\",\"t\":[\"接受 operator graph, device topology 和 parallelization strategy 作为输入，预测模型的执行时间。\"]},\"72\":{\"h\":\"执行优化器\",\"t\":[\"执行优化器，接受 operator graph 和 device topology 作为输入，自动查找高效的 parallelization strategy. 论文基于当前的 parallelization strategy 随机选择 operator，并随机替换被选择的 operator 的 parallelization strategy, 进而生成 proposals。同时，MCMC算法内部维护一个 最优 parallelization strategy, 依据如下规则进行更新:\"]},\"73\":{\"c\":[\"SOSD\"]},\"74\":{\"c\":[\"分布式系统\",\"MLSys\",\"并行运算\"]},\"75\":{\"h\":\"AISys_分布式开发（选读）\"},\"76\":{\"h\":\"序列并行\"},\"77\":{\"h\":\"Megatron\",\"t\":[\"Reducing Activation Recomputation in Large Transformer Models\",\"https://arxiv.org/pdf/2205.05198\"]},\"78\":{\"h\":\"Abstract\",\"t\":[\"在大模型训练过程中显存占用过大往往成为瓶颈，一般会通过重计算的方式降低显存占用，但会带来额外的计算代价。本文提出sequece parallel(序列并行,简称SP)和selective activation recomputation两种方法，可以结合TP有效减少不必要的计算量。\",\"下图中绿色部分表示不同参数级别模型中需要用于保存activation需要的显存大小，蓝色部分表示不同参数级别模型中需要用于保存parameter和optimizer state需要的显存大小。红色线表示baseline(A100的显存)80G。\",\"通过对比可以发现,原本单A100跑不了的模型,经过SP优化后可以在单A100上运行了,这就给我们加大数据量和多机并行提供了极大的便利\"]},\"79\":{\"h\":\"Activation Memory\",\"t\":[\"本文以Transformer结构为例估算Activation Memory，Activation指FWD和BWD梯度计算中创建的所有tensor。不包含模型参数大小和优化器中状态大小，但是包含dropout用到的mask tensor。\",\"本文推导与假设中用到了以下几个参量:\",\"本文假设h极大(实际上一般也确实极大), 认为2sb远小于sbh, 即只考虑中间过程的Memory(shb), 忽略输入输出的Memory\",\"对于Attention模块,这一部分依赖于softmax实现:\",\"Attention(Q,K,V)=softmax(dk​​QKT​)V\",\"具体实现图例见下:\",\"对于Attention块来说，输入的元素个数为sbh个，每个元素以半精度(2 bytes)来进行存储的话，对应输入的元素大小为2sbh bytes\",\"Attention块中包含一个self-attention、一个linear(线性映射层)和attention dropout层。对于linear需要保存输入的Activation大小为2sbh, 对于attention dropout层需要mask的大小为sbh(对于一个元素的mask只用1个bytes)，对于self-attention块的Activation Memory的计算有以下几块：\",\"Query(Q),Key(K),Value(V) matrix mul：input共享，元素个数为sbh个，总大小是 2sbh bytes。\",\"QKT矩阵相乘：需要分别创建保存Q和K的矩阵，每个矩阵元素总大小为2sbh bytes, 总共大小为4sbh bytes\",\"原始Self-Attention例子(此处X切分仅作示意,实际上是按行切分的):\",\"dropout的mask层矩阵的大小与softmax的输出一样，元素个数都是as2b个，但mask单个元素的大小只用1 bytes即可，总的大小为 as2b bytes\",\"softmax的输出也会用于反向的计算，需要缓存下来，对应大小 as2b bytes\",\"V矩阵的大小之前没有统计，和Q、K矩阵一样，大小也是2sbh bytes\",\"Attention 模块总的大小为 11sbh + 5as2b bytes。\",\"MLP的Activation大小计算：MLP中有两层线性layer，分别存储输入矩阵大小为2sbh bytes和8sbh bytes；GeLU的反向也需要对输入进行缓存，大小为8sbh bytes; dropout层需要sbh bytes; 总大小为19sbh。\",\"LayerNorm的Activation大小计算：每个LayerNorm层的输入需要2sbh大小，有两个LayerNorm层，总大小为4sbh bytes.\",\"一层transformer的memory总的大小为:\",\"ActivationMemoryPerLayer=sbh(34+5has​)\"]},\"80\":{\"h\":\"Tensor Parallel\",\"t\":[\"在TP并行中只在Attention和MLP两个地方进行了并行计算，对于两块的输入并没有并行操作。\",\"图中f和f​互为共轭(conjugate)，f在前向时不做操作，反向时执行all-reduce;f​在前向时执行all-reduce, 反向时不做操作。\",\"考虑TP并行(并行度为t)，并行部分有MLP的Linear部分(18sbh bytes)和Attention的QKV部分(6sbh bytes)， ActivationMemoryPerLayer的值降为：\",\"ActivationMemoryPerLayer=sbh(10+t24​+5htas​)\"]},\"81\":{\"h\":\"Sequence Parallel\",\"t\":[\"在Tensor模型并行基础上提出了Sequence Parallel，对于非TP并行的部分在sequence维度都是相互独立的，所以可以在sequence维度上进行拆分(即sequence parallel)。\",\"拆分后如下图，f和f​替换为g和g​​在前向是reduce-scatter, 反向是all-gather通信。\",\"以MLP为例，详细说明拆分步骤:\",\"MLP层由两个Linear层组成，对应的计算公式如下, 其中X的大小为s×b×h;A和B是Linear的权重weight矩阵，大小为h×4h和4h×h\",\"论文做如下符号说明:\",\"YZWV​=LayerNorm(X),=GeLU(YA),=ZB,=Dropout(W),​\",\"对X按sequence维度切分， X=[X1s​,X2s​]，LayerNorm的结果Y=[Y1s​,Y2s​]；\",\"考虑GeLU非线性，进行all-gather，计算Z=GeLU(YA)；\",\"对A进行列切分的tensor并行，得到结果YA1c​​和YA2c​\",\"对B进行行切分的tensor并行，得到结果Z1h​B1r​和Z2h​B2r​\",\"得到W1​和W2​后进行reduce-scatter\",\"具体过程见下:\",\"[Y1s​,Y2s​]Y[Z1h​,Z2h​]​W1​[W1s​,W2s​]​[V1s​,V2s​]​​​=LayerNorm([X1s​,X2s​]),​=g(Y1s​,Y2s​),=[GeLU(YA1c​), GeLU(YA2c​)],=Z1h​B1r​ and W2​=Z2h​B2r​,=gˉ​(W1​,W2​),=[Dropout(W1s​), Dropout(W2s​)].​\",\"TP在一次前向和后向总共有4次的all-reduce操作，在SP一次前向和后向总共有4次all-gather和4次reduce-scatter操作。ring all-reduce 执行过程中有两步，先是一个reduce-scatter然后一个all-gather，SP没有引入更多的通信代价。\",\"ActivationMemoryPerLayer​=sbh(t10​+t24​+5htas​)=tsbh​(34+5has​)​\"]},\"82\":{\"h\":\"DeepSpeed-Ulysses\",\"t\":[\"DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Modelshttps://arxiv.org/pdf/2309.14509\",\"DeepSpeed在知乎也有官号, 这里仅作简述, 官号本身讲的也非常不错, 链接在这儿:https://zhuanlan.zhihu.com/p/652206513\"]},\"83\":{\"h\":\"简介\",\"t\":[\"长序列在LLM应用中非常重要, 长上下文的保存有助于LLM推理, 需求大token和长Sequence的输入\",\"现有的DP TP PP不能解决序列维度的扩展问题\",\"现有的序列并行方法依托内存通讯, 不够高效\",\"DeepSpeed-Ulysses将各个样本在序列维度上分割给参与的GPU。在attention计算之前，它对已分割的查询(Q)、键(K)和值(V)执行all-to-all通信操作，以使每个GPU接收完整的序列，但仅用于注意力头的非重叠子集。 这使得参与的GPU可以并行计算不同的注意力头。最后，DeepSpeed-Ulysses还使用另一个all-to-all来在注意力头上收集结果，同时重新在序列维度上进行分区。\"]},\"84\":{\"h\":\"DeepSpeed-Ulysses的核心设计\",\"t\":[\"与已知的Transformer架构一样，设计由N个输入序列在P个可用设备上分区组成。每个本地N/P分区都被投影到查询（Q）、键（K）和值（V）嵌入中。接下来，(QKV) 嵌入通过参与计算设备之间的高度优化的全对全集合（all-to-all collectives）进行全局的 QKV 收集。在全对全集合后，每个头的注意力计算形式为：\",\"Outputcontext=Softmax(d​QKT​)V\",\"注意力计算后，另一个全对全集合将注意力计算的输出上下文张量转换为序列(N/P)并行，用于Transformer模型层的剩余模块中的后续操作。\"]},\"85\":{\"h\":\"Sequence Parallelism\",\"t\":[\"Sequence Parallelism: Long Sequence Training from System Perspectivehttps://arxiv.org/pdf/2105.13120\"]},\"86\":{\"h\":\"论文背景\",\"t\":[\"和Ulysses一样, 这个SP也是目的也是解决输入序列规模问题, 而不是像Megtron-LM一样解决计算过程中的内存占用问题.\",\"文章将自己的SP流程和PP TP模型做了比较:\"]},\"87\":{\"h\":\"主要工作\",\"t\":[\"本文认为SP最主要的问题是跨设备如何计算sub-sequences的attention scores问题，为了解决该问题，本文设计出Ring Self-Attention来解决此问题.\",\"感觉想法来源于Ring-AllReduce, 而且只优化了自注意力部分\",\"RSA感觉就是将输入序列进行切分, 通过将序列整体切分成小的chunk, 使得每一个chunk都尽可能大, 用commnication换取序列长度\"]},\"88\":{\"h\":\"Ring-Attention\",\"t\":[\"Ring Attention with Blockwise Transformers for Near-Infinite Contexthttps://arxiv.org/pdf/2310.01889\"]},\"89\":{\"h\":\"背景\",\"t\":[\"老生常谈的transformer长序列要求，不做赘述。\"]},\"90\":{\"h\":\"Ring Attention\",\"t\":[\"本文提出以分块方式执行Self-Attention和FWD计算，多机分布序列维度，从而实现并发计算和通信. 由于该方法将环中主机设备之间的Key Value Block通信与compute重叠，因此将其命名：环注意(Ring Attention)\",\"具体实现：\",\"该方法在主机设备之间构建Self-Attention的外循环，每个主机设备具有一个query block，并通过Key Value Block遍历主机Ring，以逐块的方式进行注意力和前馈网络计算。 当计算Self-Attention时，每个主机将Key Value Block发送到下一个主机，同时从前一个主机接收Key Value Block。\",\"对于内循环，每个设备计算其各自的Self-Attention和FWD。在内循环期间，每个设备将用于compute的Key Value Block的副本发送到Ring中的下一个设备，同时从前一个设备接收Key Value Block。 由于块计算比块传输需要更长的时间，与标准Transformer相比，此过程不会增加开销。\",\"这种Ring Attention方式可以突破序列长度限制（对于单卡内存需求来说，而非模型整体来说），因为我们在Attention之前矩阵乘就已经切分了Squence，让每一个卡分批成环去跑一小批token\",\"这种方式理论上并不影响训练结果，因为最小训练单位还是一个token（对Squence做切分时的原则）\",\"天才般的想法！（我觉得）好吧也需要堆卡出奇迹\",\"那么代价呢？ 文章的训练测试规模比较小，能否在大规模训练时取得想象中的线性效果，还是未知数 文章只对Attention和FWD操作做了优化，基础操作还有进一步优化的空间，可以考虑采用4D并行。\"]},\"91\":{\"h\":\"DISTFLASHATTN\",\"t\":[\"https://arxiv.org/pdf/2310.03294\"]},\"92\":{\"h\":\"背景\",\"t\":[\"部分SP方法，如Ring Attention缺少高效的Attention实现。（前文提及的基础操作还有进一步优化的空间）\"]},\"93\":{\"h\":\"论文方法\",\"t\":[\"文章针对三个challenge提出了三个解决方法，解决了高校Attention实现、分布式应用等问题。\"]},\"94\":{\"h\":\"token-level workload imbalance\",\"t\":[\"这主要是由于causal mask引起的attention计算问题，如果简单分块计算约会有1/2的计算资源浪费。 因为一般causal mask就是会用矩阵以对角线为界mask数据, 按照ring拓扑结构进行计算也会有约一半的CPU处于等待状态。\",\"针对这个问题，论文中提出Token-level workload balancing的调度算法，通过给空闲的worker来fetch部分key-value数据，计算后再传递回去。\"]},\"95\":{\"h\":\"Prohibitive communication overhead\",\"t\":[\"需要通信汇总不同worker上的attention结果，本文提出通过计算attn(qp​,kr​,vr​,sp​) prefetch张量来完成通信workerp⟵kr+1​,vr+1​​workerr+1的覆盖，实现依赖于P2P的通信模式。\"]},\"96\":{\"h\":\"Loadbalanced scheduling with communication and computation overlap\",\"t\":[\"huggingface中采用的Recomputation的检查点放置不合理，导致相对高的还原计算代价。 下面是两种重计算策略的对比：\",\"DFA相较HF每一个FlashAttention+FFN约能够减少一个FA的计算量。FlashAttention部分的梯度更新为KQV三个矩阵，可以通过FA的输出结果完成更新，因此保存FA的结果便可计算三者的矩阵。\"]},\"97\":{\"h\":\"典型并行方式时序图\"},\"98\":{\"h\":\"TP\"},\"99\":{\"h\":\"DP\"},\"100\":{\"h\":\"PP\"},\"101\":{\"c\":[\"SOSD\"]},\"102\":{\"c\":[\"分布式系统\",\"AISys\",\"并行运算\"]},\"103\":{\"h\":\"CS149_Course\"},\"104\":{\"c\":[\"CS149\"]},\"105\":{\"c\":[\"公开课\",\"并行计算\"]},\"106\":{\"h\":\"CS149 Lab Assignment1\"},\"107\":{\"h\":\"Prog1_mandelbort_threads\"},\"108\":{\"h\":\"环境配置\",\"t\":[\"本人使用OS为Ubuntu 22.04, 还是建议使用Linux系统做Lab, 很多环境配置会方便一些.\",\"CS149_Asst1并不需要额外配置运行环境, 下载解压一下编译环境就好啦! 下载包:\",\" wget https://github.com/ispc/ispc/releases/download/v1.21.0/ispc-v1.21.0-linux.tar.gz\",\"解压包:\",\" tar -xvf ispc-v1.21.0-linux.tar.gz\",\"配置环境路径:\",\" export PATH=$PATH:${HOME}/Downloads/ispc-v1.21.0-linux/bin\",\"环境配置完成后就可以clone repo到本地来开始lab了:\",\" git clone https://github.com/stanford-cs149/asst1.git\"]},\"109\":{\"h\":\"任务分析\",\"t\":[\"Pro1的内容主要是为了让学生了解std::thread的并行机制和\\\"多线程不一定高效率\\\"的并发事实, 所以难度并不算大~~(这是我的事后诸葛亮)~~, 整体框架已经在源码中基本完成了.完成后可以通过make + ./mandelbort --<args>检验正确与否.\",\"task :\",\"创建线程0和线程1, 分别计算图像的上下两个部分, 即将图像的不同空间交给不同线程计算, 这被称为空间分解(spatial decomposition).\",\"扩展代码使其能够使用2, 3, 4, 5, 6, 7, 8个线程, 进行空间分解, 生成加速图, 假设加速是否与线程数线性相关并加以验证.\",\"在workerThreadStart()的开头和结尾插入计时代码, 验证并解释task2中提出的猜想.\",\"修改一开始的线程分配方式, 实现将两个图片都拉到8线程时7-8倍加速比的效果, 找到适应任何线程数的泛型分配方式(不需要线程之间进行响应和同步), 报告最后得出的8线程加速比.\",\"使用16个线程运行改进后代码, 回答性能是否明显高于8线程并解释原因.\",\"事实上task中给的提示还是比较明显的, 在task1中解释了空间分解的概念, 那么通过对图片本身的上下多份分割,就可以解决这个问题,要注意分割的时候会不会漏行.\"]},\"110\":{\"h\":\"任务实现\",\"t\":[\"我们将一开始就对任务给出多线程的解决方式, 并在后续针对数据结果决定是否要进行优化.\",\"首先我们可以根据阅读mandelbrotSerial.cpp中的源码, 得到mandelbrotSerial()函数事实上是用来计算Mandelbrot图像的, 可以简单分析一下mandelbrotSerial()函数的各个参数:\",\" void mandelbrotSerial( float x0, float y0, float x1, float y1, // 复平面左上和右下两个点坐标 int width, int height, // 图像宽度和高度 int startRow, int numRows, // 开始行和总计算行数 int maxIterations, // 最大迭代次数 int output[]); // 每个点的迭代次数\",\"不难发现只要我们给出startRow, numRows, 其余保持图像默认参数, 就可以完成计算了. 所以可以给出函数workerThreadStart(WorkerArgs * const args)的代码:\",\" size_t rows = args -> height / args -> numThreads; // 确定要计算的行数 if (args -> height % args -> numThreads) { // 如果该遇到整除要加一行避免遗漏 rows++; } size_t startRow = args -> threadId * rows; // 确定开始行 // 如果已经到最后部分不够切分, 直接处理最后部分 rows = rows > args -> height - startRow ? args -> height - startRow : rows; // 调用mandelbrotSerial mandelbrotSerial(args -> x0, args -> y0, args -> x1, args -> y1, args -> width, args -> height, startRow, rows, args -> maxIterations, args -> output);\"]},\"111\":{\"c\":[\"CS149_Lab\"]},\"112\":{\"c\":[\"公开课\",\"并行计算\",\"Lab\"]},\"113\":{\"h\":\"MLSys_分布式开发（选读）\"},\"114\":{\"h\":\"序列并行\"},\"115\":{\"h\":\"Megatron\",\"t\":[\"Reducing Activation Recomputation in Large Transformer Models\",\"https://arxiv.org/pdf/2205.05198\"]},\"116\":{\"h\":\"Abstract\",\"t\":[\"在大模型训练过程中显存占用过大往往成为瓶颈，一般会通过重计算的方式降低显存占用，但会带来额外的计算代价。本文提出sequece parallel(序列并行,简称SP)和selective activation recomputation两种方法，可以结合TP有效减少不必要的计算量。\",\"下图中绿色部分表示不同参数级别模型中需要用于保存activation需要的显存大小，蓝色部分表示不同参数级别模型中需要用于保存parameter和optimizer state需要的显存大小。红色线表示baseline(A100的显存)80G。\",\"通过对比可以发现,原本单A100跑不了的模型,经过SP优化后可以在单A100上运行了,这就给我们加大数据量和多机并行提供了极大的便利\"]},\"117\":{\"h\":\"Activation Memory\",\"t\":[\"本文以Transformer结构为例估算Activation Memory，Activation指FWD和BWD梯度计算中创建的所有tensor。不包含模型参数大小和优化器中状态大小，但是包含dropout用到的mask tensor。\",\"本文推导与假设中用到了以下几个参量:\",\"本文假设h极大(实际上一般也确实极大), 认为2sb远小于sbh, 即只考虑中间过程的Memory(shb), 忽略输入输出的Memory\",\"对于Attention模块,这一部分依赖于softmax实现:\",\"Attention(Q,K,V)=softmax(dk​​QKT​)V\",\"具体实现图例见下:\",\"对于Attention块来说，输入的元素个数为sbh个，每个元素以半精度(2 bytes)来进行存储的话，对应输入的元素大小为2sbh bytes\",\"Attention块中包含一个self-attention、一个linear(线性映射层)和attention dropout层。对于linear需要保存输入的Activation大小为2sbh, 对于attention dropout层需要mask的大小为sbh(对于一个元素的mask只用1个bytes)，对于self-attention块的Activation Memory的计算有以下几块：\",\"Query(Q),Key(K),Value(V) matrix mul：input共享，元素个数为sbh个，总大小是 2sbh bytes。\",\"QKT矩阵相乘：需要分别创建保存Q和K的矩阵，每个矩阵元素总大小为2sbh bytes, 总共大小为4sbh bytes\",\"原始Self-Attention例子(此处X切分仅作示意,实际上是按行切分的):\",\"dropout的mask层矩阵的大小与softmax的输出一样，元素个数都是as2b个，但mask单个元素的大小只用1 bytes即可，总的大小为 as2b bytes\",\"softmax的输出也会用于反向的计算，需要缓存下来，对应大小 as2b bytes\",\"V矩阵的大小之前没有统计，和Q、K矩阵一样，大小也是2sbh bytes\",\"Attention 模块总的大小为 11sbh + 5as2b bytes。\",\"MLP的Activation大小计算：MLP中有两层线性layer，分别存储输入矩阵大小为2sbh bytes和8sbh bytes；GeLU的反向也需要对输入进行缓存，大小为8sbh bytes; dropout层需要sbh bytes; 总大小为19sbh。\",\"LayerNorm的Activation大小计算：每个LayerNorm层的输入需要2sbh大小，有两个LayerNorm层，总大小为4sbh bytes.\",\"一层transformer的memory总的大小为:\",\"ActivationMemoryPerLayer=sbh(34+5has​)\"]},\"118\":{\"h\":\"Tensor Parallel\",\"t\":[\"在TP并行中只在Attention和MLP两个地方进行了并行计算，对于两块的输入并没有并行操作。\",\"图中f和f​互为共轭(conjugate)，f在前向时不做操作，反向时执行all-reduce;f​在前向时执行all-reduce, 反向时不做操作。\",\"考虑TP并行(并行度为t)，并行部分有MLP的Linear部分(18sbh bytes)和Attention的QKV部分(6sbh bytes)， ActivationMemoryPerLayer的值降为：\",\"ActivationMemoryPerLayer=sbh(10+t24​+5htas​)\"]},\"119\":{\"h\":\"Sequence Parallel\",\"t\":[\"在Tensor模型并行基础上提出了Sequence Parallel，对于非TP并行的部分在sequence维度都是相互独立的，所以可以在sequence维度上进行拆分(即sequence parallel)。\",\"拆分后如下图，f和f​替换为g和g​​在前向是reduce-scatter, 反向是all-gather通信。\",\"以MLP为例，详细说明拆分步骤:\",\"MLP层由两个Linear层组成，对应的计算公式如下, 其中X的大小为s×b×h;A和B是Linear的权重weight矩阵，大小为h×4h和4h×h\",\"论文做如下符号说明:\",\"YZWV​=LayerNorm(X),=GeLU(YA),=ZB,=Dropout(W),​\",\"对X按sequence维度切分， X=[X1s​,X2s​]，LayerNorm的结果Y=[Y1s​,Y2s​]；\",\"考虑GeLU非线性，进行all-gather，计算Z=GeLU(YA)；\",\"对A进行列切分的tensor并行，得到结果YA1c​​和YA2c​\",\"对B进行行切分的tensor并行，得到结果Z1h​B1r​和Z2h​B2r​\",\"得到W1​和W2​后进行reduce-scatter\",\"具体过程见下:\",\"[Y1s​,Y2s​]Y[Z1h​,Z2h​]​W1​[W1s​,W2s​]​[V1s​,V2s​]​​​=LayerNorm([X1s​,X2s​]),​=g(Y1s​,Y2s​),=[GeLU(YA1c​), GeLU(YA2c​)],=Z1h​B1r​ and W2​=Z2h​B2r​,=gˉ​(W1​,W2​),=[Dropout(W1s​), Dropout(W2s​)].​\",\"TP在一次前向和后向总共有4次的all-reduce操作，在SP一次前向和后向总共有4次all-gather和4次reduce-scatter操作。ring all-reduce 执行过程中有两步，先是一个reduce-scatter然后一个all-gather，SP没有引入更多的通信代价。\",\"ActivationMemoryPerLayer​=sbh(t10​+t24​+5htas​)=tsbh​(34+5has​)​\"]},\"120\":{\"h\":\"DeepSpeed-Ulysses\",\"t\":[\"DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Modelshttps://arxiv.org/pdf/2309.14509\",\"DeepSpeed在知乎也有官号, 这里仅作简述, 官号本身讲的也非常不错, 链接在这儿:https://zhuanlan.zhihu.com/p/652206513\"]},\"121\":{\"h\":\"简介\",\"t\":[\"长序列在LLM应用中非常重要, 长上下文的保存有助于LLM推理, 需求大token和长Sequence的输入\",\"现有的DP TP PP不能解决序列维度的扩展问题\",\"现有的序列并行方法依托内存通讯, 不够高效\",\"DeepSpeed-Ulysses将各个样本在序列维度上分割给参与的GPU。在attention计算之前，它对已分割的查询(Q)、键(K)和值(V)执行all-to-all通信操作，以使每个GPU接收完整的序列，但仅用于注意力头的非重叠子集。 这使得参与的GPU可以并行计算不同的注意力头。最后，DeepSpeed-Ulysses还使用另一个all-to-all来在注意力头上收集结果，同时重新在序列维度上进行分区。\"]},\"122\":{\"h\":\"DeepSpeed-Ulysses的核心设计\",\"t\":[\"与已知的Transformer架构一样，设计由N个输入序列在P个可用设备上分区组成。每个本地N/P分区都被投影到查询（Q）、键（K）和值（V）嵌入中。接下来，(QKV) 嵌入通过参与计算设备之间的高度优化的全对全集合（all-to-all collectives）进行全局的 QKV 收集。在全对全集合后，每个头的注意力计算形式为：\",\"Outputcontext=Softmax(d​QKT​)V\",\"注意力计算后，另一个全对全集合将注意力计算的输出上下文张量转换为序列(N/P)并行，用于Transformer模型层的剩余模块中的后续操作。\"]},\"123\":{\"h\":\"Sequence Parallelism\",\"t\":[\"Sequence Parallelism: Long Sequence Training from System Perspectivehttps://arxiv.org/pdf/2105.13120\"]},\"124\":{\"h\":\"论文背景\",\"t\":[\"和Ulysses一样, 这个SP也是目的也是解决输入序列规模问题, 而不是像Megtron-LM一样解决计算过程中的内存占用问题.\",\"文章将自己的SP流程和PP TP模型做了比较:\"]},\"125\":{\"h\":\"主要工作\",\"t\":[\"本文认为SP最主要的问题是跨设备如何计算sub-sequences的attention scores问题，为了解决该问题，本文设计出Ring Self-Attention来解决此问题.\",\"感觉想法来源于Ring-AllReduce, 而且只优化了自注意力部分\",\"RSA感觉就是将输入序列进行切分, 通过将序列整体切分成小的chunk, 使得每一个chunk都尽可能大, 用commnication换取序列长度\"]},\"126\":{\"h\":\"Ring-Attention\",\"t\":[\"Ring Attention with Blockwise Transformers for Near-Infinite Contexthttps://arxiv.org/pdf/2310.01889\"]},\"127\":{\"h\":\"背景\",\"t\":[\"老生常谈的transformer长序列要求，不做赘述。\"]},\"128\":{\"h\":\"Ring Attention\",\"t\":[\"本文提出以分块方式执行Self-Attention和FWD计算，多机分布序列维度，从而实现并发计算和通信. 由于该方法将环中主机设备之间的Key Value Block通信与compute重叠，因此将其命名：环注意(Ring Attention)\",\"具体实现：\",\"该方法在主机设备之间构建Self-Attention的外循环，每个主机设备具有一个query block，并通过Key Value Block遍历主机Ring，以逐块的方式进行注意力和前馈网络计算。 当计算Self-Attention时，每个主机将Key Value Block发送到下一个主机，同时从前一个主机接收Key Value Block。\",\"对于内循环，每个设备计算其各自的Self-Attention和FWD。在内循环期间，每个设备将用于compute的Key Value Block的副本发送到Ring中的下一个设备，同时从前一个设备接收Key Value Block。 由于块计算比块传输需要更长的时间，与标准Transformer相比，此过程不会增加开销。\",\"这种Ring Attention方式可以突破序列长度限制（对于单卡内存需求来说，而非模型整体来说），因为我们在Attention之前矩阵乘就已经切分了Squence，让每一个卡分批成环去跑一小批token\",\"这种方式理论上并不影响训练结果，因为最小训练单位还是一个token（对Squence做切分时的原则）\",\"天才般的想法！（我觉得）好吧也需要堆卡出奇迹\",\"那么代价呢？ 文章的训练测试规模比较小，能否在大规模训练时取得想象中的线性效果，还是未知数 文章只对Attention和FWD操作做了优化，基础操作还有进一步优化的空间，可以考虑采用4D并行。\"]},\"129\":{\"h\":\"DISTFLASHATTN\",\"t\":[\"https://arxiv.org/pdf/2310.03294\"]},\"130\":{\"h\":\"背景\",\"t\":[\"部分SP方法，如Ring Attention缺少高效的Attention实现。（前文提及的基础操作还有进一步优化的空间）\"]},\"131\":{\"h\":\"论文方法\",\"t\":[\"文章针对三个challenge提出了三个解决方法，解决了高校Attention实现、分布式应用等问题。\"]},\"132\":{\"h\":\"token-level workload imbalance\",\"t\":[\"这主要是由于causal mask引起的attention计算问题，如果简单分块计算约会有1/2的计算资源浪费。 因为一般causal mask就是会用矩阵以对角线为界mask数据, 按照ring拓扑结构进行计算也会有约一半的CPU处于等待状态。\",\"针对这个问题，论文中提出Token-level workload balancing的调度算法，通过给空闲的worker来fetch部分key-value数据，计算后再传递回去。\"]},\"133\":{\"h\":\"Prohibitive communication overhead\",\"t\":[\"需要通信汇总不同worker上的attention结果，本文提出通过计算attn(qp​,kr​,vr​,sp​) prefetch张量来完成通信workerp⟵kr+1​,vr+1​​workerr+1的覆盖，实现依赖于P2P的通信模式。\"]},\"134\":{\"h\":\"Loadbalanced scheduling with communication and computation overlap\",\"t\":[\"huggingface中采用的Recomputation的检查点放置不合理，导致相对高的还原计算代价。 下面是两种重计算策略的对比：\",\"DFA相较HF每一个FlashAttention+FFN约能够减少一个FA的计算量。FlashAttention部分的梯度更新为KQV三个矩阵，可以通过FA的输出结果完成更新，因此保存FA的结果便可计算三者的矩阵。\"]},\"135\":{\"c\":[\"SOSD\"]},\"136\":{\"c\":[\"分布式系统\",\"MLSys\",\"并行运算\"]},\"137\":{\"h\":\"阶乘的位数估算--数学在计算机算法研究中的作用\"},\"138\":{\"h\":\"题目引入\",\"t\":[\"算法与数据结构实验题 1.10 单身狗进化 这一天晚上，弯通又做梦了，并且梦到了一个帅气的男孩纸！这个男孩给了弯通一个数字 n。男孩离开前告诉弯通，n!（n 的阶乘）的位数就是距离弯通脱单的天数。矜（ji）持（ke）的弯通想知道自己还有多久能脱单，快写个程序帮助他！ 输入: 输入第一行为一个正整数 n（1<=n<=25000）。 输出: n阶乘的位数\"]},\"139\":{\"h\":\"题目分析\",\"t\":[\"这道题看上去还挺有意思的很符合大学生的心理状态, 实际上就是要求阶乘的位数倒也没有拐弯抹角. 但是我们都知道, 要是用递归或者循环写阶乘, 这将是一件极为恐怖的事情. 在数据存储(空间复杂度)&计算用时(时间复杂度)上的开销, 将成为任何一台机器的噩梦, 更不可能过测试了. 举个栗子:\",\" int n; long long ans = 1; std::cin >> n; for (int i = 1; i < MAX; i++) ans *= i; std::cout << ans;\",\"大家可以简单跑一下这个程序, 然后就会发现, 在n = 27的时候, 就已经溢出了, 完全无法满足题目要求. 这就是第一种错误的可能, 忘记了估计数据规模, 随便算算就存爆了.\",\"还有一种可能, 就是采用高精度的算法, 将阶乘结果用表存储, 每个内存存有限位数据, 在乘法时做类似竖式乘法的高精度运算.\",\"这种方式能不能过这个题我没有试过因为我懒, 但是一般来说高精度阶乘的时间复杂度是O(n2)\",\"程序代码的复杂度和 n = 25000 所要存储的数据规模, 也会是比较大的开销.\",\"下文将介绍一种用数学方法巧妙估算阶乘结果规模的方式.\"]},\"140\":{\"h\":\"斯特林公式\",\"t\":[\"n!≈2πn​(en​)n\",\"这个公式以詹姆斯·斯特林的名字命名，虽然亚伯拉罕·棣美弗早于斯特林提出了一个类似的公式，但结果较不精确. 当n很大的时候，n阶乘的计算量十分大，所以斯特林公式十分好用，而且，即使在n很小的时候，斯特林公式的取值已经十分准确.\",\"可以通过计算对比来估计一下斯特林公式算出结果, 和阶乘计算结果的误差程度.\",\"我们可以看到, 随着n的增大, 斯特林公式估算的误差已经降到了十万分之一以下, 这对估算阶乘的规模来说是完全可以接受的误差.\",\"通过斯特林公式我们可以简单估算阶乘的位数, 我们知道对于一个n进制数x, 都可以对其取$ [\\\\log_{n}x] + 1 $来得到这个n进制数的位数, 我们将进一步推导用斯特林公式估算阶乘位数N的公式.\",\"N=[log10​[2πn​(en​)n]]+1\",\"其中内层中括号标记运算顺序, 外层中括号意为高斯取整(即向下取整).\",\"N=[21​log10​(2πn)+nlog10​(en​)]+1\",\"通过代入n, 即可轻松求得n!的位数, 时间复杂度是梦寐以求的O(1), 即常数时间复杂度.\"]},\"141\":{\"h\":\"代码实现\",\"t\":[\"代码实现没什么好说的, 套公式罢了, 由于我之前已经测试过最大数据规模, 所以ans也是为了省事儿用的int偷懒是可耻的\",\" #include <stdio.h> #include <math.h> #define PI 3.141592654 #define E 2.71828182846 int pos(int n) { int s = 1; if(n > 3) s = log10(2*PI*n) / 2 + n * log10(n/E) + 1; return s; } int main() { int num, ans; scanf(\\\"%d\\\", &num); ans = pos(num); printf(\\\"%d\\\", ans); }\"]},\"142\":{\"h\":\"总结\",\"t\":[\"数即一切\"]},\"143\":{\"c\":[\"数据结构与算法\"]},\"144\":{\"c\":[\"算法\",\"数学\"]},\"145\":{\"h\":\"Diffusion（文生图、文生视频）推理服务\"},\"146\":{\"h\":\"文生图\",\"t\":[\"随着transformer在文本生成方面逐步展现出的巨大潜力和ChatGPT等生成式对话AI的逐步商业化, 处理多模态任务的价值也不断被挖掘, 文生图文生视频等潜力巨大的任务类型开始提上研究日程.\",\"但是同生成式对话使用有限的文本量便能达成较为不错的生成效果不同, 文生视频和文生图任务由于图形任务的整体性对大块内存的使用提出了更为严苛的要求, 图块与图块 像素与像素之间的高度关联性也难以像文本推理一样通过简单的切分矩阵实现并发, 这使得最为常用的 Diffusion模型在生成时长和内存占用上的表现都差强人意.\",\"文生图 文生视频等多模态任务中的分布式推理服务就是在这种背景下被关注的.\",\"目前考虑到的针对Diffusion的优化主要集中于以下几个方面:\",\"使用更好的solver, 减少采样步数, 避免多轮采样带来的内存开销\",\"利用diffusion相邻step冗余考虑适当保存activate值, 避免不必要的重计算\",\"使用diffusion parallelism\",\"使用一些通用优化手段,像算子并行,图优化,模型压缩等等.\"]},\"147\":{\"h\":\"DistriFusion\",\"t\":[\"https://openaccess.thecvf.com/content/CVPR2024/papers/Li_DistriFusion_Distributed_Parallel_Inference_for_High-Resolution_Diffusion_Models_CVPR_2024_paper.pdf\",\"https://github.com/mit-han-lab/distrifuser\",\"2024的CVPR, 工作属于是Diffusion parallelsim的一种\"]},\"148\":{\"h\":\"特点\",\"t\":[\"无需训练\",\"加速效果较好, 可以达到6倍以上加速\",\"利用了diffusion过程相邻步之间feature map的相似性\"]},\"149\":{\"h\":\"以往方法\",\"t\":[\"常规方法: 多GPU通常仅用于批量推理. 在生成单张图像时，通常只涉及一个GPU.考虑到激活值规模庞大, 通信成本会超过分布式计算带来的节省, 张量并行等并行技术并不适合diffusion模型.\",\"传统分批方法: 将图像split成N个patch，放在N个device上进行推理，然后将N个device的结果合成为一个全分辨率的结果. 但是这种方法由于缺少各个patch间的信息感知，会生成N个小图拼接而成的大图, 边界处会出现明显的接缝, 区块间引入交互又会带来过高的同步成本\",\"用精度换效率, 但是图片不是整体了, 这也是前言讲难以像文本推理一样通过简单的切分矩阵实现并行的原因\"]},\"150\":{\"h\":\"相关工作\",\"t\":[\"difussion核心在迭代去噪点生成内容, 用巨量的计算换取极高的生成能力, 目前优化主要集中在以下几点:\",\"高效去噪: 如将高分辨率图像压缩为低分辨率的潜在表示，并在潜在空间中学习扩散模型\",\"设计无需训练的高效采样算法: 基于扩散模型与微分方程之间的联系，并利用成熟的指数积分器来减少采样步骤\",\"预训练的扩散模型中提炼出生成模型: 成效不佳\",\"优化扩散模型的神经推理\",\"DistriFusion方法: 利用多个设备上的神经网络并行性来加速扩散过程\",\"针对LLM的并行方法特点是: LLM模型尺寸较大, 但激活尺寸小, 不需要引入太多通讯开销.\",\"但difussion模型的特点是: 模型尺寸较小, 但激活尺寸大, 于是通信开销不得不成为主要矛盾, 目前主要只使用数据并行.\",\"本文方法基于patch并行性, 切分小patch分到不同设备处理, 倾向于使用AllGather而非AllReduce进行数据交互.\"]},\"151\":{\"h\":\"背景知识\",\"t\":[\"扩散模型通常会训练一个噪声预测神经网络模型（如U-Net）ϵθ​. 从纯高斯噪声xT​∼N(0,I)开始，该模型需要经过数十到数百次的迭代去噪步骤，以获得最终的清晰图像x0​，其中T表示总步数. 在时间步t给定含噪图像xt​，模型ϵθ​会将xt​、t以及额外的条件c（例如文本）作为输入，以预测xt​中的相应噪声ϵt​. 在每个去噪步骤中，可以通过以下方程推导出xt−1​:\",\"xt−1​=Update(xt​,t,ϵt​),ϵt​=ϵθ​(xt​,t,c).\",\"通过这个公式我们可以轻易发现xt−1​和xt​的强依赖关系, 这给t与t−1步的模型ϵ并行带来了极大的困难.\",\"同时本文还指出, 处理一张1024x1024的图像时，每步需要6,763 GMACs（十亿次乘加运算）。随着分辨率的增加，这种计算需求会以超过二次方的速度增加，导致生成单张高分辨率图像的延迟在实际应用中变得过高.\",\"作者还在这里cue了一下Shih等人在2023提出的ParaDiGMS, 说利用Picard迭代以数据并行的方式并行化去噪步骤, 是可能存在无效结果的&步数较大的去噪方式, 难以并行加速. 这篇文章也是NeurIPS 2023的一篇, 平均加速在2倍左右, 的确没有本文出色.\",\"本文又一次强调了模型分片到多个设备上，并使用张量并行进行推理的大通讯量的不可行性.\"]},\"152\":{\"h\":\"本文方法\",\"t\":[\"将图像分割成多个区块，在多个设备间并行化处理计算, 这存在两个选择和两个问题: \",\"独立计算各个区块并随后拼接 -> 边界处会出现明显的接缝和强烈的撕裂感\",\"区块间同步传递中间激活信息 -> 极高的通信开销, 甚至超过计算时间\",\"为解决上述问题, 本文提出了一种新的并行范式--位移区块并行.\",\"核心:利用前一步扩散过程中稍有过时或“陈旧”的激活信息，来促进区块间的交互.\",\"在计算某一区块某一层的激活信息时，并不依赖于其他区块的最新激活信息，从而使得通信可以被隐藏在后续层的计算过程中.\",\"两次扩散时间步的图相似度是很高的, 确实有相当一部分激活值完全可以\\\"暂存\\\"而不用\\\"重计算\\\"或者\\\"通讯\\\".\"]},\"153\":{\"h\":\"位移补丁并行\",\"t\":[\"在预测ϵθ​(xt​)（为简化说明，此处省略了时间步t和条件c的输入）时，首先将xt​分割成多个补丁xt(1)​, xt(2)​, ..., xt(N)​，其中N表示设备的数量. 在例图使用了N=2.每个设备都拥有模型ϵθ​的一个副本，并将独立且并行地处理一个单独的补丁.上标(1)和(2)分别代表第一个和第二个块，前一步的陈旧激活被加深了颜色.\",\"在每一步t，首先将输入xt​分割成N个块xt(1)​, xt(2)​, ..., xt(N)​，对于每一层l和每个设备i，在获取输入激活块Atl,(i)​之后，将执行两个异步操作：\",\"在设备 i 上，Atl,(i)​被重新分散回前一步的陈旧激活Atl​.这个Scatter操作的输出随后被输入到稀疏操作Fl​中（线性、卷积或注意力层），它仅在新鲜区域上执行计算并产生相应的输出.\",\"与此同时，对A_{t}^{l,(i)}执行AllGather操作，以准备下一步的完整激活Atl​.\",\"最后，将最终输出聚合在一起，以近似ϵθ​(xt​)\"]},\"154\":{\"h\":\"总结\",\"t\":[\"DistriFusion是一种利用多GPU并行加速扩散模型的新方法.将图像分割成多个区块，并将每个区块分配给独立的GPU进行处理.同时复用前一步骤的预计算激活值，以保持区块间的相互作用.\",\"局限:\",\"对于低分辨率图像，DistriFusion的加速效果有限.\",\"对于步数极少的方法，由于去噪状态迅速变化，该方法可能不适用.\"]},\"155\":{\"h\":\"PipeFusion\",\"t\":[\"https://arxiv.org/html/2405.14430v2\",\"PipeFusion可以看作是在DistriFusion上的改进与推广, 重点是把DistriFusion方法移植到了DiT模型上, 证明了DistriFusion方法的泛用性.\"]},\"156\":{\"h\":\"摘要\",\"t\":[\"PipeFusion通过将图像分割成多个patch并跨多个GPU分布网络层，采用流水线并行方式来协调通信和计算.\",\"利用相邻扩散步骤输入之间的高相似性，通过复用一步旧的特征图来为当前步骤提供上下文，从而消除流水线中的等待时间\"]},\"157\":{\"h\":\"问题\",\"t\":[\"DiT模型的推理延迟显著，随着序列长度的增加，计算时间呈二次方增长.单GPU无法满足实际应用的延迟要求，需要跨多个计算设备并行化DiT推理.\",\"DistriFusion需要为每个GPU维护所有层的KV数据，导致GPU增加时内存开销增加.\",\"DistriFusion基于U-NET模型，在每个layer上做集合通信，通信开销较大.\"]},\"158\":{\"h\":\"本文方法\",\"t\":[\"和DistriFusion相同操作如patch分割, 异步修正, warmup预热不再赘述\",\"PipeFusion与DistriFusion不同的是:\",\"采用流水线并行方式来协调不同设备上的计算和通信, 但保存原本激活值的方式和DistriFusion保持一致\",\"采用P2P通信, 考虑DiT模型特点, 不再像DistriFusion每个时间步的通信都是Scatter给每个设备, 而是点对点通信, 减少通信成本\",\"提升存储效率, 与DistriFusion相比，PipeFusion每个设备只存储与其特定阶段相关的参数的1/N.\"]},\"159\":{\"h\":\"xDIT\",\"t\":[\"DiT模型在文生图和文生视频等表现出了杰出的性能, 但是与此同时DiTs 的输入序列长度日益增长，序列增长导致Attention 机制的计算量也随之呈平方级膨胀, 推理延迟极为严重, 单卡推理必然无法满足需求, 多GPU乃至多机DiT部署是必然要求.\",\"但是在之前都是基于hf diffusers库进行改造, 正如在vLLM前都是基于hf transformer进行改造, 都是临时的不成体系的方案, 迫切需求一个性能好且易用性高的DiT推理框架.\",\"于是, 在PipeFusion基础上升级的xDit诞生了.\",\"https://arxiv.org/abs/2405.14430\",\"https://github.com/xdit-project/xDiT\",\"DiT 和 LLM 推理任务特点不同, 改进思路也不同:\",\"LLM 有 Prefill 和 Decode 两阶段，分别是compute boundary和memory boundary的; 而DiT 的计算和 Prefill 相似是compute boundary的.\",\"LLM 模型很大，而序列长度有限(虽然现在长文本需求也在增加);而 DiT 正好反过来，模型不大，推理时序列长度很长, TP的适用性较低.\",\"LLM 模型大多收敛到微调 Llama 架构, 但DiT架构则呈现出较大差异性, 框架统一难度较大.\",\"考虑到 DiT 的特点，本文提出一系列混合并行方式, 提供了一套优雅的开发接口，针对性解决了 DiT 模型更改难度高的问题，这套开发接口尽可能可能复用 diffusers 的现有逻辑，开发者通过一些 wrapper 实现复杂的混合并行，实现高效地扩展方法.\"]},\"160\":{\"h\":\"Overview\"},\"161\":{\"h\":\"DiT 主干网络混合并行\",\"t\":[\"xDiT支持四种基础并行策略以任何形式混合, 达到近似线性效果:\",\"Pipefusion Parallel\",\"Sequence Parallel\",\"Data Parallel\",\"CFG Parallel https://arxiv.org/abs/2207.12598\"]},\"162\":{\"h\":\"Parallel VAE\",\"t\":[\"针对扩散模型后处理的解码 VAE 模块 (解码器模块)在高分辨率图像生成时 OOM (Out Of Memory)问题，xDiT 实现了 Patch Parallel 版本的 VAE.\"]},\"163\":{\"h\":\"简单灵活的开发接口\"},\"164\":{\"h\":\"PipeFusion\",\"t\":[\"见前文\"]},\"165\":{\"h\":\"USP：混合序列并行\",\"t\":[\"见MLSys_分布式开发（选读） DeepSpeed-Ulysses部分\"]},\"166\":{\"h\":\"CFG Parallel\",\"t\":[\"Classifier-Free Guidance（CFG）是扩散模型领域的一个重要的技巧，可以提供更广泛的条件控制、减少训练负担、增强生成内容的质量和细节，以及提高模型的实用性和适应性.\",\"一个输入 prompt，使用 CFG 需要同时进行 unconditional guide 和 text guide 的生成, 就是DistriFusion 中的 split batch.\"]},\"167\":{\"h\":\"Hybrid Parallel\",\"t\":[\"xDiT 设计目标是扩展 DiT 推理过程到超大规模, 实现多机多卡, 不同并行方式混合在一起变得尤为重要.\",\"PipeFusion 和 Sequence 在图像内部的不同 Patch 间并行则较为复杂, 两种并行方式的混合使用，是 xDiT 核心创新点之一.\",\"PipeFusion利用过时的KV进行Attention计算，这使得PipeFusion无法像大型语言模型（LLM）那样轻松地实现并行策略的混合, 以pipe_degree=4，sp_degree=2的混合并行方法为例:\",\"标准 SP Attention实现，输入Q，K，V和输出O都是沿着序列维度切分，且切分方式一致。如果不同rank的输入patch没有重叠，每个micro step计算出fresh KV更新的位置在不同rank间也没有重叠.\",\"如下图所示，standard SP的KV Buffer中黄色部分是SP0 rank=0拥有的fresh KV，绿色部分是SP1 rank=1拥有的fresh KV，二者并不相同.\",\"在这个diffusion step内，device=0无法拿到P1,3,5,7的fresh KV进行计算，但是PipeFusion则需要在下一个diffusion step中，拥有上一个diffusion step全部的KV(保留旧有激活值避免大量通讯).\",\"standard SP只拥有1/sp_degree的fresh kv buffer，因此无法获得混合并行推理正确的结果.\",\"xDiT专门定制了序列并行的实现方式，以适应这种混合并行的需求.\",\"xDiT使用xFuserLongContextAttention把SP的中间结果存在KV Buffer内.\",\"效果如下图，每个micro-step SP执行完毕后，SP Group内不同rank设备的fresh KV是相互补充的.\",\"这样一个diffusion step后，SP Group所有设备的KV Buffer都更新成最新，供下一个Diffusion Step使用.\"]},\"168\":{\"h\":\"Parallel VAE\",\"t\":[\"VAE 模块在高清图片生成时, 会导致OOM(见https://github.com/huggingface/diffusers/issues/5924).\",\"在xDiT开发了 DistVAE解决了这个问题, 使用了两种关键策略:\",\"SP: 特征图分割成多个 Patch，并在不同设备上进行序列并行 VAE 解码, 中间激活所需的峰值内存减少到 1/N.\",\"分块输入处理: 输入特征图分割成块，并依次送入卷积运算符\"]},\"169\":{\"c\":[\"SOSD\"]},\"170\":{\"c\":[\"分布式系统\",\"分布式推理\",\"文生图\"]},\"171\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"172\":{\"h\":\"\"},\"173\":{\"h\":\"Posts\"}},\"dirtCount\":0,\"index\":[[\"特征图分割成多个\",{\"1\":{\"168\":1}}],[\"特点\",{\"0\":{\"148\":1}}],[\"供下一个diffusion\",{\"1\":{\"167\":1}}],[\"效果如下图\",{\"1\":{\"167\":1}}],[\"效率更高\",{\"1\":{\"31\":1}}],[\"拥有上一个diffusion\",{\"1\":{\"167\":1}}],[\"二者并不相同\",{\"1\":{\"167\":1}}],[\"绿色部分是sp1\",{\"1\":{\"167\":1}}],[\"且切分方式一致\",{\"1\":{\"167\":1}}],[\"标准\",{\"1\":{\"167\":1}}],[\"间并行则较为复杂\",{\"1\":{\"167\":1}}],[\"见https\",{\"1\":{\"168\":1}}],[\"见mlsys\",{\"1\":{\"165\":1}}],[\"见前文\",{\"1\":{\"164\":1}}],[\"见前文tp内容\",{\"1\":{\"65\":1}}],[\"混合序列并行\",{\"0\":{\"165\":1}}],[\"版本的\",{\"1\":{\"162\":1}}],[\"达到近似线性效果\",{\"1\":{\"161\":1}}],[\"开发者通过一些\",{\"1\":{\"159\":1}}],[\"开始\",{\"1\":{\"151\":1}}],[\"开始行和总计算行数\",{\"1\":{\"110\":1}}],[\"框架统一难度较大\",{\"1\":{\"159\":1}}],[\"架构\",{\"1\":{\"159\":1}}],[\"正好反过来\",{\"1\":{\"159\":1}}],[\"正如在vllm前都是基于hf\",{\"1\":{\"159\":1}}],[\"虽然现在长文本需求也在增加\",{\"1\":{\"159\":1}}],[\"虽然亚伯拉罕\",{\"1\":{\"140\":1}}],[\"于是\",{\"1\":{\"159\":1}}],[\"于是通信开销不得不成为主要矛盾\",{\"1\":{\"150\":1}}],[\"迫切需求一个性能好且易用性高的dit推理框架\",{\"1\":{\"159\":1}}],[\"推理过程到超大规模\",{\"1\":{\"167\":1}}],[\"推理时序列长度很长\",{\"1\":{\"159\":1}}],[\"推理任务特点不同\",{\"1\":{\"159\":1}}],[\"推理延迟极为严重\",{\"1\":{\"159\":1}}],[\"推理服务\",{\"0\":{\"145\":1}}],[\"机制的计算量也随之呈平方级膨胀\",{\"1\":{\"159\":1}}],[\"序列增长导致attention\",{\"1\":{\"159\":1}}],[\"序列并行\",{\"0\":{\"76\":1,\"114\":1},\"1\":{\"78\":1,\"116\":1}}],[\"异步修正\",{\"1\":{\"158\":1}}],[\"摘要\",{\"0\":{\"156\":1}}],[\"证明了distrifusion方法的泛用性\",{\"1\":{\"155\":1}}],[\"局限\",{\"1\":{\"154\":1}}],[\"^\",{\"1\":{\"153\":1}}],[\"它仅在新鲜区域上执行计算并产生相应的输出\",{\"1\":{\"153\":1}}],[\"它对已分割的查询\",{\"1\":{\"83\":1,\"121\":1}}],[\"线性\",{\"1\":{\"153\":1}}],[\"线性映射层\",{\"1\":{\"79\":1,\"117\":1}}],[\"时\",{\"1\":{\"153\":1}}],[\"时间复杂度是梦寐以求的o\",{\"1\":{\"140\":1}}],[\"时间复杂度\",{\"1\":{\"139\":1}}],[\"位移补丁并行\",{\"0\":{\"153\":1}}],[\"位移区块并行\",{\"1\":{\"152\":1}}],[\"或者\",{\"1\":{\"152\":1}}],[\"或者考虑让cpu也存储一部分模型数据\",{\"1\":{\"4\":1}}],[\"重点是把distrifusion方法移植到了dit模型上\",{\"1\":{\"155\":1}}],[\"重计算\",{\"1\":{\"152\":1}}],[\"重新计算\",{\"1\":{\"66\":1}}],[\"暂存\",{\"1\":{\"152\":1}}],[\"确实有相当一部分激活值完全可以\",{\"1\":{\"152\":1}}],[\"确定开始行\",{\"1\":{\"110\":1}}],[\"确定要计算的行数\",{\"1\":{\"110\":1}}],[\"确定\",{\"1\":{\"70\":1}}],[\"陈旧\",{\"1\":{\"152\":1}}],[\"核心创新点之一\",{\"1\":{\"167\":1}}],[\"核心\",{\"1\":{\"152\":1}}],[\"甚至超过计算时间\",{\"1\":{\"152\":1}}],[\"极高的通信开销\",{\"1\":{\"152\":1}}],[\"区块间同步传递中间激活信息\",{\"1\":{\"152\":1}}],[\"区块间引入交互又会带来过高的同步成本\",{\"1\":{\"149\":1}}],[\"独立计算各个区块并随后拼接\",{\"1\":{\"152\":1}}],[\"平均加速在2倍左右\",{\"1\":{\"151\":1}}],[\"难以并行加速\",{\"1\":{\"151\":1}}],[\"步数较大的去噪方式\",{\"1\":{\"151\":1}}],[\"步即可使用\",{\"1\":{\"13\":1}}],[\"说利用picard迭代以数据并行的方式并行化去噪步骤\",{\"1\":{\"151\":1}}],[\"作者还在这里cue了一下shih等人在2023提出的paradigms\",{\"1\":{\"151\":1}}],[\"作为输入\",{\"1\":{\"71\":1,\"72\":1,\"151\":1}}],[\"十亿次乘加运算\",{\"1\":{\"151\":1}}],[\"处理一张1024x1024的图像时\",{\"1\":{\"151\":1}}],[\"处理多模态任务的价值也不断被挖掘\",{\"1\":{\"146\":1}}],[\"ϵt​=ϵθ​\",{\"1\":{\"151\":1}}],[\"ϵt​\",{\"1\":{\"151\":1}}],[\"ϵθ​\",{\"1\":{\"151\":1}}],[\"例如文本\",{\"1\":{\"151\":1}}],[\"该方法可能不适用\",{\"1\":{\"154\":1}}],[\"该方法在主机设备之间构建self\",{\"1\":{\"90\":1,\"128\":1}}],[\"该模型需要经过数十到数百次的迭代去噪步骤\",{\"1\":{\"151\":1}}],[\"扩散模型通常会训练一个噪声预测神经网络模型\",{\"1\":{\"151\":1}}],[\"扩展代码使其能够使用2\",{\"1\":{\"109\":1}}],[\"倾向于使用allgather而非allreduce进行数据交互\",{\"1\":{\"150\":1}}],[\"成效不佳\",{\"1\":{\"150\":1}}],[\"预训练的扩散模型中提炼出生成模型\",{\"1\":{\"150\":1}}],[\"预测模型的执行时间\",{\"1\":{\"71\":1}}],[\"基于扩散模型与微分方程之间的联系\",{\"1\":{\"150\":1}}],[\"基础操作还有进一步优化的空间\",{\"1\":{\"90\":1,\"128\":1}}],[\"设计目标是扩展\",{\"1\":{\"167\":1}}],[\"设计无需训练的高效采样算法\",{\"1\":{\"150\":1}}],[\"设计由n个输入序列在p个可用设备上分区组成\",{\"1\":{\"84\":1,\"122\":1}}],[\"相似是compute\",{\"1\":{\"159\":1}}],[\"相关工作\",{\"0\":{\"150\":1}}],[\"相邻gpu对应位置进行通讯\",{\"1\":{\"12\":1}}],[\"边界处会出现明显的接缝和强烈的撕裂感\",{\"1\":{\"152\":1}}],[\"边界处会出现明显的接缝\",{\"1\":{\"149\":1}}],[\"会导致oom\",{\"1\":{\"168\":1}}],[\"会导致单块gpu利用率降低\",{\"1\":{\"63\":1}}],[\"会生成n个小图拼接而成的大图\",{\"1\":{\"149\":1}}],[\"放在n个device上进行推理\",{\"1\":{\"149\":1}}],[\"常规方法\",{\"1\":{\"149\":1}}],[\"加速效果较好\",{\"1\":{\"148\":1}}],[\"无需训练\",{\"1\":{\"148\":1}}],[\"无论哪一种模型\",{\"1\":{\"6\":1}}],[\"无论使用哪一种模型\",{\"1\":{\"4\":1}}],[\"无论在cv方向还是nlp等其他方向\",{\"1\":{\"4\":1}}],[\"工作属于是diffusion\",{\"1\":{\"147\":1}}],[\"像算子并行\",{\"1\":{\"146\":1}}],[\"像素与像素之间的高度关联性也难以像文本推理一样通过简单的切分矩阵实现并发\",{\"1\":{\"146\":1}}],[\"避免不必要的重计算\",{\"1\":{\"146\":1}}],[\"避免多轮采样带来的内存开销\",{\"1\":{\"146\":1}}],[\"利用相邻扩散步骤输入之间的高相似性\",{\"1\":{\"156\":1}}],[\"利用前一步扩散过程中稍有过时或\",{\"1\":{\"152\":1}}],[\"利用多个设备上的神经网络并行性来加速扩散过程\",{\"1\":{\"150\":1}}],[\"利用了diffusion过程相邻步之间feature\",{\"1\":{\"148\":1}}],[\"利用diffusion相邻step冗余考虑适当保存activate值\",{\"1\":{\"146\":1}}],[\"利用pipeline\",{\"1\":{\"53\":1}}],[\"目前主要只使用数据并行\",{\"1\":{\"150\":1}}],[\"目前优化主要集中在以下几点\",{\"1\":{\"150\":1}}],[\"目前考虑到的针对diffusion的优化主要集中于以下几个方面\",{\"1\":{\"146\":1}}],[\"目标是把红色块的数据广播到其余gpu对应的位置上\",{\"1\":{\"11\":1}}],[\"文生视频等多模态任务中的分布式推理服务就是在这种背景下被关注的\",{\"1\":{\"146\":1}}],[\"文生视频和文生图任务由于图形任务的整体性对大块内存的使用提出了更为严苛的要求\",{\"1\":{\"146\":1}}],[\"文生视频\",{\"0\":{\"145\":1}}],[\"文生图文生视频等潜力巨大的任务类型开始提上研究日程\",{\"1\":{\"146\":1}}],[\"文生图\",{\"0\":{\"145\":1,\"146\":1},\"1\":{\"146\":1},\"2\":{\"170\":1}}],[\"文章针对三个challenge提出了三个解决方法\",{\"1\":{\"93\":1,\"131\":1}}],[\"文章只对attention和fwd操作做了优化\",{\"1\":{\"90\":1,\"128\":1}}],[\"文章的训练测试规模比较小\",{\"1\":{\"90\":1,\"128\":1}}],[\"文章将自己的sp流程和pp\",{\"1\":{\"86\":1,\"124\":1}}],[\"套公式罢了\",{\"1\":{\"141\":1}}],[\"代码实现没什么好说的\",{\"1\":{\"141\":1}}],[\"代码实现\",{\"0\":{\"141\":1}}],[\"外层中括号意为高斯取整\",{\"1\":{\"140\":1}}],[\"棣美弗早于斯特林提出了一个类似的公式\",{\"1\":{\"140\":1}}],[\"斯特林的名字命名\",{\"1\":{\"140\":1}}],[\"斯特林公式估算的误差已经降到了十万分之一以下\",{\"1\":{\"140\":1}}],[\"斯特林公式的取值已经十分准确\",{\"1\":{\"140\":1}}],[\"斯特林公式\",{\"0\":{\"140\":1}}],[\"≈2πn​\",{\"1\":{\"140\":1}}],[\"所要存储的数据规模\",{\"1\":{\"139\":1}}],[\"所以ans也是为了省事儿用的int偷懒是可耻的\",{\"1\":{\"141\":1}}],[\"所以斯特林公式十分好用\",{\"1\":{\"140\":1}}],[\"所以可以给出函数workerthreadstart\",{\"1\":{\"110\":1}}],[\"所以可以在sequence维度上进行拆分\",{\"1\":{\"81\":1,\"119\":1}}],[\"所以难度并不算大~~\",{\"1\":{\"109\":1}}],[\"所以一种可取的方法是将一部中间过程结果丢弃\",{\"1\":{\"66\":1}}],[\"所以此时应采用fp32的全精度计算\",{\"1\":{\"26\":1}}],[\"所以在训练ai时我们希望能使用尽可能大的数据集\",{\"1\":{\"4\":1}}],[\"程序代码的复杂度和\",{\"1\":{\"139\":1}}],[\"随着序列长度的增加\",{\"1\":{\"157\":1}}],[\"随着分辨率的增加\",{\"1\":{\"151\":1}}],[\"随着transformer在文本生成方面逐步展现出的巨大潜力和chatgpt等生成式对话ai的逐步商业化\",{\"1\":{\"146\":1}}],[\"随着n的增大\",{\"1\":{\"140\":1}}],[\"随便算算就存爆了\",{\"1\":{\"139\":1}}],[\"随机选择\",{\"1\":{\"72\":1}}],[\"忘记了估计数据规模\",{\"1\":{\"139\":1}}],[\"完全无法满足题目要求\",{\"1\":{\"139\":1}}],[\"完成后可以通过make\",{\"1\":{\"109\":1}}],[\"完成一次周期内所有micro\",{\"1\":{\"64\":1}}],[\"举个栗子\",{\"1\":{\"139\":1}}],[\"更不可能过测试了\",{\"1\":{\"139\":1}}],[\"更新完成后\",{\"1\":{\"50\":1}}],[\"更新参数\",{\"1\":{\"49\":1}}],[\"更新模型参数\",{\"1\":{\"8\":1}}],[\"题目分析\",{\"0\":{\"139\":1}}],[\"题目引入\",{\"0\":{\"138\":1}}],[\"快写个程序帮助他\",{\"1\":{\"138\":1}}],[\"快速\",{\"1\":{\"69\":1}}],[\"持\",{\"1\":{\"138\":1}}],[\"ji\",{\"1\":{\"138\":1}}],[\"矜\",{\"1\":{\"138\":1}}],[\"男孩离开前告诉弯通\",{\"1\":{\"138\":1}}],[\"弯通又做梦了\",{\"1\":{\"138\":1}}],[\"数学\",{\"2\":{\"144\":1}}],[\"数学在计算机算法研究中的作用\",{\"0\":{\"137\":1}}],[\"数即一切\",{\"1\":{\"142\":1}}],[\"数据结构与算法\",{\"2\":{\"143\":1}}],[\"数据并行d\",{\"1\":{\"66\":1}}],[\"数据并行内存使用并不高效\",{\"1\":{\"31\":1}}],[\"数据并行比模型并行更好\",{\"1\":{\"31\":1}}],[\"数据并行\",{\"0\":{\"7\":1,\"63\":1},\"1\":{\"23\":1,\"69\":1}}],[\"数据分布式的一些基本想法\",{\"0\":{\"6\":1}}],[\"数据集越大\",{\"1\":{\"4\":1}}],[\"阶乘的位数估算\",{\"0\":{\"137\":1}}],[\"调用mandelbrotserial\",{\"1\":{\"110\":1}}],[\"调用自定义函数\",{\"1\":{\"19\":1}}],[\"直接处理最后部分\",{\"1\":{\"110\":1}}],[\"图优化\",{\"1\":{\"146\":1}}],[\"图块与图块\",{\"1\":{\"146\":1}}],[\"图像宽度和高度\",{\"1\":{\"110\":1}}],[\"图中f和f​互为共轭\",{\"1\":{\"80\":1,\"118\":1}}],[\"图中红色\",{\"1\":{\"11\":1}}],[\"图中0\",{\"1\":{\"8\":1}}],[\"复平面左上和右下两个点坐标\",{\"1\":{\"110\":1}}],[\"函数的各个参数\",{\"1\":{\"110\":1}}],[\"函数事实上是用来计算mandelbrot图像的\",{\"1\":{\"110\":1}}],[\"首先将输入xt​分割成n个块xt\",{\"1\":{\"153\":1}}],[\"首先将xt​分割成多个补丁xt\",{\"1\":{\"153\":1}}],[\"首先我们可以根据阅读mandelbrotserial\",{\"1\":{\"110\":1}}],[\"首先通过\",{\"1\":{\"49\":1}}],[\"任务实现\",{\"0\":{\"110\":1}}],[\"任务分析\",{\"0\":{\"109\":1}}],[\"回答性能是否明显高于8线程并解释原因\",{\"1\":{\"109\":1}}],[\"报告最后得出的8线程加速比\",{\"1\":{\"109\":1}}],[\"找到适应任何线程数的泛型分配方式\",{\"1\":{\"109\":1}}],[\"修改一开始的线程分配方式\",{\"1\":{\"109\":1}}],[\"验证并解释task2中提出的猜想\",{\"1\":{\"109\":1}}],[\"生成加速图\",{\"1\":{\"109\":1}}],[\"创建线程0和线程1\",{\"1\":{\"109\":1}}],[\"创建训练操作\",{\"1\":{\"14\":1}}],[\"整体框架已经在源码中基本完成了\",{\"1\":{\"109\":1}}],[\"~~\",{\"1\":{\"109\":1}}],[\"配置环境路径\",{\"1\":{\"108\":1}}],[\"解码\",{\"1\":{\"168\":1}}],[\"解码器模块\",{\"1\":{\"162\":1}}],[\"解压包\",{\"1\":{\"108\":1}}],[\"解决了高校attention实现\",{\"1\":{\"93\":1,\"131\":1}}],[\"很多环境配置会方便一些\",{\"1\":{\"108\":1}}],[\"很难做到即时通讯\",{\"1\":{\"18\":1}}],[\"本人使用os为ubuntu\",{\"1\":{\"108\":1}}],[\"本文方法\",{\"0\":{\"152\":1,\"158\":1}}],[\"本文方法基于patch并行性\",{\"1\":{\"150\":1}}],[\"本文又一次强调了模型分片到多个设备上\",{\"1\":{\"151\":1}}],[\"本文提出一系列混合并行方式\",{\"1\":{\"159\":1}}],[\"本文提出了一种新的并行范式\",{\"1\":{\"152\":1}}],[\"本文提出通过计算attn\",{\"1\":{\"95\":1,\"133\":1}}],[\"本文提出以分块方式执行self\",{\"1\":{\"90\":1,\"128\":1}}],[\"本文提出sequece\",{\"1\":{\"78\":1,\"116\":1}}],[\"本文设计出ring\",{\"1\":{\"87\":1,\"125\":1}}],[\"本文认为sp最主要的问题是跨设备如何计算sub\",{\"1\":{\"87\":1,\"125\":1}}],[\"本文假设h极大\",{\"1\":{\"79\":1,\"117\":1}}],[\"本文推导与假设中用到了以下几个参量\",{\"1\":{\"79\":1,\"117\":1}}],[\"本文以transformer结构为例估算activation\",{\"1\":{\"79\":1,\"117\":1}}],[\"本文引入了\",{\"1\":{\"68\":1}}],[\"本文探讨了以下几种因素对效率的影响\",{\"1\":{\"62\":1}}],[\"本文中研究人员提出了一种名为ptd\",{\"1\":{\"62\":1}}],[\"本文将其记作2ψ+2ψ+kψ\",{\"1\":{\"26\":1}}],[\"本文将其统称为残差状态\",{\"1\":{\"24\":1}}],[\"本文指出了ring\",{\"1\":{\"9\":1}}],[\"本文通过在不同数量卡上训练结果进行说明\",{\"1\":{\"9\":1}}],[\"本文还指出dp代码重构成本较大\",{\"1\":{\"9\":1}}],[\"环境配置完成后就可以clone\",{\"1\":{\"108\":1}}],[\"环境配置\",{\"0\":{\"108\":1}}],[\"环注意\",{\"1\":{\"90\":1,\"128\":1}}],[\"公开课\",{\"2\":{\"105\":1,\"112\":1}}],[\"典型并行方式时序图\",{\"0\":{\"97\":1}}],[\"典型数据并行的流程\",{\"0\":{\"8\":1}}],[\"导致gpu增加时内存开销增加\",{\"1\":{\"157\":1}}],[\"导致生成单张高分辨率图像的延迟在实际应用中变得过高\",{\"1\":{\"151\":1}}],[\"导致相对高的还原计算代价\",{\"1\":{\"96\":1,\"134\":1}}],[\"导致模型训练失效\",{\"1\":{\"26\":1}}],[\"针对扩散模型后处理的解码\",{\"1\":{\"162\":1}}],[\"针对性解决了\",{\"1\":{\"159\":1}}],[\"针对llm的并行方法特点是\",{\"1\":{\"150\":1}}],[\"针对这个问题\",{\"1\":{\"94\":1,\"132\":1}}],[\"针对mlp\",{\"1\":{\"60\":1}}],[\"按照ring拓扑结构进行计算也会有约一半的cpu处于等待状态\",{\"1\":{\"94\":1,\"132\":1}}],[\"部分sp方法\",{\"1\":{\"92\":1,\"130\":1}}],[\"还有一种可能\",{\"1\":{\"139\":1}}],[\"还是建议使用linux系统做lab\",{\"1\":{\"108\":1}}],[\"还是未知数\",{\"1\":{\"90\":1,\"128\":1}}],[\"还需要最小化\",{\"1\":{\"47\":1}}],[\"能否在大规模训练时取得想象中的线性效果\",{\"1\":{\"90\":1,\"128\":1}}],[\"好吧也需要堆卡出奇迹\",{\"1\":{\"90\":1,\"128\":1}}],[\"天才般的想法\",{\"1\":{\"90\":1,\"128\":1}}],[\"老生常谈的transformer长序列要求\",{\"1\":{\"89\":1,\"127\":1}}],[\"感觉想法来源于ring\",{\"1\":{\"87\":1,\"125\":1}}],[\"主干网络混合并行\",{\"0\":{\"161\":1}}],[\"主要工作\",{\"0\":{\"87\":1,\"125\":1}}],[\"主攻cv\",{\"1\":{\"0\":1}}],[\"用巨量的计算换取极高的生成能力\",{\"1\":{\"150\":1}}],[\"用精度换效率\",{\"1\":{\"149\":1}}],[\"用commnication换取序列长度\",{\"1\":{\"87\":1,\"125\":1}}],[\"用于transformer模型层的剩余模块中的后续操作\",{\"1\":{\"84\":1,\"122\":1}}],[\"用pytorch等使用虚拟内存分配方式的库时\",{\"1\":{\"29\":1}}],[\"注意力计算后\",{\"1\":{\"84\":1,\"122\":1}}],[\"注意与上张图不同\",{\"1\":{\"55\":1}}],[\"收集\",{\"1\":{\"84\":1,\"122\":1}}],[\"接下来\",{\"1\":{\"84\":1,\"122\":1}}],[\"接受\",{\"1\":{\"71\":1,\"72\":1}}],[\"键\",{\"1\":{\"83\":1,\"84\":1,\"121\":1,\"122\":1}}],[\"需求大token和长sequence的输入\",{\"1\":{\"83\":1,\"121\":1}}],[\"需要同时进行\",{\"1\":{\"166\":1}}],[\"需要跨多个计算设备并行化dit推理\",{\"1\":{\"157\":1}}],[\"需要通信汇总不同worker上的attention结果\",{\"1\":{\"95\":1,\"133\":1}}],[\"需要通过同步来保障语义对等\",{\"1\":{\"60\":1}}],[\"需要缓存下来\",{\"1\":{\"79\":1,\"117\":1}}],[\"需要分别创建保存q和k的矩阵\",{\"1\":{\"79\":1,\"117\":1}}],[\"需要综合流水线并行p\",{\"1\":{\"66\":1}}],[\"需要额外的12ψ\",{\"1\":{\"26\":1}}],[\"长上下文的保存有助于llm推理\",{\"1\":{\"83\":1,\"121\":1}}],[\"长序列在llm应用中非常重要\",{\"1\":{\"83\":1,\"121\":1}}],[\"链接在这儿\",{\"1\":{\"82\":1,\"120\":1}}],[\"官号本身讲的也非常不错\",{\"1\":{\"82\":1,\"120\":1}}],[\"先是一个reduce\",{\"1\":{\"81\":1,\"119\":1}}],[\"zhihu\",{\"1\":{\"82\":1,\"120\":1}}],[\"zhuanlan\",{\"1\":{\"82\":1,\"120\":1}}],[\"z2h​\",{\"1\":{\"81\":1,\"119\":1}}],[\"z1h​\",{\"1\":{\"81\":1,\"119\":1}}],[\"zero3\",{\"0\":{\"39\":1}}],[\"zero2\",{\"0\":{\"38\":1}}],[\"zero1\",{\"0\":{\"37\":1},\"1\":{\"39\":1}}],[\"zero考虑可以使用将模型内存切分成多分\",{\"1\":{\"33\":1}}],[\"zero使用不同的方式优化这两种内存使用\",{\"1\":{\"25\":1}}],[\"zero\",{\"0\":{\"22\":1,\"30\":1,\"31\":1,\"32\":1,\"36\":1,\"40\":1,\"44\":1,\"48\":1},\"1\":{\"22\":1,\"23\":1,\"44\":1,\"45\":1,\"47\":1,\"49\":3,\"50\":2}}],[\"具体实现\",{\"1\":{\"90\":1,\"128\":1}}],[\"具体实现图例见下\",{\"1\":{\"79\":1,\"117\":1}}],[\"具体过程见下\",{\"1\":{\"81\":1,\"119\":1}}],[\"得到mandelbrotserial\",{\"1\":{\"110\":1}}],[\"得到w1​和w2​后进行reduce\",{\"1\":{\"81\":1,\"119\":1}}],[\"得到结果z1h​b1r​和z2h​b2r​\",{\"1\":{\"81\":1,\"119\":1}}],[\"得到结果ya1c​​和ya2c​\",{\"1\":{\"81\":1,\"119\":1}}],[\"​被重新分散回前一步的陈旧激活atl​\",{\"1\":{\"153\":1}}],[\"​之后\",{\"1\":{\"153\":1}}],[\"​=g\",{\"1\":{\"81\":1,\"119\":1}}],[\"​​​=layernorm\",{\"1\":{\"81\":1,\"119\":1}}],[\"​w1​\",{\"1\":{\"81\":1,\"119\":1}}],[\"​\",{\"1\":{\"81\":4,\"119\":4,\"153\":6}}],[\"y1\",{\"1\":{\"110\":2}}],[\"y1s​\",{\"1\":{\"81\":3,\"119\":3}}],[\"y0\",{\"1\":{\"110\":2}}],[\"y\",{\"1\":{\"81\":1,\"119\":1}}],[\"y2s​\",{\"1\":{\"81\":3,\"119\":3}}],[\"ya2c​\",{\"1\":{\"81\":1,\"119\":1}}],[\"ya1c​\",{\"1\":{\"81\":1,\"119\":1}}],[\"ya\",{\"1\":{\"81\":2,\"119\":2}}],[\"yzwv​=layernorm\",{\"1\":{\"81\":1,\"119\":1}}],[\"详细说明拆分步骤\",{\"1\":{\"81\":1,\"119\":1}}],[\"拆分后如下图\",{\"1\":{\"81\":1,\"119\":1}}],[\"拆分到多个\",{\"1\":{\"69\":1}}],[\"拆分到\",{\"1\":{\"69\":1}}],[\"反向是all\",{\"1\":{\"81\":1,\"119\":1}}],[\"反向时不做操作\",{\"1\":{\"80\":1,\"118\":1}}],[\"反向时执行all\",{\"1\":{\"80\":1,\"118\":1}}],[\"反向传播逻辑\",{\"1\":{\"49\":1}}],[\"反向传播梯度\",{\"1\":{\"49\":1}}],[\"模块在高清图片生成时\",{\"1\":{\"168\":1}}],[\"模块\",{\"1\":{\"162\":1}}],[\"模块总的大小为\",{\"1\":{\"79\":1,\"117\":1}}],[\"模型更改难度高的问题\",{\"1\":{\"159\":1}}],[\"模型大多收敛到微调\",{\"1\":{\"159\":1}}],[\"模型不大\",{\"1\":{\"159\":1}}],[\"模型很大\",{\"1\":{\"159\":1}}],[\"模型ϵθ​会将xt​\",{\"1\":{\"151\":1}}],[\"模型尺寸较小\",{\"1\":{\"150\":1}}],[\"模型压缩等等\",{\"1\":{\"146\":1}}],[\"模型有\",{\"1\":{\"55\":1}}],[\"模型状态\",{\"1\":{\"47\":1}}],[\"模型并行\",{\"1\":{\"23\":1,\"69\":1}}],[\"模型绕过mp方法按层切分时可能带来的通讯瓶颈\",{\"1\":{\"6\":1}}],[\"模型\",{\"1\":{\"6\":1}}],[\"元素个数都是as2b个\",{\"1\":{\"79\":1,\"117\":1}}],[\"元素个数为sbh个\",{\"1\":{\"79\":1,\"117\":1}}],[\"原始self\",{\"1\":{\"79\":1,\"117\":1}}],[\"原本单a100跑不了的模型\",{\"1\":{\"78\":1,\"116\":1}}],[\"总结\",{\"0\":{\"142\":1,\"154\":1}}],[\"总大小为4sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"总大小为19sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"总大小是\",{\"1\":{\"79\":1,\"117\":1}}],[\"总的大小为\",{\"1\":{\"79\":1,\"117\":1}}],[\"总共大小为4sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"总是绕不开训练集和测试集这两部分\",{\"1\":{\"4\":1}}],[\"qp​\",{\"1\":{\"95\":1,\"133\":1}}],[\"qkv\",{\"1\":{\"84\":2,\"122\":2}}],[\"qkt矩阵相乘\",{\"1\":{\"79\":1,\"117\":1}}],[\"query\",{\"1\":{\"79\":1,\"117\":1}}],[\"q\",{\"1\":{\"79\":2,\"83\":1,\"84\":1,\"117\":2,\"121\":1,\"122\":1}}],[\"忽略输入输出的memory\",{\"1\":{\"79\":1,\"117\":1}}],[\"认为2sb远小于sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"经过sp优化后可以在单a100上运行了\",{\"1\":{\"78\":1,\"116\":1}}],[\"红色线表示baseline\",{\"1\":{\"78\":1,\"116\":1}}],[\"蓝色部分表示不同参数级别模型中需要用于保存parameter和optimizer\",{\"1\":{\"78\":1,\"116\":1}}],[\"选读\",{\"0\":{\"75\":1,\"113\":1},\"1\":{\"165\":1}}],[\"选择收集到一定量的梯度\",{\"1\":{\"19\":1}}],[\"依据如下规则进行更新\",{\"1\":{\"72\":1}}],[\"依据是\",{\"1\":{\"19\":1}}],[\"自动查找高效的\",{\"1\":{\"72\":1}}],[\"自动并行\",{\"0\":{\"67\":1}}],[\"执行allgather操作\",{\"1\":{\"153\":1}}],[\"执行all\",{\"1\":{\"83\":1,\"121\":1}}],[\"执行过程中有两步\",{\"1\":{\"81\":1,\"119\":1}}],[\"执行优化器\",{\"0\":{\"72\":1},\"1\":{\"72\":1}}],[\"执行模拟器\",{\"0\":{\"71\":1}}],[\"执行同步训练\",{\"1\":{\"14\":1}}],[\"比如\",{\"1\":{\"70\":1}}],[\"当n很大的时候\",{\"1\":{\"140\":1}}],[\"当计算self\",{\"1\":{\"90\":1,\"128\":1}}],[\"当然\",{\"1\":{\"70\":1}}],[\"当在同一个bucket中的梯度的hook都被调用后\",{\"1\":{\"19\":1}}],[\"种\",{\"1\":{\"70\":1}}],[\"至少有\",{\"1\":{\"70\":1}}],[\"假设加速是否与线程数线性相关并加以验证\",{\"1\":{\"109\":1}}],[\"假设模型包含\",{\"1\":{\"70\":1}}],[\"假设有ψ个参数\",{\"1\":{\"26\":1}}],[\"假设有4块gpu\",{\"1\":{\"10\":1}}],[\"维度的拆分\",{\"1\":{\"70\":1}}],[\"都是临时的不成体系的方案\",{\"1\":{\"159\":1}}],[\"都可以对其取$\",{\"1\":{\"140\":1}}],[\"都能支持2种或者更多的拆分方式\",{\"1\":{\"70\":1}}],[\"都支持\",{\"1\":{\"70\":1}}],[\"都会有大量的空闲时间\",{\"1\":{\"55\":1}}],[\"除去batchnormalization\",{\"1\":{\"70\":1}}],[\"损失函数\",{\"1\":{\"70\":1}}],[\"矩阵乘法\",{\"1\":{\"70\":1}}],[\"嵌入通过参与计算设备之间的高度优化的全对全集合\",{\"1\":{\"84\":1,\"122\":1}}],[\"嵌入中\",{\"1\":{\"84\":1,\"122\":1}}],[\"嵌入\",{\"1\":{\"70\":1}}],[\"归一化\",{\"1\":{\"70\":1}}],[\"全连接层\",{\"1\":{\"70\":1}}],[\"全局batchsize\",{\"1\":{\"66\":1}}],[\"池化\",{\"1\":{\"70\":1}}],[\"卷积或注意力层\",{\"1\":{\"153\":1}}],[\"卷积\",{\"1\":{\"70\":1}}],[\"搜索空间对于不同的\",{\"1\":{\"70\":1}}],[\"搜索算法\",{\"1\":{\"69\":1}}],[\"概述\",{\"0\":{\"70\":1}}],[\"马尔可夫链蒙特卡洛\",{\"1\":{\"69\":1}}],[\"增强生成内容的质量和细节\",{\"1\":{\"166\":1}}],[\"增量的执行模拟器\",{\"1\":{\"69\":1}}],[\"增大模型规模可以提升模型效果\",{\"1\":{\"53\":1}}],[\"空间复杂度\",{\"1\":{\"139\":1}}],[\"空间\",{\"1\":{\"69\":1}}],[\"关键的问题是\",{\"1\":{\"69\":1}}],[\"描述单个\",{\"1\":{\"69\":1}}],[\"描述参数如何并行\",{\"1\":{\"69\":1}}],[\"描述训练数据如何并行\",{\"1\":{\"69\":1}}],[\"描述不同的\",{\"1\":{\"69\":1}}],[\"引入了一个\",{\"1\":{\"68\":1}}],[\"吞吐量\",{\"1\":{\"66\":1}}],[\"取决于最终的throughput\",{\"1\":{\"66\":1}}],[\"$来得到这个n进制数的位数\",{\"1\":{\"140\":1}}],[\"$\",{\"1\":{\"66\":1,\"108\":1}}],[\"有\",{\"1\":{\"159\":1}}],[\"有两个layernorm层\",{\"1\":{\"79\":1,\"117\":1}}],[\"有$\",{\"1\":{\"66\":1}}],[\"有了对模型算子切分的方法\",{\"1\":{\"6\":1}}],[\"令\",{\"1\":{\"66\":1}}],[\"性能分析\",{\"0\":{\"66\":1}}],[\"越大\",{\"1\":{\"64\":1}}],[\"中间激活所需的峰值内存减少到\",{\"1\":{\"168\":1}}],[\"中间结果全部丢掉\",{\"1\":{\"57\":1}}],[\"中的\",{\"1\":{\"166\":1}}],[\"中的不同层发放到多块gpu上\",{\"1\":{\"64\":1}}],[\"考虑到\",{\"1\":{\"159\":1}}],[\"考虑到激活值规模庞大\",{\"1\":{\"149\":1}}],[\"考虑dit模型特点\",{\"1\":{\"158\":1}}],[\"考虑gelu非线性\",{\"1\":{\"81\":1,\"119\":1}}],[\"考虑tp并行\",{\"1\":{\"80\":1,\"118\":1}}],[\"考虑使用模型并行\",{\"1\":{\"66\":1}}],[\"考虑张量并行\",{\"0\":{\"65\":1}}],[\"考虑流水线并行\",{\"0\":{\"64\":1}}],[\"考虑一些综合利用并行方式的策略\",{\"1\":{\"6\":1}}],[\"理论上gpu数量等于batch\",{\"1\":{\"63\":1}}],[\"问题\",{\"0\":{\"157\":1},\"1\":{\"63\":1,\"162\":1}}],[\"问题是\",{\"1\":{\"55\":1}}],[\"贡献\",{\"0\":{\"62\":1}}],[\"获取嵌入向量\",{\"1\":{\"60\":1}}],[\"|\",{\"1\":{\"60\":1}}],[\"<math\",{\"1\":{\"141\":1}}],[\"<stdio\",{\"1\":{\"141\":1}}],[\"<<\",{\"1\":{\"139\":1}}],[\"<args>检验正确与否\",{\"1\":{\"109\":1}}],[\"<\",{\"1\":{\"60\":1,\"139\":1}}],[\"掉的\",{\"1\":{\"60\":1}}],[\"掉的输入\",{\"1\":{\"60\":1}}],[\"掉\",{\"1\":{\"60\":1}}],[\">>\",{\"1\":{\"139\":1}}],[\">=\",{\"1\":{\"60\":1}}],[\">\",{\"1\":{\"60\":2,\"110\":16,\"141\":1,\"152\":2}}],[\"编码\",{\"1\":{\"60\":1}}],[\"实际情况种\",{\"1\":{\"70\":1}}],[\"实际上就是要求阶乘的位数倒也没有拐弯抹角\",{\"1\":{\"139\":1}}],[\"实际上是按行切分的\",{\"1\":{\"79\":1,\"117\":1}}],[\"实际上一般也确实极大\",{\"1\":{\"79\":1,\"117\":1}}],[\"实际上\",{\"1\":{\"60\":1}}],[\"实现多机多卡\",{\"1\":{\"167\":1}}],[\"实现了\",{\"1\":{\"162\":1}}],[\"实现高效地扩展方法\",{\"1\":{\"159\":1}}],[\"实现复杂的混合并行\",{\"1\":{\"159\":1}}],[\"实现将两个图片都拉到8线程时7\",{\"1\":{\"109\":1}}],[\"实现依赖于p2p的通信模式\",{\"1\":{\"95\":1,\"133\":1}}],[\"实现mlp的整体的tensor\",{\"1\":{\"60\":1}}],[\"实现对超大规模模型的支持\",{\"1\":{\"53\":1}}],[\"实现超线性优化\",{\"1\":{\"23\":1}}],[\"即常数时间复杂度\",{\"1\":{\"140\":1}}],[\"即可轻松求得n\",{\"1\":{\"140\":1}}],[\"即向下取整\",{\"1\":{\"140\":1}}],[\"即使在n很小的时候\",{\"1\":{\"140\":1}}],[\"即将图像的不同空间交给不同线程计算\",{\"1\":{\"109\":1}}],[\"即sequence\",{\"1\":{\"81\":1,\"119\":1}}],[\"即只考虑中间过程的memory\",{\"1\":{\"79\":1,\"117\":1}}],[\"即在权重矩阵上执行\",{\"1\":{\"60\":1}}],[\"即输入与参数矩阵相乘\",{\"1\":{\"60\":1}}],[\"另一个全对全集合将注意力计算的输出上下文张量转换为序列\",{\"1\":{\"84\":1,\"122\":1}}],[\"另一种方法是沿着它的列\",{\"1\":{\"60\":1}}],[\"另一部分动态存储临时数据\",{\"1\":{\"43\":1}}],[\"另一部分用来测试模型效果\",{\"1\":{\"4\":1}}],[\"来促进区块间的交互\",{\"1\":{\"152\":1}}],[\"来进行存储的话\",{\"1\":{\"79\":1,\"117\":1}}],[\"来确定当前卡要\",{\"1\":{\"60\":1}}],[\"来实现tensor\",{\"1\":{\"60\":1}}],[\"来对尽可能多的标签进行刻画\",{\"1\":{\"4\":1}}],[\"+nlog10​\",{\"1\":{\"140\":1}}],[\"+1\",{\"1\":{\"140\":2}}],[\"+\",{\"1\":{\"60\":1,\"79\":1,\"109\":1,\"117\":1,\"140\":1,\"141\":2}}],[\"张量并行等并行技术并不适合diffusion模型\",{\"1\":{\"149\":1}}],[\"张量并行\",{\"0\":{\"58\":1}}],[\"减少训练负担\",{\"1\":{\"166\":1}}],[\"减少通信成本\",{\"1\":{\"158\":1}}],[\"减少采样步数\",{\"1\":{\"146\":1}}],[\"减少计算时内存占用\",{\"1\":{\"57\":1}}],[\"减少内存释放和分配的时间\",{\"1\":{\"35\":1}}],[\"减少内存使用\",{\"1\":{\"33\":1}}],[\"结果收敛不明显\",{\"1\":{\"56\":1}}],[\"类似cpu指令执行\",{\"1\":{\"56\":1}}],[\"切成更小的micro\",{\"1\":{\"56\":1}}],[\"切分小patch分到不同设备处理\",{\"1\":{\"150\":1}}],[\"切分成若干份给不同的gpu\",{\"1\":{\"8\":1}}],[\"切分模型让不同gpu延后计算开始时间\",{\"1\":{\"6\":1}}],[\"切分后的任务如何再将结果重新合在一起\",{\"1\":{\"3\":1}}],[\"块\",{\"1\":{\"55\":1}}],[\"个批次的forward和backward运算\",{\"1\":{\"55\":1}}],[\"个\",{\"1\":{\"55\":2,\"70\":1}}],[\"个人介绍\",{\"0\":{\"0\":1}}],[\"kv是相互补充的\",{\"1\":{\"167\":1}}],[\"kv进行计算\",{\"1\":{\"167\":1}}],[\"kv\",{\"1\":{\"167\":3}}],[\"kv更新的位置在不同rank间也没有重叠\",{\"1\":{\"167\":1}}],[\"ke\",{\"1\":{\"138\":1}}],[\"key\",{\"1\":{\"79\":1,\"117\":1}}],[\"kr​\",{\"1\":{\"95\":1,\"133\":1}}],[\"k矩阵一样\",{\"1\":{\"79\":1,\"117\":1}}],[\"k\",{\"1\":{\"55\":4,\"79\":2,\"83\":1,\"84\":1,\"117\":2,\"121\":1,\"122\":1,\"167\":1}}],[\"第\",{\"1\":{\"55\":1}}],[\"然后将n个device的结果合成为一个全分辨率的结果\",{\"1\":{\"149\":1}}],[\"然后就会发现\",{\"1\":{\"139\":1}}],[\"然后通过数据并行技术扩展集群上进行加速\",{\"1\":{\"66\":1}}],[\"然后与权重矩阵进行矩阵乘法\",{\"1\":{\"60\":1}}],[\"然后流水线并行\",{\"1\":{\"56\":1}}],[\"然后每份放到一块\",{\"1\":{\"55\":1}}],[\"然后给不同的gpu去算每一个单独的部分\",{\"1\":{\"6\":1}}],[\"份\",{\"1\":{\"55\":1}}],[\"事实上task中给的提示还是比较明显的\",{\"1\":{\"109\":1}}],[\"事实上是tp\",{\"1\":{\"55\":1}}],[\"事实上对于采用哪种并行模式\",{\"1\":{\"6\":1}}],[\"最大迭代次数\",{\"1\":{\"110\":1}}],[\"最后\",{\"1\":{\"83\":1,\"121\":1,\"153\":1}}],[\"最优\",{\"1\":{\"72\":1}}],[\"最终gpipe通过更大规模的模型和更大的batch\",{\"1\":{\"53\":1}}],[\"最小通信量为\",{\"1\":{\"47\":1}}],[\"最小化cpu和gpu之间的通信量\",{\"1\":{\"46\":1}}],[\"提供了一套优雅的开发接口\",{\"1\":{\"159\":1}}],[\"提升存储效率\",{\"1\":{\"158\":1}}],[\"提升训练精度\",{\"1\":{\"6\":1}}],[\"提高了模型并行模式下的设备利用率\",{\"1\":{\"53\":1}}],[\"方案\",{\"1\":{\"53\":1}}],[\"方法无法满足日益增长的参数量需求\",{\"1\":{\"23\":1}}],[\"必须引入分布式系统\",{\"1\":{\"53\":1}}],[\"必须在gpu上进行\",{\"1\":{\"47\":1}}],[\"仅将属于其\",{\"1\":{\"50\":1}}],[\"仅需\",{\"1\":{\"13\":1}}],[\"到\",{\"1\":{\"50\":2}}],[\"广播更新后的参数\",{\"1\":{\"49\":1}}],[\"广播到所有其他进程\",{\"1\":{\"14\":1}}],[\"删除\",{\"1\":{\"49\":1}}],[\"上标\",{\"1\":{\"153\":1}}],[\"上进行\",{\"1\":{\"60\":1}}],[\"上对参数进行类似\",{\"1\":{\"50\":1}}],[\"上并行更新对应的参数\",{\"1\":{\"50\":1}}],[\"上可用\",{\"1\":{\"50\":1}}],[\"上\",{\"1\":{\"50\":1,\"53\":1,\"55\":2,\"69\":2,\"153\":1}}],[\"上更新参数\",{\"1\":{\"49\":1}}],[\"上分配全精度参数\",{\"1\":{\"49\":1}}],[\"上分配半精度参数\",{\"1\":{\"49\":1}}],[\"上的开销\",{\"1\":{\"139\":1}}],[\"上的梯度\",{\"1\":{\"49\":1}}],[\"上的\",{\"1\":{\"47\":1}}],[\"上的内存\",{\"1\":{\"45\":1}}],[\"层视为线性层来处理\",{\"1\":{\"60\":1}}],[\"层时\",{\"1\":{\"60\":1}}],[\"层\",{\"1\":{\"49\":1,\"55\":2}}],[\"判断当前进程是否拥有第\",{\"1\":{\"49\":1}}],[\"xdit使用xfuserlongcontextattention把sp的中间结果存在kv\",{\"1\":{\"167\":1}}],[\"xdit专门定制了序列并行的实现方式\",{\"1\":{\"167\":1}}],[\"xdit支持四种基础并行策略以任何形式混合\",{\"1\":{\"161\":1}}],[\"xdit\",{\"0\":{\"159\":1},\"1\":{\"159\":2,\"162\":1,\"167\":2}}],[\"xt\",{\"1\":{\"153\":4}}],[\"xt​\",{\"1\":{\"151\":2,\"153\":2}}],[\"xt−1​=update\",{\"1\":{\"151\":1}}],[\"x1\",{\"1\":{\"110\":2}}],[\"x1s​\",{\"1\":{\"81\":2,\"119\":2}}],[\"x0\",{\"1\":{\"110\":2}}],[\"xvf\",{\"1\":{\"108\":1}}],[\"x2s​\",{\"1\":{\"81\":2,\"119\":2}}],[\"x=\",{\"1\":{\"81\":1,\"119\":1}}],[\"xavier\",{\"1\":{\"60\":1}}],[\"x\",{\"1\":{\"49\":7,\"60\":2,\"81\":1,\"119\":1,\"140\":1}}],[\"下文将介绍一种用数学方法巧妙估算阶乘结果规模的方式\",{\"1\":{\"139\":1}}],[\"下载包\",{\"1\":{\"108\":1}}],[\"下载解压一下编译环境就好啦\",{\"1\":{\"108\":1}}],[\"下面是两种重计算策略的对比\",{\"1\":{\"96\":1,\"134\":1}}],[\"下图中绿色部分表示不同参数级别模型中需要用于保存activation需要的显存大小\",{\"1\":{\"78\":1,\"116\":1}}],[\"下图gpu与cpu二次通信应该是从cpu到gpu\",{\"1\":{\"49\":1}}],[\"下也能保持效率\",{\"1\":{\"46\":1}}],[\"之间进行\",{\"1\":{\"50\":1}}],[\"之间的移动\",{\"1\":{\"45\":1}}],[\"之后\",{\"1\":{\"49\":1}}],[\"消除大部分通信成本\",{\"1\":{\"49\":1}}],[\"消除了数据和模型并行训练中的内存冗余\",{\"1\":{\"23\":1}}],[\"此处省略了时间步t和条件c的输入\",{\"1\":{\"153\":1}}],[\"此处x切分仅作示意\",{\"1\":{\"79\":1,\"117\":1}}],[\"此过程不会增加开销\",{\"1\":{\"90\":1,\"128\":1}}],[\"此外\",{\"1\":{\"49\":1}}],[\"此时不再提供任何其他新的输入\",{\"1\":{\"64\":1}}],[\"此时\",{\"1\":{\"11\":1}}],[\"只保留最开始的输入\",{\"1\":{\"57\":1}}],[\"只需要在gpu内存中临时保存少量的梯度\",{\"1\":{\"49\":1}}],[\"只有将\",{\"1\":{\"47\":1}}],[\"只有那些计算复杂度低于o\",{\"1\":{\"47\":1}}],[\"只有很少一部分被过程中产生的冗余量使用了\",{\"1\":{\"25\":1}}],[\"过程中\",{\"1\":{\"49\":1,\"50\":1}}],[\"由于去噪状态迅速变化\",{\"1\":{\"154\":1}}],[\"由于我之前已经测试过最大数据规模\",{\"1\":{\"141\":1}}],[\"由于块计算比块传输需要更长的时间\",{\"1\":{\"90\":1,\"128\":1}}],[\"由于该方法将环中主机设备之间的key\",{\"1\":{\"90\":1,\"128\":1}}],[\"由于fp16参数已经位于gpu上\",{\"1\":{\"49\":1}}],[\"由于每个机器\",{\"1\":{\"19\":1}}],[\"单卡推理必然无法满足需求\",{\"1\":{\"159\":1}}],[\"单卡策略\",{\"0\":{\"49\":1}}],[\"单gpu无法满足实际应用的延迟要求\",{\"1\":{\"157\":1}}],[\"单身狗进化\",{\"1\":{\"138\":1}}],[\"单batch\",{\"1\":{\"56\":1}}],[\"单批量以这种顺序进行计算\",{\"1\":{\"55\":1}}],[\"单机gpu内存有限\",{\"1\":{\"53\":1}}],[\"单就这一块部分内存而言\",{\"1\":{\"31\":1}}],[\"才能达成最小通信量策略\",{\"1\":{\"47\":1}}],[\"v和输出o都是沿着序列维度切分\",{\"1\":{\"167\":1}}],[\"void\",{\"1\":{\"110\":1}}],[\"vocabutility\",{\"1\":{\"60\":1}}],[\"vocab\",{\"1\":{\"60\":9}}],[\"vocabparallelembedding\",{\"1\":{\"60\":2}}],[\"v1\",{\"1\":{\"108\":4}}],[\"v1s​\",{\"1\":{\"81\":1,\"119\":1}}],[\"vr+1​​workerr+1的覆盖\",{\"1\":{\"95\":1,\"133\":1}}],[\"vr​\",{\"1\":{\"95\":1,\"133\":1}}],[\"v2s​\",{\"1\":{\"81\":1,\"119\":1}}],[\"v矩阵的大小之前没有统计\",{\"1\":{\"79\":1,\"117\":1}}],[\"vae\",{\"0\":{\"162\":1,\"168\":1},\"1\":{\"162\":2,\"168\":2}}],[\"value数据\",{\"1\":{\"94\":1,\"132\":1}}],[\"value\",{\"1\":{\"79\":1,\"90\":6,\"117\":1,\"128\":6}}],[\"variance\",{\"1\":{\"47\":1}}],[\"v\",{\"1\":{\"79\":3,\"83\":1,\"84\":2,\"117\":3,\"121\":1,\"122\":2}}],[\"visible\",{\"1\":{\"14\":1}}],[\"发送到\",{\"1\":{\"47\":2,\"50\":1}}],[\"权重更新等\",{\"1\":{\"47\":1}}],[\"如下图所示\",{\"1\":{\"167\":1}}],[\"如u\",{\"1\":{\"151\":1}}],[\"如将高分辨率图像压缩为低分辨率的潜在表示\",{\"1\":{\"150\":1}}],[\"如ring\",{\"1\":{\"92\":1,\"130\":1}}],[\"如何快速搜索\",{\"1\":{\"69\":1}}],[\"如何将一个\",{\"1\":{\"69\":2}}],[\"如何合理的使用上面的多种并行技术一直是一个困难的命题\",{\"1\":{\"62\":1}}],[\"如进行backward更新\",{\"1\":{\"66\":1}}],[\"如microbatch的大小\",{\"1\":{\"62\":1}}],[\"如图中所示\",{\"1\":{\"47\":1}}],[\"如范数计算\",{\"1\":{\"47\":1}}],[\"如果不同rank的输入patch没有重叠\",{\"1\":{\"167\":1}}],[\"如果已经到最后部分不够切分\",{\"1\":{\"110\":1}}],[\"如果该遇到整除要加一行避免遗漏\",{\"1\":{\"110\":1}}],[\"如果简单分块计算约会有1\",{\"1\":{\"94\":1,\"132\":1}}],[\"如果还是使用半精度进行运算\",{\"1\":{\"26\":1}}],[\"如果我们希望分布式系统在ai训练中发挥他的力量\",{\"1\":{\"6\":1}}],[\"如果我们一台机器存储不开我们的数据集\",{\"1\":{\"4\":1}}],[\"如果我们一台机器\",{\"1\":{\"4\":1}}],[\"和distrifusion相同操作如patch分割\",{\"1\":{\"158\":1}}],[\"和阶乘计算结果的误差程度\",{\"1\":{\"140\":1}}],[\"和ulysses一样\",{\"1\":{\"86\":1,\"124\":1}}],[\"和值\",{\"1\":{\"83\":1,\"84\":1,\"121\":1,\"122\":1}}],[\"和q\",{\"1\":{\"79\":1,\"117\":1}}],[\"和attention的qkv部分\",{\"1\":{\"80\":1,\"118\":1}}],[\"和attention\",{\"1\":{\"79\":1,\"117\":1}}],[\"和selective\",{\"1\":{\"78\":1,\"116\":1}}],[\"和memory\",{\"1\":{\"66\":1}}],[\"和数据并行时\",{\"1\":{\"66\":1}}],[\"和\",{\"1\":{\"47\":2,\"55\":1,\"60\":1,\"68\":1,\"70\":1,\"71\":1,\"72\":1,\"153\":1,\"159\":2,\"166\":1,\"167\":1}}],[\"的生成\",{\"1\":{\"166\":1}}],[\"的现有逻辑\",{\"1\":{\"159\":1}}],[\"的特点\",{\"1\":{\"159\":1}}],[\"的输入序列长度日益增长\",{\"1\":{\"159\":1}}],[\"的激活信息\",{\"1\":{\"152\":1}}],[\"的确没有本文出色\",{\"1\":{\"151\":1}}],[\"的位数\",{\"1\":{\"140\":1}}],[\"的位数就是距离弯通脱单的天数\",{\"1\":{\"138\":1}}],[\"的弯通想知道自己还有多久能脱单\",{\"1\":{\"138\":1}}],[\"的阶乘\",{\"1\":{\"138\":1}}],[\"的代码\",{\"1\":{\"110\":1}}],[\"的开头和结尾插入计时代码\",{\"1\":{\"109\":1}}],[\"的并发事实\",{\"1\":{\"109\":1}}],[\"的并行方法\",{\"1\":{\"69\":1}}],[\"的执行顺序\",{\"1\":{\"70\":1}}],[\"的效能\",{\"1\":{\"62\":1}}],[\"的模型参数\",{\"1\":{\"55\":1}}],[\"的方案\",{\"1\":{\"53\":1}}],[\"的平均梯度\",{\"1\":{\"50\":1}}],[\"的\",{\"1\":{\"47\":2,\"50\":1,\"55\":1,\"60\":2,\"72\":1}}],[\"的通信带宽\",{\"1\":{\"47\":1}}],[\"的计算和\",{\"1\":{\"159\":1}}],[\"的计算复杂度都是o\",{\"1\":{\"47\":1}}],[\"的计算才能转移到cpu上\",{\"1\":{\"47\":1}}],[\"的额外通讯开销\",{\"1\":{\"39\":1}}],[\"训练的计算复杂度通常为o\",{\"1\":{\"47\":1}}],[\"训练效果越好\",{\"1\":{\"4\":1}}],[\"允许将cpu优化器步骤与gpu计算重叠\",{\"1\":{\"46\":1}}],[\"延迟一步的参数更新\",{\"1\":{\"46\":1}}],[\"其余保持图像默认参数\",{\"1\":{\"110\":1}}],[\"其复杂度为o\",{\"1\":{\"47\":1}}],[\"其速度比现有技术快6倍\",{\"1\":{\"46\":1}}],[\"其中n表示设备的数量\",{\"1\":{\"153\":1}}],[\"其中t表示总步数\",{\"1\":{\"151\":1}}],[\"其中内层中括号标记运算顺序\",{\"1\":{\"140\":1}}],[\"其中x的大小为s×b×h\",{\"1\":{\"81\":1,\"119\":1}}],[\"其中m为模型大小\",{\"1\":{\"47\":1}}],[\"其中m和b分别为模型规模和\",{\"1\":{\"46\":1}}],[\"其中k取决于模型\",{\"1\":{\"26\":1}}],[\"高效去噪\",{\"1\":{\"150\":1}}],[\"高效的cpu优化器\",{\"1\":{\"46\":1}}],[\"高估了gpu的存储性能\",{\"1\":{\"45\":1}}],[\"采用p2p通信\",{\"1\":{\"158\":1}}],[\"采用流水线并行方式来协调不同设备上的计算和通信\",{\"1\":{\"158\":1}}],[\"采用流水线并行方式来协调通信和计算\",{\"1\":{\"156\":1}}],[\"采用megatron的模型并行方式\",{\"1\":{\"41\":1}}],[\"采取了两种优化措施\",{\"1\":{\"46\":1}}],[\"较大\",{\"1\":{\"46\":1}}],[\"次计算\",{\"1\":{\"46\":2}}],[\"防止通信瓶颈\",{\"1\":{\"46\":1}}],[\"防止cpu性能瓶颈\",{\"1\":{\"46\":1}}],[\"以适应这种混合并行的需求\",{\"1\":{\"167\":1}}],[\"以pipe\",{\"1\":{\"167\":1}}],[\"以及提高模型的实用性和适应性\",{\"1\":{\"166\":1}}],[\"以保持区块间的相互作用\",{\"1\":{\"154\":1}}],[\"以近似ϵθ​\",{\"1\":{\"153\":1}}],[\"以准备下一步的完整激活atl​\",{\"1\":{\"153\":1}}],[\"以预测xt​中的相应噪声ϵt​\",{\"1\":{\"151\":1}}],[\"以获得最终的清晰图像x0​\",{\"1\":{\"151\":1}}],[\"以往方法\",{\"0\":{\"149\":1}}],[\"以逐块的方式进行注意力和前馈网络计算\",{\"1\":{\"90\":1,\"128\":1}}],[\"以使每个gpu接收完整的序列\",{\"1\":{\"83\":1,\"121\":1}}],[\"以mlp为例\",{\"1\":{\"81\":1,\"119\":1}}],[\"以节省内存\",{\"1\":{\"66\":1}}],[\"以在三个关键方面达到最优化\",{\"1\":{\"46\":1}}],[\"以处理本地\",{\"1\":{\"14\":1}}],[\"通信开销较大\",{\"1\":{\"157\":1}}],[\"通信成本会超过分布式计算带来的节省\",{\"1\":{\"149\":1}}],[\"通常只涉及一个gpu\",{\"1\":{\"149\":1}}],[\"通常还会加一些约束条件\",{\"1\":{\"70\":1}}],[\"通常建议在gpu内存满足的情况下最大化模型并行\",{\"1\":{\"66\":1}}],[\"通常建议在集群上采用流水线并行\",{\"1\":{\"66\":1}}],[\"通常在一个节点内部采用不超过g张卡做张量并行\",{\"1\":{\"66\":1}}],[\"通常pipeline\",{\"1\":{\"64\":1}}],[\"通常张量并行只适用于一个multi\",{\"1\":{\"62\":1}}],[\"通过复用一步旧的特征图来为当前步骤提供上下文\",{\"1\":{\"156\":1}}],[\"通过这个公式我们可以轻易发现xt−1​和xt​的强依赖关系\",{\"1\":{\"151\":1}}],[\"通过代入n\",{\"1\":{\"140\":1}}],[\"通过斯特林公式我们可以简单估算阶乘的位数\",{\"1\":{\"140\":1}}],[\"通过给空闲的worker来fetch部分key\",{\"1\":{\"94\":1,\"132\":1}}],[\"通过将序列整体切分成小的chunk\",{\"1\":{\"87\":1,\"125\":1}}],[\"通过对比可以发现\",{\"1\":{\"78\":1,\"116\":1}}],[\"通过编排forward\",{\"1\":{\"64\":1}}],[\"通过获取当前\",{\"1\":{\"60\":1}}],[\"通过结合上述两种方法\",{\"1\":{\"60\":1}}],[\"通过分析确定了cpu和gpu设备之间的最佳计算和数据划分策略\",{\"1\":{\"46\":1}}],[\"通讯\",{\"1\":{\"152\":1}}],[\"通讯效率低\",{\"1\":{\"18\":1}}],[\"通讯成本会比较高昂\",{\"1\":{\"6\":1}}],[\"与distrifusion相比\",{\"1\":{\"158\":1}}],[\"与此同时\",{\"1\":{\"153\":1}}],[\"与标准transformer相比\",{\"1\":{\"90\":1,\"128\":1}}],[\"与已知的transformer架构一样\",{\"1\":{\"84\":1,\"122\":1}}],[\"与\",{\"1\":{\"45\":1,\"47\":1}}],[\"与zero\",{\"1\":{\"41\":1}}],[\"尽量减少数据在\",{\"1\":{\"45\":1}}],[\"优化扩散模型的神经推理\",{\"1\":{\"150\":1}}],[\"优化器计算要求cpu进行o\",{\"1\":{\"46\":1}}],[\"优化器状态和梯度\",{\"1\":{\"49\":1}}],[\"优化器状态\",{\"1\":{\"6\":1,\"50\":1}}],[\"优化\",{\"1\":{\"45\":1}}],[\"背景知识\",{\"0\":{\"151\":1}}],[\"背景\",{\"0\":{\"45\":1,\"89\":1,\"92\":1,\"127\":1,\"130\":1}}],[\"将最终输出聚合在一起\",{\"1\":{\"153\":1}}],[\"将执行两个异步操作\",{\"1\":{\"153\":1}}],[\"将图像分割成多个区块\",{\"1\":{\"152\":1,\"154\":1}}],[\"将图像split成n个patch\",{\"1\":{\"149\":1}}],[\"将阶乘结果用表存储\",{\"1\":{\"139\":1}}],[\"将成为任何一台机器的噩梦\",{\"1\":{\"139\":1}}],[\"将原本一个batch的数据分解为多个更小的micro\",{\"1\":{\"64\":1}}],[\"将\",{\"1\":{\"60\":1}}],[\"将weight矩阵a分割\",{\"1\":{\"60\":1}}],[\"将模型切分成一连串stage\",{\"1\":{\"53\":1}}],[\"将自己的\",{\"1\":{\"50\":1}}],[\"将更新后的参数复制回\",{\"1\":{\"49\":1}}],[\"将梯度和优化器状态在不同的\",{\"1\":{\"50\":1}}],[\"将梯度复制到\",{\"1\":{\"49\":1}}],[\"将梯度减少到拥有该层的进程\",{\"1\":{\"49\":1}}],[\"将fp16参数存储在gpu上\",{\"1\":{\"49\":1}}],[\"将该图分割为cpu和gpu设备之间的部分\",{\"1\":{\"47\":1}}],[\"将数据池分为两部分\",{\"1\":{\"43\":1}}],[\"将百度的\",{\"1\":{\"13\":1}}],[\"内存都有重要的影响\",{\"1\":{\"62\":1}}],[\"内存复制到\",{\"1\":{\"49\":1}}],[\"内存中的fp16参数中\",{\"1\":{\"49\":1}}],[\"内存中\",{\"1\":{\"49\":1,\"50\":2}}],[\"内存碎片通过对不同寿命的内存进行整理\",{\"1\":{\"35\":1}}],[\"内存其实根本无法使用\",{\"1\":{\"29\":1}}],[\"多gpu乃至多机dit部署是必然要求\",{\"1\":{\"159\":1}}],[\"多gpu通常仅用于批量推理\",{\"1\":{\"149\":1}}],[\"多线程不一定高效率\",{\"1\":{\"109\":1}}],[\"多机分布序列维度\",{\"1\":{\"90\":1,\"128\":1}}],[\"多机集群\",{\"1\":{\"62\":1}}],[\"多个\",{\"1\":{\"69\":1}}],[\"多batch训练时\",{\"1\":{\"56\":1}}],[\"多batch可以提速\",{\"1\":{\"56\":1}}],[\"多卡策略\",{\"0\":{\"50\":1}}],[\"多次通讯的方式收集数据\",{\"1\":{\"33\":1}}],[\"多台机器之间通讯耗费的时间会不会比原先计算的时间更长\",{\"1\":{\"3\":1}}],[\"再重新计算这些中间结果\",{\"1\":{\"57\":1}}],[\"再进行通讯获取参数\",{\"1\":{\"31\":1}}],[\"再做聚合和同步\",{\"1\":{\"19\":1}}],[\"而\",{\"1\":{\"159\":1}}],[\"而序列长度有限\",{\"1\":{\"159\":1}}],[\"而dit\",{\"1\":{\"159\":1}}],[\"而不用\",{\"1\":{\"152\":1}}],[\"而不是像megtron\",{\"1\":{\"86\":1,\"124\":1}}],[\"而且\",{\"1\":{\"140\":1}}],[\"而且只优化了自注意力部分\",{\"1\":{\"87\":1,\"125\":1}}],[\"而非模型整体来说\",{\"1\":{\"90\":1,\"128\":1}}],[\"而非对所有输入\",{\"1\":{\"55\":1}}],[\"而通信成本大大增长\",{\"1\":{\"63\":1}}],[\"而流水线并行则几乎只用于更大的模型\",{\"1\":{\"62\":1}}],[\"而其余的计算\",{\"1\":{\"47\":1}}],[\"而gpu需进行o\",{\"1\":{\"46\":1}}],[\"而是点对点通信\",{\"1\":{\"158\":1}}],[\"而是切分后再流水线并行\",{\"1\":{\"56\":1}}],[\"而是对输出进行通讯和更新\",{\"1\":{\"41\":1}}],[\"而是直接替换\",{\"1\":{\"12\":1}}],[\"而在其他gpu需要使用其进行计算时\",{\"1\":{\"31\":1}}],[\"两种并行方式的混合使用\",{\"1\":{\"167\":1}}],[\"两种并行都需要存储模型状态变量\",{\"1\":{\"31\":1}}],[\"两阶段\",{\"1\":{\"159\":1}}],[\"两次扩散时间步的图相似度是很高的\",{\"1\":{\"152\":1}}],[\"两者使用均不高效\",{\"1\":{\"31\":1}}],[\"因为一般causal\",{\"1\":{\"94\":1,\"132\":1}}],[\"因为最小训练单位还是一个token\",{\"1\":{\"90\":1,\"128\":1}}],[\"因为我们在attention之前矩阵乘就已经切分了squence\",{\"1\":{\"90\":1,\"128\":1}}],[\"因为每一个分发计算都需要完全copy所有数据\",{\"1\":{\"31\":1}}],[\"因为通讯更少\",{\"1\":{\"31\":1}}],[\"因此无法获得混合并行推理正确的结果\",{\"1\":{\"167\":1}}],[\"因此保存fa的结果便可计算三者的矩阵\",{\"1\":{\"96\":1,\"134\":1}}],[\"因此将其命名\",{\"1\":{\"90\":1,\"128\":1}}],[\"因此这部分计算不需要与cpu进行通信\",{\"1\":{\"49\":1}}],[\"因此\",{\"1\":{\"19\":1,\"49\":1}}],[\"因此不同机器之间处理的bucket的顺序将会不一致\",{\"1\":{\"19\":1}}],[\"也会是比较大的开销\",{\"1\":{\"139\":1}}],[\"也就是端到端的计算能够正确的完成\",{\"1\":{\"64\":1}}],[\"也就是单计算仅需4ψ\",{\"1\":{\"26\":1}}],[\"也可以考虑每个gpu都各自保存和重算部分数据\",{\"1\":{\"33\":1}}],[\"也可以是一个batch\",{\"1\":{\"8\":1}}],[\"存储持续整个训练过程\",{\"1\":{\"50\":1}}],[\"存储momentum和variance\",{\"1\":{\"26\":1}}],[\"存储模型的参数和输入\",{\"1\":{\"26\":1}}],[\"均采用fp16半精度运算\",{\"1\":{\"26\":1}}],[\"一层transformer的memory总的大小为\",{\"1\":{\"79\":1,\"117\":1}}],[\"一般会通过重计算的方式降低显存占用\",{\"1\":{\"78\":1,\"116\":1}}],[\"一般来说\",{\"1\":{\"26\":1}}],[\"一个输入\",{\"1\":{\"166\":1}}],[\"一个linear\",{\"1\":{\"79\":1,\"117\":1}}],[\"一个高效的并行策略搜索引擎\",{\"1\":{\"68\":1}}],[\"一个更加全面的并行策略搜索空间\",{\"1\":{\"68\":1}}],[\"一种方法是沿着其行\",{\"1\":{\"60\":1}}],[\"一旦梯度在\",{\"1\":{\"50\":1}}],[\"一部分存储大批量的计算数据\",{\"1\":{\"43\":1}}],[\"一张gpu\",{\"1\":{\"4\":1}}],[\"要是用递归或者循环写阶乘\",{\"1\":{\"139\":1}}],[\"要注意分割的时候会不会漏行\",{\"1\":{\"109\":1}}],[\"要在合理时间内找到最优解依然比较困难\",{\"1\":{\"70\":1}}],[\"要服务于计算速度内存变化就会发生膨胀\",{\"1\":{\"26\":1}}],[\"要用多少张卡进行并行\",{\"1\":{\"6\":1}}],[\"看上去这是仅仅由1变2的内存保留变化\",{\"1\":{\"26\":1}}],[\"论文方法\",{\"0\":{\"93\":1,\"131\":1}}],[\"论文背景\",{\"0\":{\"86\":1,\"124\":1}}],[\"论文做如下符号说明\",{\"1\":{\"81\":1,\"119\":1}}],[\"论文基于当前的\",{\"1\":{\"72\":1}}],[\"论文提出从\",{\"1\":{\"69\":1}}],[\"论文提出了一种名为efficiency的offload策略\",{\"1\":{\"46\":1}}],[\"论文中提出token\",{\"1\":{\"94\":1,\"132\":1}}],[\"论文中提到的mp就是传统的mp方案\",{\"1\":{\"55\":1}}],[\"论文中所提到的dp背景本篇blog前文均有提及\",{\"1\":{\"17\":1}}],[\"论文假设有30\",{\"1\":{\"29\":1}}],[\"论文以adam优化器为例说明了模型造成的内存浪费是一件难以接受的事情\",{\"1\":{\"26\":1}}],[\"临时缓冲区和不可用的碎片内存所消耗\",{\"1\":{\"24\":1}}],[\"剩余内存被激活\",{\"1\":{\"24\":1}}],[\"大家可以简单跑一下这个程序\",{\"1\":{\"139\":1}}],[\"大家都对ml有一些基本的认识了\",{\"1\":{\"6\":1}}],[\"大小为h×4h和4h×h\",{\"1\":{\"81\":1,\"119\":1}}],[\"大小为8sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"大小也是2sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"大多数\",{\"1\":{\"70\":1}}],[\"大op\",{\"1\":{\"69\":1}}],[\"大模型即使的带宽小的情况下\",{\"1\":{\"33\":1}}],[\"大部分内存都被用于模型本身数据的存储和运算\",{\"1\":{\"25\":1}}],[\"大部分内存\",{\"1\":{\"24\":1}}],[\"流水线并行技术将一个模型\",{\"1\":{\"64\":1}}],[\"流水线并行中的schedule\",{\"1\":{\"62\":1}}],[\"流水线并行\",{\"0\":{\"51\":1},\"1\":{\"24\":1}}],[\"简单灵活的开发接口\",{\"0\":{\"163\":1}}],[\"简单增加机器数量也用处不大\",{\"1\":{\"24\":1}}],[\"简介\",{\"0\":{\"69\":1,\"83\":1,\"121\":1}}],[\"简称sp\",{\"1\":{\"78\":1,\"116\":1}}],[\"简称pp\",{\"1\":{\"6\":1}}],[\"简称fsdp\",{\"1\":{\"6\":1}}],[\"简称tp\",{\"1\":{\"6\":1}}],[\"简称mp\",{\"1\":{\"6\":1}}],[\"简称dp\",{\"1\":{\"6\":1}}],[\"巨量参数可以大幅提升nlp处理能力\",{\"1\":{\"24\":1}}],[\"同时复用前一步骤的预计算激活值\",{\"1\":{\"154\":1}}],[\"同时本文还指出\",{\"1\":{\"151\":1}}],[\"同时从前一个设备接收key\",{\"1\":{\"90\":1,\"128\":1}}],[\"同时从前一个主机接收key\",{\"1\":{\"90\":1,\"128\":1}}],[\"同时重新在序列维度上进行分区\",{\"1\":{\"83\":1,\"121\":1}}],[\"同时\",{\"1\":{\"72\":1}}],[\"同时输入\",{\"1\":{\"70\":1}}],[\"同时确保准确性\",{\"1\":{\"46\":1}}],[\"同时最大限度地节省\",{\"1\":{\"45\":1}}],[\"同时又有12ψ个byte\",{\"1\":{\"26\":1}}],[\"同时保持高效率\",{\"1\":{\"23\":1}}],[\"同时保持了低通信量和高计算粒度\",{\"1\":{\"23\":1}}],[\"同时对分布式开发\",{\"1\":{\"0\":1}}],[\"零冗余优化器\",{\"1\":{\"23\":1}}],[\"现有的序列并行方法依托内存通讯\",{\"1\":{\"83\":1,\"121\":1}}],[\"现有的dp\",{\"1\":{\"83\":1,\"121\":1}}],[\"现有的pp\",{\"1\":{\"24\":1}}],[\"现有系统在模型训练中内存消耗的全部范围\",{\"1\":{\"24\":1}}],[\"现有dp\",{\"1\":{\"23\":1}}],[\"现在假定我们要做一个训练任务\",{\"1\":{\"6\":1}}],[\"深度学习模型在训练万亿级别参数时存在根本性限制\",{\"1\":{\"23\":1}}],[\"集合通讯库\",{\"0\":{\"20\":1}}],[\"则直接标记为ready\",{\"1\":{\"19\":1}}],[\"对a\",{\"1\":{\"153\":1}}],[\"对a进行列切分的tensor并行\",{\"1\":{\"81\":1,\"119\":1}}],[\"对squence做切分时的原则\",{\"1\":{\"90\":1,\"128\":1}}],[\"对b进行行切分的tensor并行\",{\"1\":{\"81\":1,\"119\":1}}],[\"对x按sequence维度切分\",{\"1\":{\"81\":1,\"119\":1}}],[\"对通信\",{\"1\":{\"62\":1}}],[\"对数据按层切分\",{\"1\":{\"55\":1}}],[\"对数据并行中存储的fp16参数进行切分\",{\"1\":{\"39\":1}}],[\"对数据并行中存储的fp16梯度进行切分\",{\"1\":{\"38\":1}}],[\"对应大小\",{\"1\":{\"79\":1,\"117\":1}}],[\"对应输入的元素大小为2sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"对应的计算公式如下\",{\"1\":{\"81\":1,\"119\":1}}],[\"对应的嵌入向量处理成\",{\"1\":{\"60\":1}}],[\"对应的每个\",{\"1\":{\"50\":1}}],[\"对应位置数据不再做相加\",{\"1\":{\"12\":1}}],[\"对切分后部分输入分开运算\",{\"1\":{\"41\":1}}],[\"对优化器本身存储的fp32数据进行切分\",{\"1\":{\"37\":1}}],[\"对于步数极少的方法\",{\"1\":{\"154\":1}}],[\"对于低分辨率图像\",{\"1\":{\"154\":1}}],[\"对于每一层l和每个设备i\",{\"1\":{\"153\":1}}],[\"对于单卡内存需求来说\",{\"1\":{\"90\":1,\"128\":1}}],[\"对于单个\",{\"1\":{\"69\":3}}],[\"对于内循环\",{\"1\":{\"90\":1,\"128\":1}}],[\"对于内存使用和kernel计算效率\",{\"1\":{\"62\":1}}],[\"对于非tp并行的部分在sequence维度都是相互独立的\",{\"1\":{\"81\":1,\"119\":1}}],[\"对于两块的输入并没有并行操作\",{\"1\":{\"80\":1,\"118\":1}}],[\"对于self\",{\"1\":{\"79\":1,\"117\":1}}],[\"对于一个元素的mask只用1个bytes\",{\"1\":{\"79\":1,\"117\":1}}],[\"对于linear需要保存输入的activation大小为2sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"对于attention\",{\"1\":{\"79\":1,\"117\":1}}],[\"对于attention块来说\",{\"1\":{\"79\":1,\"117\":1}}],[\"对于attention模块\",{\"1\":{\"79\":1,\"117\":1}}],[\"对于多个gpu集群\",{\"1\":{\"66\":1}}],[\"对于临时缓存采用开一段固定大小内存的方式进行反复存储\",{\"1\":{\"34\":1}}],[\"对于大型模型\",{\"1\":{\"24\":1}}],[\"对于没有参与计算的参数\",{\"1\":{\"19\":1}}],[\"哪些参数没有参与计算\",{\"1\":{\"19\":1}}],[\"记录哪些参数参与计算\",{\"1\":{\"19\":1}}],[\"前一步的陈旧激活被加深了颜色\",{\"1\":{\"153\":1}}],[\"前文提及的基础操作还有进一步优化的空间\",{\"1\":{\"92\":1,\"130\":1}}],[\"前向传播逻辑\",{\"1\":{\"49\":1}}],[\"前向传播\",{\"1\":{\"49\":1}}],[\"前向传播结束后从输出开始遍历计算图\",{\"1\":{\"19\":1}}],[\"前面所提及的方法\",{\"1\":{\"6\":1}}],[\"后向传播的顺序与梯度更新的顺序大致可认为是相同的\",{\"1\":{\"19\":1}}],[\"改进思路也不同\",{\"1\":{\"159\":1}}],[\"改进做法\",{\"0\":{\"19\":1}}],[\"改进了\",{\"1\":{\"13\":1}}],[\"传统分批方法\",{\"1\":{\"149\":1}}],[\"传统做法\",{\"0\":{\"18\":1}}],[\"传统的dp在带来使用大数据集可能的同时\",{\"1\":{\"9\":1}}],[\"不再像distrifusion每个时间步的通信都是scatter给每个设备\",{\"1\":{\"158\":1}}],[\"不再赘述\",{\"1\":{\"17\":1}}],[\"不难发现只要我们给出startrow\",{\"1\":{\"110\":1}}],[\"不需要引入太多通讯开销\",{\"1\":{\"150\":1}}],[\"不需要线程之间进行响应和同步\",{\"1\":{\"109\":1}}],[\"不需要进行额外通讯\",{\"1\":{\"60\":1}}],[\"不做赘述\",{\"1\":{\"89\":1,\"127\":1}}],[\"不够高效\",{\"1\":{\"83\":1,\"121\":1}}],[\"不包含模型参数大小和优化器中状态大小\",{\"1\":{\"79\":1,\"117\":1}}],[\"不过由于我们使用的a是同一个\",{\"1\":{\"60\":1}}],[\"不是magatron\",{\"1\":{\"55\":1}}],[\"不同并行方式混合在一起变得尤为重要\",{\"1\":{\"167\":1}}],[\"不同属性如何并行\",{\"1\":{\"69\":1}}],[\"不同gpu之间需要进行定期同步\",{\"1\":{\"64\":1}}],[\"不同的并行策略\",{\"1\":{\"62\":1}}],[\"不同参数的梯度在后向调度的不同位置计算\",{\"1\":{\"49\":1}}],[\"不同迭代中\",{\"1\":{\"19\":1}}],[\"不得不保存的模型内容\",{\"1\":{\"24\":1}}],[\"不切分数据的方式\",{\"1\":{\"6\":1}}],[\"就是distrifusion\",{\"1\":{\"166\":1}}],[\"就是采用高精度的算法\",{\"1\":{\"139\":1}}],[\"就是将单一计算机节点要做的任务分布在多个计算机节点上完成的\",{\"1\":{\"3\":1}}],[\"就已经溢出了\",{\"1\":{\"139\":1}}],[\"就可以完成计算了\",{\"1\":{\"110\":1}}],[\"就可以解决这个问题\",{\"1\":{\"109\":1}}],[\"就可以实现从本机训练到分布式训练的部署\",{\"1\":{\"16\":1}}],[\"就放在第\",{\"1\":{\"55\":1}}],[\"就调用allreduce对该bucket进行通信\",{\"1\":{\"19\":1}}],[\"unconditional\",{\"1\":{\"166\":1}}],[\"unique\",{\"0\":{\"47\":1}}],[\"ulysses部分\",{\"1\":{\"165\":1}}],[\"ulysses的核心设计\",{\"0\":{\"84\":1,\"122\":1}}],[\"ulysses还使用另一个all\",{\"1\":{\"83\":1,\"121\":1}}],[\"ulysses将各个样本在序列维度上分割给参与的gpu\",{\"1\":{\"83\":1,\"121\":1}}],[\"ulysses\",{\"0\":{\"82\":1,\"120\":1},\"1\":{\"82\":1,\"120\":1}}],[\"usp\",{\"0\":{\"165\":1}}],[\"using\",{\"1\":{\"52\":1,\"59\":1,\"61\":1}}],[\"user\",{\"1\":{\"20\":1}}],[\"update\",{\"1\":{\"16\":1,\"47\":2,\"49\":1}}],[\"uber\",{\"1\":{\"9\":1}}],[\"boundary的\",{\"1\":{\"159\":2}}],[\"boundary和memory\",{\"1\":{\"159\":1}}],[\"bin\",{\"1\":{\"108\":1}}],[\"billion\",{\"1\":{\"44\":1,\"59\":1}}],[\"block的副本发送到ring中的下一个设备\",{\"1\":{\"90\":1,\"128\":1}}],[\"block发送到下一个主机\",{\"1\":{\"90\":1,\"128\":1}}],[\"block遍历主机ring\",{\"1\":{\"90\":1,\"128\":1}}],[\"block\",{\"1\":{\"90\":3,\"128\":3}}],[\"block通信与compute重叠\",{\"1\":{\"90\":1,\"128\":1}}],[\"blockwise\",{\"1\":{\"88\":1,\"126\":1}}],[\"beyond\",{\"1\":{\"68\":1}}],[\"b考虑\",{\"1\":{\"66\":1}}],[\"bytes和8sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"bytes即可\",{\"1\":{\"79\":1,\"117\":1}}],[\"bytes\",{\"1\":{\"79\":13,\"80\":2,\"117\":13,\"118\":2}}],[\"by\",{\"1\":{\"60\":1}}],[\"bk\",{\"1\":{\"55\":2}}],[\"bwd后\",{\"1\":{\"47\":1}}],[\"bwd\",{\"1\":{\"47\":1,\"49\":2,\"50\":1}}],[\"b为有效batch\",{\"1\":{\"47\":1}}],[\"b\",{\"1\":{\"31\":1,\"33\":1,\"66\":2}}],[\"buffer都更新成最新\",{\"1\":{\"167\":1}}],[\"buffer内\",{\"1\":{\"167\":1}}],[\"buffer\",{\"1\":{\"167\":1}}],[\"buffer中黄色部分是sp0\",{\"1\":{\"167\":1}}],[\"bubble\",{\"1\":{\"64\":1}}],[\"bubble大小也有影响\",{\"1\":{\"62\":1}}],[\"bubble的大小\",{\"1\":{\"62\":1}}],[\"buﬀers\",{\"0\":{\"28\":1,\"34\":1,\"42\":1}}],[\"bucketing\",{\"1\":{\"19\":1}}],[\"bf16\",{\"1\":{\"26\":1}}],[\"balancing的调度算法\",{\"1\":{\"94\":1,\"132\":1}}],[\"batch的数量\",{\"1\":{\"64\":1}}],[\"batch的操作\",{\"1\":{\"64\":1}}],[\"batch过小\",{\"1\":{\"63\":1}}],[\"batches\",{\"0\":{\"56\":1}}],[\"batch\",{\"1\":{\"46\":4,\"49\":3,\"56\":2,\"64\":1,\"66\":2,\"166\":1}}],[\"batch较小时\",{\"1\":{\"18\":1}}],[\"backward\",{\"1\":{\"16\":2,\"49\":4,\"55\":1,\"60\":1}}],[\"broadcastglobalvariableshook\",{\"1\":{\"14\":1}}],[\"broadcast\",{\"1\":{\"13\":1,\"49\":1}}],[\"9\",{\"1\":{\"16\":1}}],[\"8倍加速比的效果\",{\"1\":{\"109\":1}}],[\"8个线程\",{\"1\":{\"109\":1}}],[\"80g\",{\"1\":{\"78\":1,\"116\":1}}],[\"8位指数位\",{\"1\":{\"26\":2}}],[\"8\",{\"1\":{\"16\":1}}],[\"7的fresh\",{\"1\":{\"167\":1}}],[\"763\",{\"1\":{\"151\":1}}],[\"71828182846\",{\"1\":{\"141\":1}}],[\"7位尾数位\",{\"1\":{\"26\":1}}],[\"7\",{\"1\":{\"16\":1,\"109\":1}}],[\"652206513\",{\"1\":{\"82\":1,\"120\":1}}],[\"6sbh\",{\"1\":{\"80\":1,\"118\":1}}],[\"6\",{\"1\":{\"16\":1,\"109\":1}}],[\"5924\",{\"1\":{\"168\":1}}],[\"5as2b\",{\"1\":{\"79\":1,\"117\":1}}],[\"5位指数位\",{\"1\":{\"26\":1}}],[\"5\",{\"1\":{\"16\":1,\"109\":1,\"167\":1}}],[\"34+5has​\",{\"1\":{\"79\":1,\"81\":1,\"117\":1,\"119\":1}}],[\"3d并行\",{\"0\":{\"61\":1}}],[\"32和p32\",{\"1\":{\"47\":1}}],[\"32\",{\"1\":{\"47\":1}}],[\"3\",{\"1\":{\"16\":1,\"55\":1,\"109\":1,\"141\":2,\"167\":1}}],[\"3次更新之后\",{\"1\":{\"11\":1}}],[\"e\",{\"1\":{\"141\":2}}],[\"en​\",{\"1\":{\"140\":3}}],[\"enabling\",{\"1\":{\"82\":1,\"120\":1}}],[\"end\",{\"1\":{\"60\":3}}],[\"empty\",{\"1\":{\"60\":1}}],[\"embeddings\",{\"1\":{\"60\":4}}],[\"embedding\",{\"1\":{\"60\":7}}],[\"efficient\",{\"1\":{\"52\":1,\"61\":1}}],[\"efficiency\",{\"0\":{\"46\":1}}],[\"else\",{\"1\":{\"49\":2,\"60\":1}}],[\"extreme\",{\"1\":{\"82\":1,\"120\":1}}],[\"extended\",{\"0\":{\"24\":1}}],[\"execution\",{\"1\":{\"68\":1,\"69\":2}}],[\"export\",{\"1\":{\"108\":1}}],[\"exp\",{\"1\":{\"16\":2}}],[\"experiences\",{\"1\":{\"15\":1}}],[\"easy\",{\"1\":{\"9\":1}}],[\"wrapper\",{\"1\":{\"159\":1}}],[\"warmup预热不再赘述\",{\"1\":{\"158\":1}}],[\"width\",{\"1\":{\"110\":2}}],[\"with\",{\"0\":{\"96\":1,\"134\":1},\"1\":{\"14\":1,\"16\":1,\"88\":1,\"126\":1}}],[\"wget\",{\"1\":{\"108\":1}}],[\"workerargs\",{\"1\":{\"110\":1}}],[\"workload\",{\"0\":{\"94\":1,\"132\":1},\"1\":{\"94\":1,\"132\":1}}],[\"world\",{\"1\":{\"49\":1}}],[\"w1​\",{\"1\":{\"81\":1,\"119\":1}}],[\"w1s​\",{\"1\":{\"81\":2,\"119\":2}}],[\"w2​\",{\"1\":{\"81\":1,\"119\":1}}],[\"w2​=z2h​b2r​\",{\"1\":{\"81\":1,\"119\":1}}],[\"w2s​\",{\"1\":{\"81\":2,\"119\":2}}],[\"w\",{\"1\":{\"81\":1,\"119\":1}}],[\"weight\",{\"1\":{\"60\":2}}],[\"where\",{\"0\":{\"25\":1}}],[\"while\",{\"1\":{\"14\":1}}],[\"www\",{\"1\":{\"20\":1}}],[\"cfg\",{\"0\":{\"166\":1},\"1\":{\"161\":1,\"166\":2}}],[\"cvpr\",{\"1\":{\"147\":1}}],[\"cvpr2024\",{\"1\":{\"147\":1}}],[\"cin\",{\"1\":{\"139\":1}}],[\"cpp中的源码\",{\"1\":{\"110\":1}}],[\"cpu计算量可能成为瓶颈\",{\"1\":{\"46\":1}}],[\"cpu计算量并不是瓶颈\",{\"1\":{\"46\":1}}],[\"cpu\",{\"1\":{\"45\":2,\"47\":4,\"49\":11,\"50\":4}}],[\"cs149\",{\"0\":{\"103\":1,\"106\":1},\"1\":{\"108\":2},\"2\":{\"104\":1,\"111\":1}}],[\"carlo\",{\"1\":{\"69\":1}}],[\"category\",{\"1\":{\"60\":3}}],[\"chain\",{\"1\":{\"69\":1}}],[\"checkpointing\",{\"0\":{\"41\":1}}],[\"checkpoint\",{\"1\":{\"14\":1}}],[\"clusters\",{\"1\":{\"61\":1}}],[\"clone\",{\"1\":{\"60\":1,\"108\":1}}],[\"classifier\",{\"1\":{\"166\":1}}],[\"class\",{\"1\":{\"60\":2}}],[\"ctx\",{\"1\":{\"60\":2}}],[\"cell\",{\"1\":{\"55\":2}}],[\"c\",{\"1\":{\"31\":1,\"151\":1}}],[\"cout\",{\"1\":{\"139\":1}}],[\"course\",{\"0\":{\"103\":1}}],[\"collectives\",{\"1\":{\"84\":1,\"122\":1}}],[\"columns\",{\"1\":{\"60\":2}}],[\"co\",{\"1\":{\"70\":1}}],[\"copy\",{\"1\":{\"49\":2}}],[\"content\",{\"1\":{\"147\":1}}],[\"contexthttps\",{\"1\":{\"88\":1,\"126\":1}}],[\"conjugate\",{\"1\":{\"80\":1,\"118\":1}}],[\"const\",{\"1\":{\"110\":1}}],[\"constant\",{\"0\":{\"42\":1}}],[\"consumption\",{\"0\":{\"27\":1}}],[\"config=config\",{\"1\":{\"14\":1}}],[\"configproto\",{\"1\":{\"14\":1}}],[\"config\",{\"1\":{\"14\":2}}],[\"computation\",{\"0\":{\"96\":1,\"134\":1}}],[\"compute\",{\"1\":{\"49\":1,\"70\":1}}],[\"communication\",{\"0\":{\"95\":1,\"96\":1,\"133\":1,\"134\":1}}],[\"com\",{\"1\":{\"9\":1,\"15\":1,\"20\":1,\"22\":1,\"59\":1,\"61\":1,\"82\":1,\"108\":2,\"120\":1,\"147\":2,\"159\":1,\"168\":1}}],[\"保留旧有激活值避免大量通讯\",{\"1\":{\"167\":1}}],[\"保证带宽不浪费\",{\"1\":{\"42\":1}}],[\"保证各个计算gpu模型同步\",{\"1\":{\"8\":1}}],[\"保存到检查点以及在完成或发生错误时关闭\",{\"1\":{\"14\":1}}],[\"从纯高斯噪声xt​∼n\",{\"1\":{\"151\":1}}],[\"从下图中我们可以看出\",{\"1\":{\"60\":1}}],[\"从而消除流水线中的等待时间\",{\"1\":{\"156\":1}}],[\"从而使得通信可以被隐藏在后续层的计算过程中\",{\"1\":{\"152\":1}}],[\"从而实现并发计算和通信\",{\"1\":{\"90\":1,\"128\":1}}],[\"从而降低内存占用\",{\"1\":{\"31\":1}}],[\"从而保证通讯无boundary\",{\"1\":{\"6\":1}}],[\"从检查点恢复\",{\"1\":{\"14\":1}}],[\"负责会话初始化\",{\"1\":{\"14\":1}}],[\"04\",{\"1\":{\"108\":1}}],[\"04473https\",{\"1\":{\"61\":1}}],[\"03294\",{\"1\":{\"91\":1,\"129\":1}}],[\"05198\",{\"1\":{\"77\":1,\"115\":1}}],[\"05358\",{\"1\":{\"68\":1}}],[\"05799https\",{\"1\":{\"9\":1}}],[\"08053https\",{\"1\":{\"59\":1}}],[\"06965\",{\"1\":{\"52\":1}}],[\"06840\",{\"1\":{\"44\":1}}],[\"06840https\",{\"1\":{\"44\":1}}],[\"02054\",{\"1\":{\"22\":1}}],[\"01889\",{\"1\":{\"88\":1,\"126\":1}}],[\"01\",{\"1\":{\"14\":1,\"16\":1}}],[\"0\",{\"1\":{\"14\":3,\"49\":1,\"60\":4,\"108\":4,\"151\":1}}],[\"构建模型\",{\"1\":{\"14\":1}}],[\"llama\",{\"1\":{\"159\":1}}],[\"llm\",{\"1\":{\"159\":4,\"167\":1}}],[\"llm模型尺寸较大\",{\"1\":{\"150\":1}}],[\"level\",{\"0\":{\"94\":1,\"132\":1},\"1\":{\"94\":1,\"132\":1}}],[\"learning\",{\"1\":{\"9\":1}}],[\"lm一样解决计算过程中的内存占用问题\",{\"1\":{\"86\":1,\"124\":1}}],[\"lm\",{\"0\":{\"59\":1},\"1\":{\"59\":2,\"61\":2}}],[\"lm提出的新mp方案\",{\"1\":{\"55\":1}}],[\"l\",{\"1\":{\"49\":14,\"153\":1}}],[\"lab\",{\"0\":{\"106\":1},\"1\":{\"147\":1},\"2\":{\"111\":1,\"112\":1}}],[\"large\",{\"1\":{\"61\":1,\"77\":1,\"115\":1}}],[\"language\",{\"1\":{\"59\":1,\"61\":1}}],[\"layernorm的结果y=\",{\"1\":{\"81\":1,\"119\":1}}],[\"layernorm的activation大小计算\",{\"1\":{\"79\":1,\"117\":1}}],[\"layer\",{\"1\":{\"55\":1}}],[\"layers\",{\"1\":{\"49\":12}}],[\"latest\",{\"1\":{\"20\":1}}],[\"lr=0\",{\"1\":{\"16\":1}}],[\"li\",{\"1\":{\"147\":1}}],[\"linux\",{\"1\":{\"108\":3}}],[\"linear\",{\"1\":{\"16\":1}}],[\"list\",{\"1\":{\"14\":1}}],[\"log10\",{\"1\":{\"141\":2}}],[\"log10​\",{\"1\":{\"140\":1}}],[\"log\",{\"1\":{\"140\":1}}],[\"logs\",{\"1\":{\"14\":1}}],[\"loadbalanced\",{\"0\":{\"96\":1,\"134\":1}}],[\"long\",{\"1\":{\"82\":1,\"85\":1,\"120\":1,\"123\":1,\"139\":2}}],[\"location\",{\"1\":{\"70\":1}}],[\"local\",{\"1\":{\"14\":1}}],[\"loss\",{\"1\":{\"14\":2,\"49\":1}}],[\"oom\",{\"1\":{\"162\":1}}],[\"overlap\",{\"0\":{\"96\":1,\"134\":1}}],[\"overhead\",{\"0\":{\"95\":1,\"133\":1}}],[\"overview\",{\"0\":{\"30\":1,\"31\":1,\"32\":1,\"160\":1}}],[\"of\",{\"1\":{\"52\":1,\"82\":1,\"120\":1,\"162\":1}}],[\"offload在cpu上直接更新fp32参数和剩余的优化器状态\",{\"1\":{\"49\":1}}],[\"offload在小\",{\"1\":{\"46\":1}}],[\"offload可以立即将这些梯度逐个或切分传输到\",{\"1\":{\"49\":1}}],[\"offload将数据进行分区\",{\"1\":{\"49\":1}}],[\"offload将深度学习训练建模为数据流图\",{\"1\":{\"47\":1}}],[\"offload\",{\"0\":{\"44\":1,\"47\":1,\"48\":1},\"1\":{\"44\":1,\"45\":1,\"46\":1,\"50\":3}}],[\"owns\",{\"1\":{\"49\":1}}],[\"owner\",{\"1\":{\"49\":6}}],[\"outputcontext=softmax\",{\"1\":{\"84\":1,\"122\":1}}],[\"output\",{\"1\":{\"60\":5,\"110\":2}}],[\"out\",{\"1\":{\"16\":2,\"162\":1}}],[\"one\",{\"1\":{\"60\":1}}],[\"on\",{\"1\":{\"15\":1,\"49\":4,\"61\":1}}],[\"op并行\",{\"1\":{\"69\":1}}],[\"operator\",{\"1\":{\"69\":4,\"70\":6,\"71\":1,\"72\":3}}],[\"openaccess\",{\"1\":{\"147\":1}}],[\"open\",{\"1\":{\"20\":1}}],[\"op\",{\"1\":{\"14\":2}}],[\"optimal\",{\"0\":{\"47\":1}}],[\"optimizations\",{\"1\":{\"22\":1,\"82\":1,\"120\":1}}],[\"optimizer\",{\"0\":{\"26\":1},\"1\":{\"16\":1,\"69\":1}}],[\"optim\",{\"1\":{\"16\":3,\"49\":2}}],[\"options\",{\"1\":{\"14\":1}}],[\"opt\",{\"1\":{\"14\":4,\"16\":2}}],[\"org\",{\"1\":{\"9\":1,\"15\":1,\"20\":1,\"22\":1,\"44\":2,\"52\":1,\"59\":1,\"61\":1,\"68\":1,\"77\":1,\"82\":1,\"85\":1,\"88\":1,\"91\":1,\"115\":1,\"120\":1,\"123\":1,\"126\":1,\"129\":1,\"155\":1,\"159\":1,\"161\":1}}],[\"=tsbh​\",{\"1\":{\"81\":1,\"119\":1}}],[\"=gˉ​\",{\"1\":{\"81\":1,\"119\":1}}],[\"=gelu\",{\"1\":{\"81\":1,\"119\":1}}],[\"=z1h​b1r​\",{\"1\":{\"81\":1,\"119\":1}}],[\"=zb\",{\"1\":{\"81\":1,\"119\":1}}],[\"=dropout\",{\"1\":{\"81\":1,\"119\":1}}],[\"=softmax\",{\"1\":{\"79\":1,\"117\":1}}],[\"=\",{\"1\":{\"14\":7,\"16\":6,\"49\":7,\"60\":10,\"81\":2,\"110\":3,\"119\":2,\"139\":5,\"141\":3}}],[\"固定\",{\"1\":{\"14\":1}}],[\"初始化权重\",{\"1\":{\"60\":1}}],[\"初始化每个进程的层\",{\"1\":{\"49\":1}}],[\"初始化\",{\"1\":{\"14\":1}}],[\"i++\",{\"1\":{\"139\":1}}],[\"imbalance\",{\"0\":{\"94\":1,\"132\":1}}],[\"import\",{\"1\":{\"14\":2,\"16\":4}}],[\"idx\",{\"1\":{\"60\":3}}],[\"id\",{\"1\":{\"60\":1}}],[\"if\",{\"1\":{\"49\":4,\"60\":2,\"110\":1,\"141\":1}}],[\"i\",{\"1\":{\"49\":18,\"139\":3,\"151\":1,\"153\":4}}],[\"issues\",{\"1\":{\"168\":1}}],[\"ispc\",{\"1\":{\"108\":5}}],[\"is\",{\"1\":{\"49\":4}}],[\"io\",{\"1\":{\"20\":1}}],[\"inference\",{\"1\":{\"147\":1}}],[\"infinite\",{\"1\":{\"88\":1,\"126\":1}}],[\"include\",{\"1\":{\"141\":2}}],[\"index\",{\"1\":{\"60\":8}}],[\"int\",{\"1\":{\"110\":6,\"139\":2,\"141\":5}}],[\"into\",{\"0\":{\"36\":1,\"40\":1,\"56\":1}}],[\"introduction\",{\"0\":{\"24\":1}}],[\"insights\",{\"0\":{\"30\":1,\"31\":1,\"32\":1}}],[\"input共享\",{\"1\":{\"79\":1,\"117\":1}}],[\"input\",{\"1\":{\"60\":12}}],[\"inp\",{\"1\":{\"16\":2}}],[\"initialize\",{\"1\":{\"16\":1,\"49\":2}}],[\"init\",{\"1\":{\"14\":1,\"16\":1,\"60\":3}}],[\"in\",{\"1\":{\"9\":1,\"49\":7,\"77\":1,\"115\":1}}],[\"404\",{\"1\":{\"171\":1}}],[\"4个维度描述搜索空间\",{\"1\":{\"69\":1}}],[\"4个cell\",{\"1\":{\"55\":1}}],[\"4\",{\"1\":{\"13\":1,\"16\":1,\"55\":1,\"109\":1}}],[\"操作符的共置\",{\"1\":{\"70\":1}}],[\"操作视为对输入进行索引操作\",{\"1\":{\"60\":1}}],[\"操作\",{\"1\":{\"13\":1,\"50\":1,\"60\":3}}],[\"添加hook\",{\"1\":{\"14\":1}}],[\"添加\",{\"1\":{\"13\":1,\"14\":1}}],[\"添加了对单机多卡的支持\",{\"1\":{\"13\":1}}],[\"并依次送入卷积运算符\",{\"1\":{\"168\":1}}],[\"并不依赖于其他区块的最新激活信息\",{\"1\":{\"152\":1}}],[\"并使用张量并行进行推理的大通讯量的不可行性\",{\"1\":{\"151\":1}}],[\"并使用模型并行操作来优化\",{\"1\":{\"60\":1}}],[\"并利用成熟的指数积分器来减少采样步骤\",{\"1\":{\"150\":1}}],[\"并在不同设备上进行序列并行\",{\"1\":{\"168\":1}}],[\"并在潜在空间中学习扩散模型\",{\"1\":{\"150\":1}}],[\"并在后续针对数据结果决定是否要进行优化\",{\"1\":{\"110\":1}}],[\"并且梦到了一个帅气的男孩纸\",{\"1\":{\"138\":1}}],[\"并且每个\",{\"1\":{\"50\":1}}],[\"并通过key\",{\"1\":{\"90\":1,\"128\":1}}],[\"并随机替换被选择的\",{\"1\":{\"72\":1}}],[\"并沿着其列\",{\"1\":{\"60\":1}}],[\"并将每个区块分配给独立的gpu进行处理\",{\"1\":{\"154\":1}}],[\"并将独立且并行地处理一个单独的补丁\",{\"1\":{\"153\":1}}],[\"并将更新后的\",{\"1\":{\"49\":1}}],[\"并将其分为两部分\",{\"1\":{\"24\":1}}],[\"并减少\",{\"1\":{\"45\":1}}],[\"并优化了性能\",{\"1\":{\"13\":1}}],[\"并行计算\",{\"2\":{\"105\":1,\"112\":1}}],[\"并行\",{\"1\":{\"84\":1,\"122\":1}}],[\"并行部分有mlp的linear部分\",{\"1\":{\"80\":1,\"118\":1}}],[\"并行度为t\",{\"1\":{\"80\":1,\"118\":1}}],[\"并行运算\",{\"2\":{\"74\":1,\"102\":1,\"136\":1}}],[\"并行运算感兴趣\",{\"1\":{\"0\":1}}],[\"并行使用的gpu的数量\",{\"1\":{\"64\":1}}],[\"并行中使用的参数如何调整\",{\"1\":{\"6\":1}}],[\"库实现了\",{\"1\":{\"13\":1}}],[\"n=\",{\"1\":{\"140\":2}}],[\"n=ptd\",{\"1\":{\"66\":1}}],[\"n阶乘的计算量十分大\",{\"1\":{\"140\":1}}],[\"n阶乘的位数\",{\"1\":{\"138\":1}}],[\"n2\",{\"1\":{\"139\":1}}],[\"n\",{\"1\":{\"66\":1,\"70\":1,\"84\":1,\"122\":1,\"138\":4,\"139\":3,\"140\":4,\"141\":5,\"153\":2,\"158\":1,\"168\":1}}],[\"naive\",{\"0\":{\"55\":1}}],[\"near\",{\"1\":{\"88\":1,\"126\":1}}],[\"neural\",{\"1\":{\"52\":1,\"68\":1}}],[\"net模型\",{\"1\":{\"157\":1}}],[\"networks\",{\"1\":{\"52\":1,\"68\":1}}],[\"net\",{\"1\":{\"16\":5,\"151\":1}}],[\"numthreads\",{\"1\":{\"110\":2}}],[\"numrows\",{\"1\":{\"110\":2}}],[\"num\",{\"1\":{\"49\":4,\"60\":4,\"141\":3}}],[\"novel\",{\"1\":{\"68\":1}}],[\"norm\",{\"1\":{\"60\":2}}],[\"normal\",{\"1\":{\"60\":1}}],[\"node\",{\"1\":{\"47\":1}}],[\"not\",{\"1\":{\"14\":1,\"171\":1}}],[\"n参数\",{\"1\":{\"41\":1}}],[\"n份数据\",{\"1\":{\"37\":1,\"38\":1,\"39\":1}}],[\"nvidia本身支持这种优化并不会有通讯增加\",{\"1\":{\"39\":1}}],[\"nvidia\",{\"1\":{\"20\":1,\"59\":1,\"61\":1}}],[\"nn\",{\"1\":{\"16\":5}}],[\"nccl\",{\"1\":{\"13\":1,\"20\":3}}],[\"nlp等ai方向\",{\"1\":{\"0\":1}}],[\"命名为\",{\"1\":{\"13\":1}}],[\"包括流水线并行和张量并行\",{\"1\":{\"66\":1}}],[\"包括优化器状态\",{\"1\":{\"24\":1}}],[\"包\",{\"1\":{\"13\":1}}],[\"进行空间分解\",{\"1\":{\"109\":1}}],[\"进行全局的\",{\"1\":{\"84\":1,\"122\":1}}],[\"进行all\",{\"1\":{\"81\":1,\"119\":1}}],[\"进行数据计算\",{\"1\":{\"6\":1}}],[\"进而生成\",{\"1\":{\"72\":1}}],[\"进程直接在\",{\"1\":{\"50\":1}}],[\"进程\",{\"1\":{\"19\":1}}],[\"进入all\",{\"1\":{\"11\":1}}],[\"使每个gpu仅留1\",{\"1\":{\"37\":1,\"38\":1,\"39\":1}}],[\"使我们能够根据设备数量按比例扩展模型大小\",{\"1\":{\"23\":1}}],[\"使得每一个chunk都尽可能大\",{\"1\":{\"87\":1,\"125\":1}}],[\"使得每个gpu只和其相邻的两块gpu通讯\",{\"1\":{\"11\":1}}],[\"使得参数额外开销尽可能减少\",{\"1\":{\"39\":1}}],[\"使得某些参数的梯度不需要用到\",{\"1\":{\"19\":1}}],[\"使用了两种关键策略\",{\"1\":{\"168\":1}}],[\"使用一些通用优化手段\",{\"1\":{\"146\":1}}],[\"使用diffusion\",{\"1\":{\"146\":1}}],[\"使用更好的solver\",{\"1\":{\"146\":1}}],[\"使用16个线程运行改进后代码\",{\"1\":{\"109\":1}}],[\"使用固定大小buffer指定数据的单批发送量\",{\"1\":{\"42\":1}}],[\"使用reduce\",{\"1\":{\"41\":1}}],[\"使用时通讯获取完整参数\",{\"1\":{\"39\":1}}],[\"使用的参数可能不相同\",{\"1\":{\"19\":1}}],[\"使用参数的反序作为梯度放入bucket的顺序\",{\"1\":{\"19\":1}}],[\"使用方法\",{\"0\":{\"14\":1}}],[\"使用\",{\"1\":{\"13\":1,\"166\":1}}],[\"使用尽可能多的参数\",{\"1\":{\"4\":1}}],[\"定义网络拓扑关系\",{\"1\":{\"11\":1}}],[\"rsa感觉就是将输入序列进行切分\",{\"1\":{\"87\":1,\"125\":1}}],[\"rows++\",{\"1\":{\"110\":1}}],[\"rows\",{\"1\":{\"110\":6}}],[\"row\",{\"1\":{\"60\":1}}],[\"r\",{\"0\":{\"32\":1,\"40\":1}}],[\"resolution\",{\"1\":{\"147\":1}}],[\"residual\",{\"0\":{\"27\":1},\"1\":{\"24\":1,\"25\":1}}],[\"repo到本地来开始lab了\",{\"1\":{\"108\":1}}],[\"releases\",{\"1\":{\"108\":1}}],[\"recomputation两种方法\",{\"1\":{\"78\":1,\"116\":1}}],[\"recomputation\",{\"1\":{\"77\":1,\"115\":1}}],[\"recompute也是一种可以考虑的方法\",{\"1\":{\"66\":1}}],[\"region\",{\"1\":{\"60\":1}}],[\"re\",{\"0\":{\"57\":1}}],[\"return\",{\"1\":{\"49\":2,\"60\":3,\"141\":1}}],[\"reducing\",{\"0\":{\"33\":1},\"1\":{\"77\":1,\"115\":1}}],[\"reduce操作\",{\"1\":{\"81\":1,\"119\":1}}],[\"reduce\",{\"0\":{\"11\":1},\"1\":{\"11\":1,\"49\":1,\"50\":1,\"60\":3,\"80\":2,\"81\":1,\"118\":2,\"119\":1}}],[\"range\",{\"1\":{\"49\":5,\"60\":1}}],[\"randn\",{\"1\":{\"16\":2}}],[\"rank=1拥有的fresh\",{\"1\":{\"167\":1}}],[\"rank=0拥有的fresh\",{\"1\":{\"167\":1}}],[\"rank\",{\"1\":{\"14\":3,\"49\":5,\"60\":2}}],[\"run\",{\"1\":{\"14\":1,\"16\":2}}],[\"ring\",{\"0\":{\"10\":1,\"88\":1,\"90\":1,\"126\":1,\"128\":1},\"1\":{\"10\":1,\"13\":2,\"81\":1,\"88\":1,\"90\":1,\"119\":1,\"126\":1,\"128\":1}}],[\"guidance\",{\"1\":{\"166\":1}}],[\"guide\",{\"1\":{\"20\":1,\"166\":2}}],[\"gmacs\",{\"1\":{\"151\":1}}],[\"gz\",{\"1\":{\"108\":2}}],[\"get\",{\"1\":{\"60\":1}}],[\"gelu的反向也需要对输入进行缓存\",{\"1\":{\"79\":1,\"117\":1}}],[\"gelu\",{\"1\":{\"60\":1,\"81\":2,\"119\":2}}],[\"global\",{\"1\":{\"60\":1,\"66\":1}}],[\"gloo\",{\"1\":{\"20\":1}}],[\"gloo和mpi\",{\"1\":{\"20\":1}}],[\"git\",{\"1\":{\"108\":2}}],[\"github\",{\"1\":{\"9\":1,\"15\":1,\"22\":1,\"59\":1,\"61\":1,\"108\":2,\"147\":1,\"159\":1,\"168\":1}}],[\"giant\",{\"1\":{\"52\":1}}],[\"gpipe使用模型并行\",{\"1\":{\"53\":1}}],[\"gpipe\",{\"0\":{\"52\":1,\"53\":1},\"1\":{\"52\":1}}],[\"gpu总数\",{\"1\":{\"66\":1}}],[\"gpu服务器内部\",{\"1\":{\"62\":1}}],[\"gpu内存占用是一件非常昂贵的事情\",{\"1\":{\"45\":1}}],[\"gpu\",{\"1\":{\"14\":3,\"45\":2,\"47\":3,\"49\":5,\"50\":6,\"53\":1,\"55\":4,\"57\":1,\"60\":1,\"61\":1}}],[\"graph\",{\"1\":{\"70\":1,\"71\":1,\"72\":1}}],[\"gradient\",{\"1\":{\"60\":3}}],[\"gradients\",{\"0\":{\"26\":1}}],[\"grad\",{\"1\":{\"49\":7,\"60\":1}}],[\"group所有设备的kv\",{\"1\":{\"167\":1}}],[\"group内不同rank设备的fresh\",{\"1\":{\"167\":1}}],[\"group\",{\"1\":{\"16\":1}}],[\"go\",{\"0\":{\"25\":1}}],[\"gateway\",{\"1\":{\"20\":1}}],[\"gather和4次reduce\",{\"1\":{\"81\":1,\"119\":1}}],[\"gather通信\",{\"1\":{\"81\":1,\"119\":1}}],[\"gather阶段\",{\"1\":{\"11\":1}}],[\"gather\",{\"0\":{\"12\":1},\"1\":{\"10\":1,\"50\":1,\"81\":2,\"119\":2}}],[\"s\",{\"1\":{\"141\":3}}],[\"system\",{\"1\":{\"82\":1,\"85\":1,\"120\":1,\"123\":1}}],[\"sample并行\",{\"1\":{\"69\":1}}],[\"sample\",{\"1\":{\"69\":5,\"70\":1}}],[\"sa两个环节做了如下优化\",{\"1\":{\"60\":1}}],[\"simulator\",{\"1\":{\"68\":1,\"69\":1}}],[\"size\",{\"0\":{\"42\":1},\"1\":{\"46\":4,\"47\":1,\"49\":1,\"53\":1,\"60\":4,\"63\":1,\"66\":2,\"110\":2}}],[\"softmax的输出也会用于反向的计算\",{\"1\":{\"79\":1,\"117\":1}}],[\"sosd\",{\"2\":{\"73\":1,\"101\":1,\"135\":1,\"169\":1}}],[\"soap\",{\"1\":{\"68\":1,\"69\":2,\"70\":1}}],[\"solo\",{\"1\":{\"20\":1}}],[\"sp执行完毕后\",{\"1\":{\"167\":1}}],[\"sp只拥有1\",{\"1\":{\"167\":1}}],[\"sp的kv\",{\"1\":{\"167\":1}}],[\"sp\",{\"1\":{\"167\":5,\"168\":1}}],[\"spatial\",{\"1\":{\"109\":1}}],[\"sparse\",{\"1\":{\"60\":1}}],[\"sp​\",{\"1\":{\"95\":1,\"133\":1}}],[\"sp没有引入更多的通信代价\",{\"1\":{\"81\":1,\"119\":1}}],[\"split\",{\"0\":{\"56\":1},\"1\":{\"166\":1}}],[\"src\",{\"1\":{\"49\":1}}],[\"scheduling\",{\"0\":{\"96\":1,\"134\":1}}],[\"schedule\",{\"0\":{\"48\":1}}],[\"scores问题\",{\"1\":{\"87\":1,\"125\":1}}],[\"scanf\",{\"1\":{\"141\":1}}],[\"scale\",{\"1\":{\"44\":1,\"60\":1,\"61\":1}}],[\"scatter然后一个all\",{\"1\":{\"81\":1,\"119\":1}}],[\"scatter操作\",{\"1\":{\"81\":1,\"119\":1}}],[\"scatter更新各个参数状态\",{\"1\":{\"41\":1}}],[\"scatter阶段结束\",{\"1\":{\"11\":1}}],[\"scatter\",{\"0\":{\"11\":1},\"1\":{\"50\":1,\"81\":2,\"119\":2}}],[\"scatter和all\",{\"1\":{\"10\":1}}],[\"super\",{\"1\":{\"47\":1,\"60\":1}}],[\"sgd\",{\"1\":{\"16\":1}}],[\"sequences的attention\",{\"1\":{\"87\":1,\"125\":1}}],[\"sequence\",{\"0\":{\"81\":1,\"85\":1,\"119\":1,\"123\":1},\"1\":{\"82\":1,\"85\":2,\"120\":1,\"123\":2,\"161\":1,\"167\":1}}],[\"semantics\",{\"1\":{\"64\":1}}],[\"self\",{\"1\":{\"60\":24,\"87\":1,\"125\":1}}],[\"select\",{\"1\":{\"60\":1}}],[\"setup\",{\"1\":{\"16\":1}}],[\"sess\",{\"1\":{\"14\":3}}],[\"std\",{\"1\":{\"139\":2}}],[\"standard\",{\"1\":{\"167\":2}}],[\"stanford\",{\"1\":{\"108\":1}}],[\"state需要的显存大小\",{\"1\":{\"78\":1,\"116\":1}}],[\"states\",{\"0\":{\"26\":2},\"1\":{\"24\":1,\"25\":2,\"49\":2}}],[\"startrow\",{\"1\":{\"110\":5}}],[\"start\",{\"1\":{\"60\":4}}],[\"step使用\",{\"1\":{\"167\":1}}],[\"step后\",{\"1\":{\"167\":1}}],[\"step全部的kv\",{\"1\":{\"167\":1}}],[\"step中\",{\"1\":{\"167\":1}}],[\"step内\",{\"1\":{\"167\":1}}],[\"step计算出fresh\",{\"1\":{\"167\":1}}],[\"step\",{\"1\":{\"16\":1,\"49\":2,\"167\":1}}],[\"stop\",{\"1\":{\"14\":1}}],[\"strategy\",{\"0\":{\"47\":1},\"1\":{\"71\":1,\"72\":4}}],[\"str\",{\"1\":{\"14\":1}}],[\"shb\",{\"1\":{\"79\":1,\"117\":1}}],[\"should\",{\"1\":{\"14\":1}}],[\"sharded\",{\"1\":{\"6\":1}}],[\"可拆分的维度并不一样\",{\"1\":{\"70\":1}}],[\"可以提供更广泛的条件控制\",{\"1\":{\"166\":1}}],[\"可以达到6倍以上加速\",{\"1\":{\"148\":1}}],[\"可以通过以下方程推导出xt−1​\",{\"1\":{\"151\":1}}],[\"可以通过计算对比来估计一下斯特林公式算出结果\",{\"1\":{\"140\":1}}],[\"可以通过fa的输出结果完成更新\",{\"1\":{\"96\":1,\"134\":1}}],[\"可以简单分析一下mandelbrotserial\",{\"1\":{\"110\":1}}],[\"可以考虑采用4d并行\",{\"1\":{\"90\":1,\"128\":1}}],[\"可以结合tp有效减少不必要的计算量\",{\"1\":{\"78\":1,\"116\":1}}],[\"可以准确预测并行策略的性能\",{\"1\":{\"68\":1}}],[\"可以得到以下结论\",{\"1\":{\"66\":1}}],[\"可以将\",{\"1\":{\"60\":1}}],[\"可以在transformer中实现简单并发\",{\"1\":{\"60\":1}}],[\"可以切为\",{\"1\":{\"55\":1}}],[\"可以转移到cpu上\",{\"1\":{\"47\":1}}],[\"可以明显看到gpu数量增多时\",{\"1\":{\"9\":1}}],[\"可证明地最大化节约gpu内存\",{\"1\":{\"46\":1}}],[\"可能会导致梯度更新不稳定\",{\"1\":{\"56\":1}}],[\"可能内存池的维护并不能做到完全利用\",{\"1\":{\"29\":1}}],[\"可能并不会产生权重累计\",{\"1\":{\"26\":1}}],[\"可能在模型训练过程中梯度不大等原因\",{\"1\":{\"26\":1}}],[\"可能大幅度提升计算效率\",{\"1\":{\"3\":1}}],[\"又同时增加了额外的通讯开销\",{\"1\":{\"9\":1}}],[\"又切分模型的参数\",{\"1\":{\"6\":1}}],[\"1<=n<=25000\",{\"1\":{\"138\":1}}],[\"1位符号位\",{\"1\":{\"26\":3}}],[\"1909\",{\"1\":{\"59\":1}}],[\"1910\",{\"1\":{\"22\":1}}],[\"19\",{\"1\":{\"16\":1}}],[\"18sbh\",{\"1\":{\"80\":1,\"118\":1}}],[\"1807\",{\"1\":{\"68\":1}}],[\"1802\",{\"1\":{\"9\":1}}],[\"1811\",{\"1\":{\"52\":1}}],[\"18\",{\"1\":{\"16\":1}}],[\"17\",{\"1\":{\"16\":1}}],[\"16\",{\"1\":{\"16\":1}}],[\"15\",{\"1\":{\"16\":1}}],[\"15704https\",{\"1\":{\"15\":1}}],[\"14430\",{\"1\":{\"159\":1}}],[\"14430v2\",{\"1\":{\"155\":1}}],[\"141592654\",{\"1\":{\"141\":1}}],[\"14509\",{\"1\":{\"82\":1,\"120\":1}}],[\"14\",{\"1\":{\"16\":1}}],[\"13120\",{\"1\":{\"85\":1,\"123\":1}}],[\"13\",{\"1\":{\"16\":1}}],[\"12598\",{\"1\":{\"161\":1}}],[\"12\",{\"1\":{\"16\":1,\"55\":1}}],[\"11sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"11\",{\"1\":{\"16\":1}}],[\"10+t24​+5htas​\",{\"1\":{\"80\":1,\"118\":1}}],[\"10位尾数位\",{\"1\":{\"26\":1}}],[\"10\",{\"1\":{\"16\":5,\"138\":1}}],[\"1\",{\"0\":{\"56\":1},\"1\":{\"16\":1,\"19\":1,\"24\":1,\"49\":1,\"60\":2,\"138\":1,\"139\":2,\"140\":2,\"141\":2,\"153\":3,\"168\":1}}],[\"1块梯度收集gpu\",{\"1\":{\"8\":1}}],[\"atl\",{\"1\":{\"153\":1}}],[\"attention实现\",{\"1\":{\"167\":1}}],[\"attention缺少高效的attention实现\",{\"1\":{\"92\":1,\"130\":1}}],[\"attention方式可以突破序列长度限制\",{\"1\":{\"90\":1,\"128\":1}}],[\"attention和fwd\",{\"1\":{\"90\":1,\"128\":1}}],[\"attention和fwd计算\",{\"1\":{\"90\":1,\"128\":1}}],[\"attention时\",{\"1\":{\"90\":1,\"128\":1}}],[\"attention的外循环\",{\"1\":{\"90\":1,\"128\":1}}],[\"attention来解决此问题\",{\"1\":{\"87\":1,\"125\":1}}],[\"attention例子\",{\"1\":{\"79\":1,\"117\":1}}],[\"attention块的activation\",{\"1\":{\"79\":1,\"117\":1}}],[\"attention块中包含一个self\",{\"1\":{\"79\":1,\"117\":1}}],[\"attention\",{\"0\":{\"88\":1,\"90\":1,\"126\":1,\"128\":1},\"1\":{\"79\":3,\"88\":1,\"90\":1,\"117\":3,\"126\":1,\"128\":1}}],[\"attribute\",{\"1\":{\"69\":2}}],[\"ans\",{\"1\":{\"139\":3,\"141\":3}}],[\"and\",{\"0\":{\"26\":1,\"30\":1,\"31\":1,\"32\":1,\"96\":1,\"134\":1},\"1\":{\"9\":1,\"16\":1,\"68\":1,\"81\":1,\"119\":1}}],[\"args\",{\"1\":{\"110\":16}}],[\"arxiv\",{\"1\":{\"9\":1,\"15\":1,\"22\":1,\"44\":2,\"52\":1,\"59\":1,\"61\":1,\"68\":1,\"77\":1,\"82\":1,\"85\":1,\"88\":1,\"91\":1,\"115\":1,\"120\":1,\"123\":1,\"126\":1,\"129\":1,\"155\":1,\"159\":1,\"161\":1}}],[\"a和b是linear的权重weight矩阵\",{\"1\":{\"81\":1,\"119\":1}}],[\"a100的显存\",{\"1\":{\"78\":1,\"116\":1}}],[\"aisys\",{\"0\":{\"75\":1},\"2\":{\"102\":1}}],[\"autograd\",{\"1\":{\"60\":1}}],[\"activationmemoryperlayer​=sbh\",{\"1\":{\"81\":1,\"119\":1}}],[\"activationmemoryperlayer的值降为\",{\"1\":{\"80\":1,\"118\":1}}],[\"activationmemoryperlayer=sbh\",{\"1\":{\"79\":1,\"80\":1,\"117\":1,\"118\":1}}],[\"activation指fwd和bwd梯度计算中创建的所有tensor\",{\"1\":{\"79\":1,\"117\":1}}],[\"activation\",{\"0\":{\"33\":1,\"41\":1,\"79\":1,\"117\":1},\"1\":{\"66\":1,\"77\":1,\"78\":1,\"115\":1,\"116\":1}}],[\"accelerating\",{\"1\":{\"15\":1}}],[\"a\",{\"1\":{\"31\":1,\"33\":1}}],[\"adam\",{\"1\":{\"26\":1}}],[\"adam本身对每一个参数都需要保留momentum和variance两个参数进行数据更新\",{\"1\":{\"26\":1}}],[\"adagradoptimizer\",{\"1\":{\"14\":1}}],[\"asst1\",{\"1\":{\"108\":1}}],[\"asst1并不需要额外配置运行环境\",{\"1\":{\"108\":1}}],[\"assignment1\",{\"0\":{\"106\":1}}],[\"as2b\",{\"1\":{\"79\":2,\"117\":2}}],[\"as\",{\"1\":{\"14\":3,\"16\":3}}],[\"api\",{\"1\":{\"13\":1,\"20\":1}}],[\"all来在注意力头上收集结果\",{\"1\":{\"83\":1,\"121\":1}}],[\"all通信操作\",{\"1\":{\"83\":1,\"121\":1}}],[\"allocate\",{\"1\":{\"49\":4}}],[\"all\",{\"0\":{\"12\":1,\"25\":1},\"1\":{\"50\":1,\"60\":1,\"81\":1,\"84\":2,\"119\":1,\"122\":2}}],[\"allreduce将这个过程分为reduce\",{\"1\":{\"10\":1}}],[\"allreduce就是要让每块gpu上的数据都变成箭头右边汇总的样子\",{\"1\":{\"10\":1}}],[\"allreduce\",{\"0\":{\"10\":1},\"1\":{\"13\":2,\"87\":1,\"125\":1}}],[\"allreduce的方式\",{\"1\":{\"9\":1}}],[\"abstract\",{\"0\":{\"23\":1,\"53\":1,\"78\":1,\"116\":1}}],[\"abs\",{\"1\":{\"9\":1,\"15\":1,\"22\":1,\"52\":1,\"59\":1,\"159\":1,\"161\":1}}],[\"huggingface\",{\"1\":{\"168\":1}}],[\"huggingface中采用的recomputation的检查点放置不合理\",{\"1\":{\"96\":1,\"134\":1}}],[\"hybrid\",{\"0\":{\"167\":1}}],[\"han\",{\"1\":{\"147\":1}}],[\"high\",{\"1\":{\"147\":1}}],[\"h>\",{\"1\":{\"141\":2}}],[\"height\",{\"1\":{\"110\":6}}],[\"html\",{\"1\":{\"20\":1,\"155\":1}}],[\"https\",{\"1\":{\"9\":1,\"15\":1,\"20\":3,\"22\":1,\"44\":1,\"52\":1,\"59\":1,\"61\":1,\"68\":1,\"77\":1,\"82\":1,\"91\":1,\"108\":2,\"115\":1,\"120\":1,\"129\":1,\"147\":2,\"155\":1,\"159\":2,\"161\":1}}],[\"home\",{\"1\":{\"108\":1}}],[\"hot\",{\"1\":{\"60\":1}}],[\"hooks=hooks\",{\"1\":{\"14\":1}}],[\"hooks\",{\"1\":{\"14\":1}}],[\"horovod工作\",{\"0\":{\"13\":1}}],[\"horovod\",{\"0\":{\"9\":1},\"1\":{\"9\":2,\"13\":2,\"14\":3}}],[\"hvd\",{\"1\":{\"14\":5}}],[\"found\",{\"1\":{\"171\":1}}],[\"for\",{\"1\":{\"49\":6,\"68\":1,\"82\":1,\"88\":1,\"120\":1,\"126\":1,\"139\":1,\"147\":1}}],[\"forward\",{\"1\":{\"16\":1,\"49\":3,\"55\":1,\"60\":2}}],[\"f和f​替换为g和g​​在前向是reduce\",{\"1\":{\"81\":1,\"119\":1}}],[\"f​在前向时执行all\",{\"1\":{\"80\":1,\"118\":1}}],[\"f在前向时不做操作\",{\"1\":{\"80\":1,\"118\":1}}],[\"float\",{\"1\":{\"110\":4}}],[\"float2half\",{\"1\":{\"47\":1}}],[\"flashattention部分的梯度更新为kqv三个矩阵\",{\"1\":{\"96\":1,\"134\":1}}],[\"flexflow负责将\",{\"1\":{\"70\":1}}],[\"flexflow\",{\"0\":{\"68\":1},\"1\":{\"68\":2}}],[\"flush消耗的时间则越小\",{\"1\":{\"64\":1}}],[\"flush操作\",{\"1\":{\"64\":1}}],[\"free\",{\"1\":{\"166\":1}}],[\"freq\",{\"1\":{\"60\":1}}],[\"from\",{\"1\":{\"60\":2,\"85\":1,\"123\":1}}],[\"fragmented\",{\"0\":{\"35\":1}}],[\"fragmentation\",{\"0\":{\"29\":1}}],[\"function\",{\"1\":{\"60\":1}}],[\"fully\",{\"1\":{\"6\":1}}],[\"f\",{\"1\":{\"60\":2}}],[\"fk和\",{\"1\":{\"55\":2}}],[\"false\",{\"1\":{\"49\":1}}],[\"fast\",{\"1\":{\"9\":1}}],[\"fwd\",{\"1\":{\"47\":1,\"49\":1}}],[\"fp16梯度和所有优化器状态存储在cpu上\",{\"1\":{\"49\":1}}],[\"fp16\",{\"1\":{\"26\":2,\"49\":3}}],[\"fp32\",{\"1\":{\"26\":2,\"47\":1,\"49\":6}}],[\"fsdp并行\",{\"0\":{\"21\":1}}],[\"聚合再下发梯度操作被称为allreduce\",{\"1\":{\"8\":1}}],[\"把\",{\"1\":{\"60\":1}}],[\"把当前卡上不要的\",{\"1\":{\"60\":1}}],[\"把本地梯度push到梯度收集gpu\",{\"1\":{\"8\":1}}],[\"把一份data\",{\"1\":{\"8\":1}}],[\"每步需要6\",{\"1\":{\"151\":1}}],[\"每pipeline中的microbatch的数量\",{\"1\":{\"66\":1}}],[\"每块\",{\"1\":{\"55\":1}}],[\"每块gpu上都有一块数据拥有了对应位置完整的聚合\",{\"1\":{\"11\":1}}],[\"每块gpu上的数据也对应被切成4份\",{\"1\":{\"10\":1}}],[\"每块gpu均拷贝一份完整的模型\",{\"1\":{\"8\":1}}],[\"每份\",{\"1\":{\"55\":1}}],[\"每个micro\",{\"1\":{\"167\":2}}],[\"每个内存存有限位数据\",{\"1\":{\"139\":1}}],[\"每个点的迭代次数\",{\"1\":{\"110\":1}}],[\"每个设备都拥有模型ϵθ​的一个副本\",{\"1\":{\"153\":1}}],[\"每个设备将用于compute的key\",{\"1\":{\"90\":1,\"128\":1}}],[\"每个设备计算其各自的self\",{\"1\":{\"90\":1,\"128\":1}}],[\"每个主机将key\",{\"1\":{\"90\":1,\"128\":1}}],[\"每个主机设备具有一个query\",{\"1\":{\"90\":1,\"128\":1}}],[\"每个头的注意力计算形式为\",{\"1\":{\"84\":1,\"122\":1}}],[\"每个本地n\",{\"1\":{\"84\":1,\"122\":1}}],[\"每个layernorm层的输入需要2sbh大小\",{\"1\":{\"79\":1,\"117\":1}}],[\"每个矩阵元素总大小为2sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"每个元素以半精度\",{\"1\":{\"79\":1,\"117\":1}}],[\"每个stage放在独立的设备\",{\"1\":{\"53\":1}}],[\"每个\",{\"1\":{\"50\":1}}],[\"每个梯度传输可以与反向计算重叠\",{\"1\":{\"49\":1}}],[\"每个gpu都没有保存或通讯获取完整的参数\",{\"1\":{\"41\":1}}],[\"每个gpu保存1\",{\"1\":{\"41\":1}}],[\"每个进程一个\",{\"1\":{\"14\":1}}],[\"每次都会有ψ\",{\"1\":{\"39\":1}}],[\"每次发送对应位置的数据进行累加\",{\"1\":{\"11\":1}}],[\"每一个集群上的节点假如包含g个gpu卡\",{\"1\":{\"66\":1}}],[\"每一列代表一个时间段\",{\"1\":{\"55\":1}}],[\"每一种颜色代表一块\",{\"1\":{\"55\":1}}],[\"每一次累加更新都形成一个拓扑环\",{\"1\":{\"11\":1}}],[\"每一块gpu完成forward和backward后\",{\"1\":{\"8\":1}}],[\"2405\",{\"1\":{\"155\":1,\"159\":1}}],[\"2πn\",{\"1\":{\"140\":1}}],[\"2πn​\",{\"1\":{\"140\":1}}],[\"25000\",{\"1\":{\"139\":1}}],[\"27的时候\",{\"1\":{\"139\":1}}],[\"2的计算资源浪费\",{\"1\":{\"94\":1,\"132\":1}}],[\"2sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"2^n\",{\"1\":{\"70\":1}}],[\"2m\",{\"1\":{\"47\":2}}],[\"2依托allreduce算法实现\",{\"1\":{\"39\":1}}],[\"2310\",{\"1\":{\"88\":1,\"91\":1,\"126\":1,\"129\":1}}],[\"2309\",{\"1\":{\"82\":1,\"120\":1}}],[\"23位尾数位\",{\"1\":{\"26\":1}}],[\"23\",{\"1\":{\"16\":1}}],[\"2207\",{\"1\":{\"161\":1}}],[\"2205\",{\"1\":{\"77\":1,\"115\":1}}],[\"22\",{\"1\":{\"16\":1,\"108\":1}}],[\"21​log10​\",{\"1\":{\"140\":1}}],[\"2105\",{\"1\":{\"85\":1,\"123\":1}}],[\"2104\",{\"1\":{\"61\":1}}],[\"2101\",{\"1\":{\"44\":2}}],[\"21\",{\"1\":{\"16\":1,\"108\":4}}],[\"2023的一篇\",{\"1\":{\"151\":1}}],[\"2024的cvpr\",{\"1\":{\"147\":1}}],[\"2024\",{\"1\":{\"147\":1}}],[\"20\",{\"1\":{\"16\":3}}],[\"2006\",{\"1\":{\"15\":1}}],[\"2\",{\"0\":{\"57\":1},\"1\":{\"8\":1,\"16\":1,\"19\":1,\"24\":1,\"50\":1,\"79\":1,\"117\":1,\"141\":3,\"153\":3}}],[\"分块输入处理\",{\"1\":{\"168\":1}}],[\"分别是compute\",{\"1\":{\"159\":1}}],[\"分别代表第一个和第二个块\",{\"1\":{\"153\":1}}],[\"分别计算图像的上下两个部分\",{\"1\":{\"109\":1}}],[\"分别存储输入矩阵大小为2sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"分别表示pp\",{\"1\":{\"66\":1}}],[\"分别表示第\",{\"1\":{\"55\":2}}],[\"分发到不同的设备上\",{\"1\":{\"70\":1}}],[\"分割a\",{\"1\":{\"60\":1}}],[\"分开重算\",{\"1\":{\"33\":1}}],[\"分配n块计算gpu\",{\"1\":{\"8\":1}}],[\"分布式推理\",{\"2\":{\"170\":1}}],[\"分布式应用等问题\",{\"1\":{\"93\":1,\"131\":1}}],[\"分布式优化器\",{\"1\":{\"14\":1}}],[\"分布式系统\",{\"2\":{\"74\":1,\"102\":1,\"136\":1,\"170\":1}}],[\"分布式系统如何发挥作用\",{\"0\":{\"5\":1}}],[\"分布式系统顾名思义\",{\"1\":{\"3\":1}}],[\"分布式开发\",{\"0\":{\"1\":1,\"75\":1,\"113\":1},\"1\":{\"165\":1}}],[\"downloads\",{\"1\":{\"108\":1}}],[\"download\",{\"1\":{\"108\":1}}],[\"docs\",{\"1\":{\"20\":3}}],[\"doc\",{\"1\":{\"20\":4}}],[\"dfa相较hf每一个flashattention+ffn约能够减少一个fa的计算量\",{\"1\":{\"96\":1,\"134\":1}}],[\"d​qkt​\",{\"1\":{\"84\":1,\"122\":1}}],[\"dropout\",{\"1\":{\"81\":2,\"119\":2}}],[\"dropout的mask层矩阵的大小与softmax的输出一样\",{\"1\":{\"79\":1,\"117\":1}}],[\"dropout层需要sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"dropout层需要mask的大小为sbh\",{\"1\":{\"79\":1,\"117\":1}}],[\"dropout层\",{\"1\":{\"79\":1,\"117\":1}}],[\"dk​​qkt​\",{\"1\":{\"79\":1,\"117\":1}}],[\"d\",{\"1\":{\"66\":2,\"141\":2}}],[\"dtype\",{\"1\":{\"60\":1}}],[\"dtype=args\",{\"1\":{\"60\":1}}],[\"dx\",{\"1\":{\"49\":3}}],[\"ddp支持用户使用统一的api\",{\"1\":{\"20\":1}}],[\"ddp支持三种通讯库\",{\"1\":{\"20\":1}}],[\"ddp\",{\"0\":{\"15\":1}}],[\"dit\",{\"0\":{\"161\":1},\"1\":{\"159\":4,\"167\":1}}],[\"dit模型在文生图和文生视频等表现出了杰出的性能\",{\"1\":{\"159\":1}}],[\"dit模型的推理延迟显著\",{\"1\":{\"157\":1}}],[\"diffusers\",{\"1\":{\"159\":1,\"168\":1}}],[\"diffusers库进行改造\",{\"1\":{\"159\":1}}],[\"diffusion模型在生成时长和内存占用上的表现都差强人意\",{\"1\":{\"146\":1}}],[\"diffusion\",{\"0\":{\"145\":1},\"1\":{\"147\":1}}],[\"difussion核心在迭代去噪点生成内容\",{\"1\":{\"150\":1}}],[\"distvae解决了这个问题\",{\"1\":{\"168\":1}}],[\"distrifuser\",{\"1\":{\"147\":1}}],[\"distrifusion基于u\",{\"1\":{\"157\":1}}],[\"distrifusion需要为每个gpu维护所有层的kv数据\",{\"1\":{\"157\":1}}],[\"distrifusion的加速效果有限\",{\"1\":{\"154\":1}}],[\"distrifusion是一种利用多gpu并行加速扩散模型的新方法\",{\"1\":{\"154\":1}}],[\"distrifusion方法\",{\"1\":{\"150\":1}}],[\"distrifusion\",{\"0\":{\"147\":1},\"1\":{\"147\":1}}],[\"distributeddataparallel\",{\"1\":{\"16\":1}}],[\"distributedoptimizer\",{\"1\":{\"14\":1}}],[\"distributed\",{\"1\":{\"9\":1,\"15\":1,\"16\":1,\"147\":1}}],[\"distflashattn\",{\"0\":{\"91\":1,\"129\":1}}],[\"dim\",{\"1\":{\"60\":2}}],[\"dive\",{\"0\":{\"36\":1,\"40\":1}}],[\"did\",{\"0\":{\"25\":1}}],[\"dir=\",{\"1\":{\"14\":1}}],[\"degree的fresh\",{\"1\":{\"167\":1}}],[\"degree=2的混合并行方法为例\",{\"1\":{\"167\":1}}],[\"degree=4\",{\"1\":{\"167\":1}}],[\"decode\",{\"1\":{\"159\":1}}],[\"decomposition\",{\"1\":{\"109\":1}}],[\"design\",{\"0\":{\"54\":1}}],[\"dest\",{\"1\":{\"49\":1}}],[\"del\",{\"1\":{\"49\":1}}],[\"define\",{\"1\":{\"141\":2}}],[\"def\",{\"1\":{\"49\":5,\"60\":4}}],[\"defragmentation\",{\"0\":{\"43\":1}}],[\"democratizing\",{\"1\":{\"44\":1}}],[\"device=0无法拿到p1\",{\"1\":{\"167\":1}}],[\"devices\",{\"1\":{\"69\":2}}],[\"device\",{\"1\":{\"14\":1,\"70\":1,\"71\":1,\"72\":1}}],[\"deepspeed在知乎也有官号\",{\"1\":{\"82\":1,\"120\":1}}],[\"deepspeed\",{\"0\":{\"82\":1,\"84\":1,\"120\":1,\"122\":1},\"1\":{\"82\":1,\"83\":2,\"120\":1,\"121\":2,\"165\":1}}],[\"deepspeedhttps\",{\"1\":{\"22\":1}}],[\"deeplearning\",{\"1\":{\"20\":1}}],[\"deep\",{\"0\":{\"36\":1,\"40\":1},\"1\":{\"9\":1,\"68\":1}}],[\"dp并行维度\",{\"1\":{\"66\":1}}],[\"dp的区别是\",{\"1\":{\"41\":1}}],[\"dp\",{\"0\":{\"7\":1,\"31\":1,\"36\":1,\"99\":1},\"1\":{\"50\":1}}],[\"dataset\",{\"1\":{\"49\":1}}],[\"data\",{\"1\":{\"6\":2,\"15\":1,\"68\":1,\"161\":1}}],[\"是\",{\"1\":{\"167\":1}}],[\"是扩散模型领域的一个重要的技巧\",{\"1\":{\"166\":1}}],[\"是可能存在无效结果的\",{\"1\":{\"151\":1}}],[\"是指碎片化内存\",{\"1\":{\"29\":1}}],[\"是指通讯过程中\",{\"1\":{\"28\":1}}],[\"是独立计算的\",{\"1\":{\"19\":1}}],[\"是一件非常复杂的事情\",{\"1\":{\"6\":1}}],[\"是在利用切分层内数据\",{\"1\":{\"6\":1}}],[\"在xdit开发了\",{\"1\":{\"168\":1}}],[\"在这个diffusion\",{\"1\":{\"167\":1}}],[\"在图像内部的不同\",{\"1\":{\"167\":1}}],[\"在图像和nlp的模型上都得到更好的模型效果\",{\"1\":{\"53\":1}}],[\"在高分辨率图像生成时\",{\"1\":{\"162\":1}}],[\"在pipefusion基础上升级的xdit诞生了\",{\"1\":{\"159\":1}}],[\"在pytorch中可以通过下面的代码简单实现\",{\"1\":{\"60\":1}}],[\"在设备\",{\"1\":{\"153\":1}}],[\"在获取输入激活块atl\",{\"1\":{\"153\":1}}],[\"在例图使用了n=2\",{\"1\":{\"153\":1}}],[\"在预测ϵθ​\",{\"1\":{\"153\":1}}],[\"在计算某一区块某一层的激活信息时\",{\"1\":{\"152\":1}}],[\"在计算时\",{\"1\":{\"26\":1}}],[\"在多个设备间并行化处理计算\",{\"1\":{\"152\":1}}],[\"在每个layer上做集合通信\",{\"1\":{\"157\":1}}],[\"在每个去噪步骤中\",{\"1\":{\"151\":1}}],[\"在每一步t\",{\"1\":{\"153\":1}}],[\"在每一个batch计算完毕时执行pipeline\",{\"1\":{\"64\":1}}],[\"在时间步t给定含噪图像xt​\",{\"1\":{\"151\":1}}],[\"在生成单张图像时\",{\"1\":{\"149\":1}}],[\"在乘法时做类似竖式乘法的高精度运算\",{\"1\":{\"139\":1}}],[\"在n\",{\"1\":{\"139\":1}}],[\"在数据存储\",{\"1\":{\"139\":1}}],[\"在workerthreadstart\",{\"1\":{\"109\":1}}],[\"在内循环期间\",{\"1\":{\"90\":1,\"128\":1}}],[\"在全对全集合后\",{\"1\":{\"84\":1,\"122\":1}}],[\"在attention计算之前\",{\"1\":{\"83\":1,\"121\":1}}],[\"在sp一次前向和后向总共有4次all\",{\"1\":{\"81\":1,\"119\":1}}],[\"在task1中解释了空间分解的概念\",{\"1\":{\"109\":1}}],[\"在tensor模型并行基础上提出了sequence\",{\"1\":{\"81\":1,\"119\":1}}],[\"在tp并行中只在attention和mlp两个地方进行了并行计算\",{\"1\":{\"80\":1,\"118\":1}}],[\"在需要时\",{\"1\":{\"66\":1}}],[\"在大模型训练过程中显存占用过大往往成为瓶颈\",{\"1\":{\"78\":1,\"116\":1}}],[\"在大模型训练中内存往往比算力更加宝贵\",{\"1\":{\"66\":1}}],[\"在大多数情况下\",{\"1\":{\"46\":1}}],[\"在流水线并行技术中\",{\"1\":{\"64\":1}}],[\"在一个大规模的gpu集群上达到超过50\",{\"1\":{\"62\":1}}],[\"在所有模型并行\",{\"1\":{\"60\":1}}],[\"在优化\",{\"1\":{\"60\":1}}],[\"在反向传播时要保证在保存不同切分w的gpu中均及时更新a\",{\"1\":{\"60\":1}}],[\"在反向传播计算完成后\",{\"1\":{\"19\":1}}],[\"在该方法下\",{\"1\":{\"60\":1}}],[\"在mlp中最常用的操作是mm\",{\"1\":{\"60\":1}}],[\"在将梯度传输到cpu内存之前\",{\"1\":{\"49\":1}}],[\"在\",{\"1\":{\"49\":5,\"50\":2}}],[\"在实现最小通信量的同时\",{\"1\":{\"46\":1}}],[\"在cpu上的计算量比gpu少多个数量级\",{\"1\":{\"46\":1}}],[\"在过去训练中人们往往忽略了cpu的计算潜力\",{\"1\":{\"45\":1}}],[\"在模型计算的过程中\",{\"1\":{\"25\":1}}],[\"在小batch时\",{\"1\":{\"19\":1}}],[\"在初始化期间将变量从\",{\"1\":{\"14\":1}}],[\"在128卡时甚至已经超过训练开销\",{\"1\":{\"9\":1}}],[\"在探讨了dp\",{\"1\":{\"6\":1}}],[\"在训练过程中\",{\"1\":{\"49\":1}}],[\"在训练过程中总会有数据\",{\"1\":{\"6\":1}}],[\"在训练ai时\",{\"1\":{\"4\":1}}],[\"posts\",{\"0\":{\"173\":1}}],[\"pos\",{\"1\":{\"141\":2}}],[\"pi\",{\"1\":{\"141\":2}}],[\"pipefusion利用过时的kv进行attention计算\",{\"1\":{\"167\":1}}],[\"pipefusion每个设备只存储与其特定阶段相关的参数的1\",{\"1\":{\"158\":1}}],[\"pipefusion与distrifusion不同的是\",{\"1\":{\"158\":1}}],[\"pipefusion通过将图像分割成多个patch并跨多个gpu分布网络层\",{\"1\":{\"156\":1}}],[\"pipefusion可以看作是在distrifusion上的改进与推广\",{\"1\":{\"155\":1}}],[\"pipefusion\",{\"0\":{\"155\":1,\"164\":1},\"1\":{\"161\":1,\"167\":1}}],[\"pipeline尺寸\",{\"1\":{\"64\":1}}],[\"pipeline\",{\"0\":{\"56\":1,\"57\":1},\"1\":{\"52\":1,\"62\":2}}],[\"pipe\",{\"1\":{\"6\":1}}],[\"prefill\",{\"1\":{\"159\":2}}],[\"prefetch张量来完成通信workerp⟵kr+1​\",{\"1\":{\"95\":1,\"133\":1}}],[\"printf\",{\"1\":{\"141\":1}}],[\"prompt\",{\"1\":{\"166\":1}}],[\"project\",{\"1\":{\"159\":1}}],[\"pro1的内容主要是为了让学生了解std\",{\"1\":{\"109\":1}}],[\"prog1\",{\"0\":{\"107\":1}}],[\"prohibitive\",{\"0\":{\"95\":1,\"133\":1}}],[\"proposals\",{\"1\":{\"72\":1}}],[\"properly\",{\"1\":{\"16\":1}}],[\"processgroup来调用不同的集合通讯库\",{\"1\":{\"20\":1}}],[\"process\",{\"1\":{\"16\":1}}],[\"p分区都被投影到查询\",{\"1\":{\"84\":1,\"122\":1}}],[\"p\",{\"1\":{\"66\":1,\"82\":1,\"84\":1,\"120\":1,\"122\":1}}],[\"p的策略\",{\"1\":{\"62\":1}}],[\"perspectivehttps\",{\"1\":{\"85\":1,\"123\":1}}],[\"per\",{\"1\":{\"60\":2}}],[\"pdf\",{\"1\":{\"44\":2,\"61\":1,\"68\":1,\"77\":1,\"82\":1,\"85\":1,\"88\":1,\"91\":1,\"115\":1,\"120\":1,\"123\":1,\"126\":1,\"129\":1,\"147\":1}}],[\"pytorch使用hook机制\",{\"1\":{\"19\":1}}],[\"pytorch使用gradient\",{\"1\":{\"19\":1}}],[\"pytorch在设计api时做到了仅调用第11行代码中的distributeddataparallel部分\",{\"1\":{\"16\":1}}],[\"pytorch\",{\"0\":{\"15\":1},\"1\":{\"15\":3,\"20\":1,\"60\":1}}],[\"python前端api\",{\"0\":{\"16\":1}}],[\"python\",{\"1\":{\"13\":1}}],[\"pp不能解决序列维度的扩展问题\",{\"1\":{\"83\":1,\"121\":1}}],[\"pp\",{\"0\":{\"100\":1},\"1\":{\"6\":1,\"53\":1}}],[\"paper\",{\"1\":{\"147\":1}}],[\"papers\",{\"1\":{\"147\":1}}],[\"patch\",{\"1\":{\"162\":1,\"167\":1,\"168\":1}}],[\"path=$path\",{\"1\":{\"108\":1}}],[\"patallel\",{\"1\":{\"6\":3}}],[\"padding\",{\"1\":{\"60\":1}}],[\"pass可以来获取不同的性能\",{\"1\":{\"64\":1}}],[\"pass和backward\",{\"1\":{\"64\":1}}],[\"pass\",{\"1\":{\"16\":2,\"49\":1}}],[\"part\",{\"1\":{\"50\":5}}],[\"partition\",{\"1\":{\"50\":1,\"60\":2}}],[\"partitioned\",{\"0\":{\"41\":1}}],[\"params\",{\"1\":{\"60\":1}}],[\"param\",{\"1\":{\"47\":1,\"49\":7}}],[\"parameter\",{\"1\":{\"22\":1,\"59\":1,\"60\":1,\"69\":2}}],[\"parameters\",{\"0\":{\"26\":1},\"1\":{\"16\":2}}],[\"parallelsim的一种\",{\"1\":{\"147\":1}}],[\"parallelization\",{\"1\":{\"71\":1,\"72\":4}}],[\"parallelism\",{\"0\":{\"55\":1,\"56\":1,\"57\":1,\"85\":1,\"123\":1},\"1\":{\"52\":1,\"59\":1,\"68\":1,\"85\":1,\"123\":1,\"146\":1}}],[\"parallel且语义和原始mlp对等\",{\"1\":{\"60\":1}}],[\"parallel\",{\"0\":{\"60\":1,\"80\":1,\"81\":1,\"118\":1,\"119\":1,\"162\":1,\"166\":1,\"167\":1,\"168\":1},\"1\":{\"6\":2,\"15\":1,\"16\":1,\"49\":1,\"60\":10,\"78\":1,\"81\":2,\"116\":1,\"119\":2,\"147\":1,\"161\":4,\"162\":1}}],[\"par\",{\"1\":{\"16\":2}}],[\"梯度与\",{\"1\":{\"47\":1}}],[\"梯度和参数\",{\"1\":{\"24\":1}}],[\"梯度同步算法\",{\"0\":{\"17\":1}}],[\"梯度收集gpu聚合梯度\",{\"1\":{\"8\":1}}],[\"梯度\",{\"1\":{\"6\":1}}],[\"既切分优化器状态\",{\"1\":{\"6\":1}}],[\"既然我们有了对数据切分的方法\",{\"1\":{\"6\":1}}],[\"既然可能有很大的数据集需要切分\",{\"1\":{\"6\":1}}],[\"谷歌专门为tp开发了tpu进行并发适配\",{\"1\":{\"6\":1}}],[\"t以及额外的条件c\",{\"1\":{\"151\":1}}],[\"task\",{\"1\":{\"109\":1}}],[\"tar\",{\"1\":{\"108\":3}}],[\"threadid\",{\"1\":{\"110\":1}}],[\"thread的并行机制和\",{\"1\":{\"109\":1}}],[\"threads\",{\"0\":{\"107\":1}}],[\"thecvf\",{\"1\":{\"147\":1}}],[\"the\",{\"0\":{\"25\":1}}],[\"t10​+t24​+5htas​\",{\"1\":{\"81\":1,\"119\":1}}],[\"t和p\",{\"1\":{\"66\":1}}],[\"t\",{\"1\":{\"66\":1,\"110\":2,\"151\":2,\"153\":1}}],[\"type\",{\"1\":{\"60\":1}}],[\"text\",{\"1\":{\"166\":1}}],[\"temporary\",{\"0\":{\"28\":1,\"34\":1}}],[\"tensorflow框架下的通讯开销越大\",{\"1\":{\"9\":1}}],[\"tensorflow\",{\"1\":{\"9\":1,\"13\":2,\"14\":2}}],[\"tensor\",{\"0\":{\"80\":1,\"118\":1},\"1\":{\"6\":1,\"60\":6,\"79\":1,\"117\":1}}],[\"transformer进行改造\",{\"1\":{\"159\":1}}],[\"transformer\",{\"1\":{\"64\":1,\"77\":1,\"82\":1,\"115\":1,\"120\":1}}],[\"transformers\",{\"0\":{\"60\":1},\"1\":{\"88\":1,\"126\":1}}],[\"training\",{\"1\":{\"15\":1,\"22\":1,\"44\":1,\"52\":1,\"59\":1,\"61\":1,\"82\":1,\"85\":1,\"120\":1,\"123\":1}}],[\"train\",{\"1\":{\"14\":5}}],[\"true\",{\"1\":{\"49\":1}}],[\"trillion\",{\"1\":{\"22\":1}}],[\"token\",{\"0\":{\"94\":1,\"132\":1}}],[\"to\",{\"1\":{\"83\":2,\"84\":1,\"121\":2,\"122\":1}}],[\"topology\",{\"1\":{\"70\":1,\"71\":1,\"72\":1}}],[\"toward\",{\"1\":{\"22\":1}}],[\"torch\",{\"1\":{\"16\":7,\"60\":2}}],[\"tmp\",{\"1\":{\"14\":1}}],[\"tf\",{\"1\":{\"14\":4}}],[\"tp的适用性较低\",{\"1\":{\"159\":1}}],[\"tp模型做了比较\",{\"1\":{\"86\":1,\"124\":1}}],[\"tp在一次前向和后向总共有4次的all\",{\"1\":{\"81\":1,\"119\":1}}],[\"tp\",{\"0\":{\"58\":1,\"98\":1},\"1\":{\"66\":1,\"83\":1,\"121\":1}}],[\"tpu\",{\"1\":{\"53\":1}}],[\"tpu运算中无以为继\",{\"1\":{\"24\":1}}],[\"tpu没办法高效完成我们的运算\",{\"1\":{\"4\":1}}],[\"tp基本并行方式后\",{\"1\":{\"6\":1}}],[\"tp也是应用较广的基本并发方式\",{\"1\":{\"6\":1}}],[\"算法\",{\"2\":{\"144\":1}}],[\"算法与数据结构实验题\",{\"1\":{\"138\":1}}],[\"算法的实现转化为一个独立的\",{\"1\":{\"13\":1}}],[\"算法还是0基础新人\",{\"1\":{\"0\":1}}],[\"算子并行\",{\"1\":{\"6\":1}}],[\"产生部分输出\",{\"1\":{\"6\":1}}],[\"让每一个卡分批成环去跑一小批token\",{\"1\":{\"90\":1,\"128\":1}}],[\"让每一个gpu仅保留几个隐藏层的模型\",{\"1\":{\"6\":1}}],[\"让每一张卡只保留必须的参数和算子\",{\"1\":{\"6\":1}}],[\"计算用时\",{\"1\":{\"139\":1}}],[\"计算后再传递回去\",{\"1\":{\"94\":1,\"132\":1}}],[\"计算z=gelu\",{\"1\":{\"81\":1,\"119\":1}}],[\"计算梯度时\",{\"1\":{\"57\":1}}],[\"计算梯度并平均\",{\"1\":{\"50\":1}}],[\"计算\",{\"1\":{\"55\":1}}],[\"计算损失并反向传播\",{\"1\":{\"49\":1}}],[\"计算损失\",{\"1\":{\"49\":1}}],[\"计算放置在一起\",{\"1\":{\"47\":1}}],[\"计算时间呈二次方增长\",{\"1\":{\"157\":1}}],[\"计算时间\",{\"1\":{\"45\":1}}],[\"计算参数时\",{\"1\":{\"41\":1}}],[\"计算粒度更精细\",{\"1\":{\"31\":1}}],[\"计算过程中产生的一些临时数据\",{\"1\":{\"28\":1}}],[\"计算与通信比率的定量或定性度量\",{\"1\":{\"23\":1}}],[\"计算与聚合之间存在间隔\",{\"1\":{\"18\":1}}],[\"计算成本过大的问题\",{\"1\":{\"9\":1}}],[\"计算gpu从聚合gpu中pull完整梯度\",{\"1\":{\"8\":1}}],[\"计算出本地的梯度\",{\"1\":{\"8\":1}}],[\"计算矩阵进行切分\",{\"1\":{\"6\":1}}],[\"计算机大二本科小白\",{\"1\":{\"0\":1}}],[\"为简化说明\",{\"1\":{\"153\":1}}],[\"为解决上述问题\",{\"1\":{\"152\":1}}],[\"为什么不流水线并行batch\",{\"1\":{\"56\":1}}],[\"为什么ai训练需要分布式系统\",{\"0\":{\"4\":1}}],[\"为什么ai需要分布式系统\",{\"0\":{\"2\":1}}],[\"为一个\",{\"1\":{\"47\":1}}],[\"为避免cpu计算成为瓶颈\",{\"1\":{\"47\":1}}],[\"为了加速并行策略的搜索\",{\"1\":{\"68\":1}}],[\"为了确保optimizer\",{\"1\":{\"64\":1}}],[\"为了确定最佳的下载策略\",{\"1\":{\"47\":1}}],[\"为了解决该问题\",{\"1\":{\"87\":1,\"125\":1}}],[\"为了解决这个问题\",{\"1\":{\"46\":1}}],[\"为了解决传统dp计算节点和聚合节点比例不好确定导致的通讯成本\",{\"1\":{\"9\":1}}],[\"为了解决层切分的弊端\",{\"1\":{\"6\":1}}],[\"mlp层由两个linear层组成\",{\"1\":{\"81\":1,\"119\":1}}],[\"mlp中有两层线性layer\",{\"1\":{\"79\":1,\"117\":1}}],[\"mlp的activation大小计算\",{\"1\":{\"79\":1,\"117\":1}}],[\"mlsys\",{\"0\":{\"1\":1,\"113\":1},\"2\":{\"74\":1,\"136\":1}}],[\"mul\",{\"1\":{\"79\":1,\"117\":1}}],[\"multi\",{\"1\":{\"59\":1}}],[\"mcmc算法内部维护一个\",{\"1\":{\"72\":1}}],[\"m=b\",{\"1\":{\"66\":1}}],[\"mm\",{\"1\":{\"60\":1}}],[\"method=init\",{\"1\":{\"60\":1}}],[\"megatron\",{\"0\":{\"59\":1,\"77\":1,\"115\":1},\"1\":{\"59\":2,\"61\":2}}],[\"memory的计算有以下几块\",{\"1\":{\"79\":1,\"117\":1}}],[\"memory\",{\"0\":{\"25\":1,\"27\":1,\"29\":1,\"33\":1,\"35\":1,\"43\":1,\"79\":1,\"117\":1},\"1\":{\"22\":1,\"79\":1,\"117\":1,\"162\":1}}],[\"map的相似性\",{\"1\":{\"148\":1}}],[\"main\",{\"1\":{\"141\":1}}],[\"mandelbrotserial\",{\"1\":{\"110\":2}}],[\"mandelbort\",{\"0\":{\"107\":1},\"1\":{\"109\":1}}],[\"managing\",{\"0\":{\"34\":1,\"35\":1}}],[\"matrix\",{\"1\":{\"79\":1,\"117\":1}}],[\"materialization\",{\"0\":{\"57\":1}}],[\"markov\",{\"1\":{\"69\":1}}],[\"maxiterations\",{\"1\":{\"110\":2}}],[\"max\",{\"1\":{\"60\":1,\"139\":1}}],[\"mask就是会用矩阵以对角线为界mask数据\",{\"1\":{\"94\":1,\"132\":1}}],[\"mask引起的attention计算问题\",{\"1\":{\"94\":1,\"132\":1}}],[\"masked\",{\"1\":{\"60\":4}}],[\"mask\",{\"1\":{\"60\":6}}],[\"mb\",{\"1\":{\"46\":1,\"47\":3}}],[\"m\",{\"1\":{\"46\":1,\"47\":1}}],[\"mp占据模型内存\",{\"1\":{\"33\":1}}],[\"mp都在通信和计算效率之间权衡\",{\"1\":{\"24\":1}}],[\"mp\",{\"1\":{\"23\":1,\"53\":1}}],[\"mpi\",{\"1\":{\"20\":2}}],[\"mit\",{\"1\":{\"147\":1}}],[\"mini\",{\"1\":{\"56\":1}}],[\"minimize\",{\"1\":{\"14\":1}}],[\"microbatch的选择也非常重要\",{\"1\":{\"66\":1}}],[\"micro\",{\"0\":{\"56\":1},\"1\":{\"64\":1,\"66\":1}}],[\"microsoft\",{\"1\":{\"22\":1}}],[\"mseloss\",{\"1\":{\"16\":1}}],[\"momentum\",{\"1\":{\"47\":1}}],[\"monte\",{\"1\":{\"69\":1}}],[\"mon\",{\"1\":{\"14\":3}}],[\"monitoredtrainingsession\",{\"1\":{\"14\":2}}],[\"modelshttps\",{\"1\":{\"82\":1,\"120\":1}}],[\"models\",{\"1\":{\"22\":1,\"59\":1,\"77\":1,\"115\":1,\"147\":1}}],[\"model\",{\"0\":{\"26\":1,\"55\":1,\"60\":1},\"1\":{\"6\":1,\"16\":1,\"25\":1,\"44\":1,\"59\":1,\"60\":8,\"61\":1,\"68\":1}}],[\"被模型状态占用\",{\"1\":{\"24\":1}}],[\"被称作模型并行\",{\"1\":{\"6\":1}}],[\"被我们称作数据并行\",{\"1\":{\"6\":1}}],[\"那样轻松地实现并行策略的混合\",{\"1\":{\"167\":1}}],[\"那么通过对图片本身的上下多份分割\",{\"1\":{\"109\":1}}],[\"那么代价呢\",{\"1\":{\"90\":1,\"128\":1}}],[\"那并行配置\",{\"1\":{\"70\":1}}],[\"那就有4ψ个byte\",{\"1\":{\"26\":1}}],[\"那自然也可能有很大的模型进行切分\",{\"1\":{\"6\":1}}],[\"那我们也可以考虑进行结合\",{\"1\":{\"6\":1}}],[\"那我们就可以考虑把数据切分成好几份\",{\"1\":{\"6\":1}}],[\"那我们可以分到多台设备上\",{\"1\":{\"4\":1}}],[\"那我们可以分到多张卡上面\",{\"1\":{\"4\":1}}],[\"给不同的gpu做计算的方式\",{\"1\":{\"6\":1}}],[\"我觉得\",{\"1\":{\"90\":1,\"128\":1}}],[\"我希望能把这个数据很快的训练完\",{\"1\":{\"6\":1}}],[\"我们知道对于一个n进制数x\",{\"1\":{\"140\":1}}],[\"我们将进一步推导用斯特林公式估算阶乘位数n的公式\",{\"1\":{\"140\":1}}],[\"我们将一开始就对任务给出多线程的解决方式\",{\"1\":{\"110\":1}}],[\"我们将这种方式称作张量并行\",{\"1\":{\"6\":1}}],[\"我们称这个过程中的等待时间为pipeline\",{\"1\":{\"64\":1}}],[\"我们需要保证不同机器处理bucket的顺序一致\",{\"1\":{\"19\":1}}],[\"我们期望有可以自动为我们选定并行方法的策略\",{\"1\":{\"6\":1}}],[\"我们可以看到\",{\"1\":{\"140\":1}}],[\"我们可以考虑在某一个gpu中存储和更新参数\",{\"1\":{\"31\":1}}],[\"我们可以考虑将三种并行方式进行综合\",{\"1\":{\"6\":1}}],[\"我们可以考虑将计算过程中的算子\",{\"1\":{\"6\":1}}],[\"我们可以把一台机器存不下的任务放到多个结点里面\",{\"1\":{\"3\":1}}],[\"我们可以把一台机器的task进行切分\",{\"1\":{\"3\":1}}],[\"我们有我们自己的很大的数据集\",{\"1\":{\"6\":1}}],[\"我们要怎么做呢\",{\"1\":{\"6\":1}}],[\"我们针对ai训练达成的一种共识\",{\"1\":{\"4\":1}}],[\"我们总要拿出一部分数据用来预训练模型\",{\"1\":{\"4\":1}}],[\"等等需要保存的东西\",{\"1\":{\"6\":1}}],[\"激活函数\",{\"1\":{\"6\":2,\"60\":1,\"70\":1}}],[\"参数的选取\",{\"1\":{\"62\":1}}],[\"参数都按块切分\",{\"1\":{\"55\":1}}],[\"参数更新逻辑\",{\"1\":{\"49\":1}}],[\"参数从\",{\"1\":{\"49\":1}}],[\"参数\",{\"1\":{\"6\":2,\"26\":1,\"47\":1,\"50\":1}}],[\"输出\",{\"1\":{\"6\":1,\"26\":1,\"55\":1,\"138\":1}}],[\"输入特征图分割成块\",{\"1\":{\"168\":1}}],[\"输入q\",{\"1\":{\"167\":1}}],[\"输入第一行为一个正整数\",{\"1\":{\"138\":1}}],[\"输入的元素个数为sbh个\",{\"1\":{\"79\":1,\"117\":1}}],[\"输入x分割\",{\"1\":{\"60\":1}}],[\"输入\",{\"1\":{\"6\":1,\"26\":1,\"138\":1}}],[\"但dit架构则呈现出较大差异性\",{\"1\":{\"159\":1}}],[\"但difussion模型的特点是\",{\"1\":{\"150\":1}}],[\"但保存原本激活值的方式和distrifusion保持一致\",{\"1\":{\"158\":1}}],[\"但激活尺寸大\",{\"1\":{\"150\":1}}],[\"但激活尺寸小\",{\"1\":{\"150\":1}}],[\"但结果较不精确\",{\"1\":{\"140\":1}}],[\"但仅用于注意力头的非重叠子集\",{\"1\":{\"83\":1,\"121\":1}}],[\"但mask单个元素的大小只用1\",{\"1\":{\"79\":1,\"117\":1}}],[\"但会带来额外的计算代价\",{\"1\":{\"78\":1,\"116\":1}}],[\"但即使这样\",{\"1\":{\"70\":1}}],[\"但大多数\",{\"1\":{\"70\":1}}],[\"但要增加计算时长\",{\"1\":{\"57\":1}}],[\"但占用空间会多很多\",{\"1\":{\"56\":1}}],[\"但对于小\",{\"1\":{\"46\":1}}],[\"但重点是计算规模和速度\",{\"1\":{\"24\":1}}],[\"但参数量的上升在传统的单机gpu\",{\"1\":{\"24\":1}}],[\"但是pipefusion则需要在下一个diffusion\",{\"1\":{\"167\":1}}],[\"但是在之前都是基于hf\",{\"1\":{\"159\":1}}],[\"但是在更新参数进行新的计算时\",{\"1\":{\"26\":1}}],[\"但是与此同时dits\",{\"1\":{\"159\":1}}],[\"但是图片不是整体了\",{\"1\":{\"149\":1}}],[\"但是这种方法由于缺少各个patch间的信息感知\",{\"1\":{\"149\":1}}],[\"但是同生成式对话使用有限的文本量便能达成较为不错的生成效果不同\",{\"1\":{\"146\":1}}],[\"但是一般来说高精度阶乘的时间复杂度是o\",{\"1\":{\"139\":1}}],[\"但是我们都知道\",{\"1\":{\"139\":1}}],[\"但是包含dropout用到的mask\",{\"1\":{\"79\":1,\"117\":1}}],[\"但是单卡\",{\"1\":{\"53\":1}}],[\"但是事实上可以考虑在不同隐藏层中实现异步更新参数\",{\"1\":{\"39\":1}}],[\"但是zero3需要对参数进行切分和更新\",{\"1\":{\"39\":1}}],[\"但是需要时常更新\",{\"1\":{\"33\":1}}],[\"但是考虑在更新权重的时候\",{\"1\":{\"26\":1}}],[\"但是由于nvidia对于fp16精度计算的高优化度\",{\"1\":{\"26\":1}}],[\"但是也可以利用类似cpu指令流水线执行的方式\",{\"1\":{\"6\":1}}],[\"但是仅让一个卡存模型的几层在计算某些必须要用到之前的数据的模型时可能不尽如人意\",{\"1\":{\"6\":1}}],[\"但每一个gpu均做保留完整的模型做计算\",{\"1\":{\"6\":1}}],[\"但可惜一台机器的性能总是有限的\",{\"1\":{\"4\":1}}],[\"但同时也可能引入更多的问题\",{\"1\":{\"3\":1}}],[\"这套开发接口尽可能可能复用\",{\"1\":{\"159\":1}}],[\"这存在两个选择和两个问题\",{\"1\":{\"152\":1}}],[\"这篇文章也是neurips\",{\"1\":{\"151\":1}}],[\"这给t与t−1步的模型ϵ并行带来了极大的困难\",{\"1\":{\"151\":1}}],[\"这也是前言讲难以像文本推理一样通过简单的切分矩阵实现并行的原因\",{\"1\":{\"149\":1}}],[\"这也限制了可以使用的卡的规模\",{\"1\":{\"63\":1}}],[\"这使得pipefusion无法像大型语言模型\",{\"1\":{\"167\":1}}],[\"这使得最为常用的\",{\"1\":{\"146\":1}}],[\"这使得参与的gpu可以并行计算不同的注意力头\",{\"1\":{\"83\":1,\"121\":1}}],[\"这对估算阶乘的规模来说是完全可以接受的误差\",{\"1\":{\"140\":1}}],[\"这将是一件极为恐怖的事情\",{\"1\":{\"139\":1}}],[\"这道题看上去还挺有意思的很符合大学生的心理状态\",{\"1\":{\"139\":1}}],[\"这一天晚上\",{\"1\":{\"138\":1}}],[\"这一部分依赖于softmax实现\",{\"1\":{\"79\":1,\"117\":1}}],[\"这一部分主要介绍作者使用zero的一些想法\",{\"1\":{\"30\":1}}],[\"这被称为空间分解\",{\"1\":{\"109\":1}}],[\"这是我的事后诸葛亮\",{\"1\":{\"109\":1}}],[\"这主要是由于causal\",{\"1\":{\"94\":1,\"132\":1}}],[\"这里仅作简述\",{\"1\":{\"82\":1,\"120\":1}}],[\"这就是第一种错误的可能\",{\"1\":{\"139\":1}}],[\"这就是ai使用分布式系统想要解决的问题\",{\"1\":{\"4\":1}}],[\"这就给我们加大数据量和多机并行提供了极大的便利\",{\"1\":{\"78\":1,\"116\":1}}],[\"这个scatter操作的输出随后被输入到稀疏操作fl​中\",{\"1\":{\"153\":1}}],[\"这个sp也是目的也是解决输入序列规模问题\",{\"1\":{\"86\":1,\"124\":1}}],[\"这个公式以詹姆斯\",{\"1\":{\"140\":1}}],[\"这个男孩给了弯通一个数字\",{\"1\":{\"138\":1}}],[\"这个操作等价于先对输入进行\",{\"1\":{\"60\":1}}],[\"这个时候就不得不引入分布式系统来改善这种局面了\",{\"1\":{\"4\":1}}],[\"这两种措施共同保证了zero\",{\"1\":{\"46\":1}}],[\"这又是一个仅在计算时才能用到的额外copy\",{\"1\":{\"26\":1}}],[\"这会导致梯度同步结果出现错误\",{\"1\":{\"19\":1}}],[\"这样一个diffusion\",{\"1\":{\"167\":1}}],[\"这样就可以将每一部分计算矩阵进行并发处理\",{\"1\":{\"6\":1}}],[\"这样可以训练更大的模型\",{\"1\":{\"6\":1}}],[\"这种计算需求会以超过二次方的速度增加\",{\"1\":{\"151\":1}}],[\"这种ring\",{\"1\":{\"90\":1,\"128\":1}}],[\"这种方法的好处是保障了各自在独立计算时的语义对等\",{\"1\":{\"60\":1}}],[\"这种方式能不能过这个题我没有试过因为我懒\",{\"1\":{\"139\":1}}],[\"这种方式理论上并不影响训练结果\",{\"1\":{\"90\":1,\"128\":1}}],[\"这种方式有两个问题需要注意\",{\"1\":{\"19\":1}}],[\"这种方式被称作流水线并行\",{\"1\":{\"6\":1}}],[\"这种策略被称作自动并行\",{\"1\":{\"6\":1}}],[\"这种并行考量被称为3d并行\",{\"1\":{\"6\":1}}],[\"这种并行方式被称作完全分片数据并行\",{\"1\":{\"6\":1}}],[\"这种按层切分的方式固然可以增加可容纳的模型的大小\",{\"1\":{\"6\":1}}],[\"这种按层切分模型\",{\"1\":{\"6\":1}}],[\"这种每个一份数据切分成多份\",{\"1\":{\"6\":1}}],[\"这种系统的好处是显而易见的\",{\"1\":{\"3\":1}}],[\"这基本上是在不考虑机器性能上限的情况下\",{\"1\":{\"4\":1}}],[\"这些都问题都有进一步研究的价值\",{\"1\":{\"3\":1}}],[\"拓展数据规模\",{\"1\":{\"3\":1}}],[\"各个节点之间通过网络进行通讯的计算机系统\",{\"1\":{\"3\":1}}],[\"什么是分布式系统\",{\"0\":{\"3\":1}}],[\"路漫漫其修远兮\",{\"1\":{\"0\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,zt(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n,id:o}})=>{const u=bt[s];e==="suggest"?self.postMessage([e,o,tt(t,u,n)]):e==="search"?self.postMessage([e,o,Z(t,u,n)]):self.postMessage({suggestions:[e,o,tt(t,u,n)],results:[e,o,Z(t,u,n)]})};
//# sourceMappingURL=index.js.map
