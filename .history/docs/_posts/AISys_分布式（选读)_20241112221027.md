---
# 文章标题
title: AISys_分布式开发（选读）
# 设置写作时间
date: 2024-10-28
# 一个页面可以有多个分类
category:
  - SOSD
# 一个页面可以有多个标签
tag:
  - 分布式系统
  - AISys
  - 并行运算
# 此页面会在文章列表置顶
sticky: true
# 此页面会出现在文章收藏中
star: true
# 侧边栏的顺序
# 数字越小越靠前，支持非整数和负数，比如 -10 < -9.5 < 3.2, order 为 -10 的文章会最靠上。
# 个人偏好将非干货或随想短文的 order 设置在 -0.01 到 -0.99，将干货类长文的 order 设置在 -1 到负无穷。每次新增文章都会在上一篇的基础上递减 order 值。
order: -1.3
---
## 序列并行

### Megatron

Reducing Activation Recomputation in Large Transformer Models

`https://arxiv.org/pdf/2205.05198`

#### Abstract

在大模型训练过程中显存占用过大往往成为瓶颈，一般会通过`重计算`的方式降低显存占用，但会带来额外的计算代价。本文提出`sequece parallel(序列并行,简称SP)`和`selective activation recomputation`两种方法，可以结合TP有效减少不必要的计算量。  

下图中绿色部分表示不同参数级别模型中需要用于保存activation需要的显存大小，蓝色部分表示不同参数级别模型中需要用于保存`parameter`和`optimizer state`需要的显存大小。红色线表示baseline(A100的显存)80G。  

通过对比可以发现,原本单A100跑不了的模型,经过SP优化后可以在单A100上运行了,这就给我们加大数据量和多机并行提供了极大的便利  

![SP效果示例](../.vuepress/public/img/SP1.png)  

#### Activation Memory

本文以Transformer结构为例估算`Activation Memory`，Activation指FWD和BWD梯度计算中创建的所有`tensor`。不包含模型参数大小和优化器中状态大小，但是包含dropout用到的`mask tensor`。  

![Activation](../.vuepress/public/img/SP2.png)  

本文推导与假设中用到了以下几个参量:  
![parameter](../.vuepress/public/img/SP3.png)  

本文假设h极大(实际上一般也确实极大), 认为2sb远小于sbh, 即只考虑中间过程的Memory(shb), 忽略输入输出的Memory  

对于Attention模块,这一部分依赖于softmax实现:  

$$ Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V $$  

具体实现图例见下:  

![Attention](../.vuepress/public/img/SP4.png)  

对于`Attention`块来说，输入的元素个数为`sbh`个，每个元素以`半精度`(2 bytes)来进行存储的话，对应输入的元素大小为`2sbh bytes`  

`Attention`块中包含一个`self-attention`、一个`linear(线性映射层)`和`attention dropout`层。对于`linear`需要保存输入的`Activation`大小为`2sbh`, 对于`attention dropout`层需要`mask`的大小为`sbh`(对于一个元素的mask只用1个bytes)，对于`self-attention`块的`Activation Memory`的计算有以下几块：  

- Query(Q),Key(K),Value(V) `matrix mul`：input共享，元素个数为sbh个，总大小是 `2sbh bytes`。
- $QK^{T}$矩阵相乘：需要分别创建保存$Q$和$K$的矩阵，每个矩阵元素总大小为`2sbh bytes`, 总共大小为`4sbh bytes`

原始Self-Attention例子(此处X切分仅作示意,实际上是按行切分的):  

![TP](../.vuepress/public/img/SP5.png)  

![Softmax](../.vuepress/public/img/SP6.png)  

