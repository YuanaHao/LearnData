---
# 文章标题
title: AISys_分布式开发（选读）
# 设置写作时间
date: 2024-10-28
# 一个页面可以有多个分类
category:
  - SOSD
# 一个页面可以有多个标签
tag:
  - 分布式系统
  - AISys
  - 并行运算
# 此页面会在文章列表置顶
sticky: true
# 此页面会出现在文章收藏中
star: true
# 侧边栏的顺序
# 数字越小越靠前，支持非整数和负数，比如 -10 < -9.5 < 3.2, order 为 -10 的文章会最靠上。
# 个人偏好将非干货或随想短文的 order 设置在 -0.01 到 -0.99，将干货类长文的 order 设置在 -1 到负无穷。每次新增文章都会在上一篇的基础上递减 order 值。
order: -1.3
---
## 序列并行

### Megatron

Reducing Activation Recomputation in Large Transformer Models

`https://arxiv.org/pdf/2205.05198`

#### Abstract

在大模型训练过程中显存占用过大往往成为瓶颈，一般会通过`重计算`的方式降低显存占用，但会带来额外的计算代价。本文提出`sequece parallel(序列并行,简称SP)`和`selective activation recomputation`两种方法，可以结合TP有效减少不必要的计算量。  

下图中绿色部分表示不同参数级别模型中需要用于保存activation需要的显存大小，蓝色部分表示不同参数级别模型中需要用于保存`parameter`和`optimizer state`需要的显存大小。红色线表示baseline(A100的显存)80G。  

通过对比可以发现,原本单A100跑不了的模型,经过SP优化后可以在单A100上运行了,这就给我们加大数据量和多机并行提供了极大的便利  

![SP效果示例](../.vuepress/public/img/SP1.png)  

#### Activation Memory

本文以Transformer结构为例估算`Activation Memory`，Activation指FWD和BWD梯度计算中创建的所有`tensor`。不包含模型参数大小和优化器中状态大小，但是包含dropout用到的`mask tensor`。  

![Activation](../.vuepress/public/img/SP2.png)  

本文推导与假设中用到了以下几个参量:  
![parameter](../.vuepress/public/img/SP3.png)  

本文假设h极大(实际上一般也确实极大), 认为2sb远小于sbh, 即只考虑中间过程的Memory(shb), 忽略输入输出的Memory  

对于Attention模块,这一部分依赖于softmax实现:  

$$ Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V $$  

具体实现图例见下:  

![Attention](../.vuepress/public/img/SP4.png)  

对于`Attention`块来说，输入的元素个数为`sbh`个，每个元素以`半精度`(2 bytes)来进行存储的话，对应输入的元素大小为`2sbh bytes`  

`Attention`块中包含一个`self-attention`、一个`linear(线性映射层)`和`attention dropout`层。对于`linear`需要保存输入的`Activation`大小为`2sbh`, 对于`attention dropout`层需要`mask`的大小为`sbh`(对于一个元素的mask只用1个bytes)，对于`self-attention`块的`Activation Memory`的计算有以下几块：  

- Query(Q),Key(K),Value(V) `matrix mul`：input共享，元素个数为sbh个，总大小是 `2sbh bytes`。
- $QK^{T}$矩阵相乘：需要分别创建保存$Q$和$K$的矩阵，每个矩阵元素总大小为`2sbh bytes`, 总共大小为`4sbh bytes`

原始Self-Attention例子(此处X切分仅作示意,实际上是按行切分的):  

![TP](../.vuepress/public/img/SP5.png)  

![Softmax](../.vuepress/public/img/SP6.png)  

- dropout的mask层矩阵的大小与softmax的输出一样，元素个数都是$as^{2}b$个，但mask单个元素的大小只用`1 bytes`即可，总的大小为 $as^{2}b$ bytes

- softmax的输出也会用于反向的计算，需要缓存下来，对应大小 $as^{2}b$ bytes

- $V$矩阵的大小之前没有统计，和$Q$、$K$矩阵一样，大小也是`2sbh bytes`

> Attention 模块总的大小为 11sbh + 5$as^{2}b$ bytes。  

`MLP的Activation大小计算`：MLP中有两层线性layer，分别存储输入矩阵大小为`2sbh bytes`和`8sbh bytes`；GeLU的反向也需要对输入进行缓存，大小为`8sbh bytes`; dropout层需要`sbh bytes`; 总大小为`19sbh`。

`LayerNorm的Activation大小计算`：每个LayerNorm层的输入需要`2sbh`大小，有两个LayerNorm层，总大小为`4sbh bytes`.  

一层transformer的memory总的大小为:  

$$ ActivationMemoryPerLayer=sbh\left(34+5\frac{as}h\right) $$  

#### Tensor Parallel

在TP并行中只在Attention和MLP两个地方进行了并行计算，对于两块的输入并没有并行操作。    

图中$f$和$\overline{f}$互为共轭(conjugate)，$f$在前向时不做操作，反向时执行all-reduce;$\overline{f}$在前向时执行all-reduce, 反向时不做操作。

![TP](../.vuepress/public/img/SP7.png)  

#### Sequence Parallel

在Tensor模型并行基础上提出了Sequence Parallel，对于非Tensor模型并行的部分在sequence维度都是相互独立的，所以可以在sequence维度上进行拆分(即sequence parallel)。  
拆分后如下图， f f f 和 f ‾ \overline{f} f​ 替换为 g g g 和 g ‾ \overline{g} g​， g g g 和 g ‾ \overline{g} g​ 也是共轭的， g g g 在前向是all-gather通信，反向是reduce-scatter通信； g ‾ \overline{g} g​在前向是reduce-scatter, 反向是all-gather通信。
